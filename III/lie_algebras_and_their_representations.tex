\documentclass[a4paper]{article}

\def\npart{III}

\def\ntitle{Lie Algebras and Their Representations}
\def\nlecturer{I.\ Grojnowski}

\def\nterm{Michaelmas}
\def\nyear{2019}

\input{header}

\DeclareMathOperator{\Mat}{Mat}
\newcommand*{\Lie}[1]{\mathfrak{#1}} % Lie groups
\renewcommand*{\P}{\mathbb{P}}
\newcommand{\ad}{\mathrm{ad}} % adjoint
\let\ch\relax
\DeclareMathOperator{\ch}{ch} % character
\DeclareMathOperator{\cha}{ch} % characteristic
%\newcommand*{\gg}{\Lie{g}}

\begin{document}

\input{titlepage}

\tableofcontents

\section{Introduction \& Motivation}

The objects of interest in this course are
\begin{align*}
  \SL_n &= \{A \in \Mat_n: \det A = 1\} \\
  \SO_n &= \{A \in \SL_n: AA^T = I\} \\
  \Sp_{2n} &= \cdots
\end{align*}
and five more examples. First of all they are algebraic groups.

We have \(\SU_2 \subseteq \SL_2\). Note that \(\SU_2\) is homeomorphic to \(S^3\) and so is compact. In fact it is maximal compact and every maximal compact subgroup of \(\SL_2\) is conjugate to \(\SU_2\).

We will look at the tangent space of the group at the identity, which is just a finite-dimensional vector space.

\begin{definition}
  A \emph{linear algebraic group} is a subgroup of \(\Mat_n\) which is defined by polynomial equations in the matrix coefficients.
\end{definition}

For example \(\SL_n\) and \(\SO_n\) are linear algebraic groups. \(\GL_n\) is also an example as we have embedding
\begin{align*}
  \GL_n &\to \Mat_{n + 1} \\
  A &\to
      \begin{pmatrix}
        A & \\
        & \lambda
      \end{pmatrix}
\end{align*}
where the image is given by \(\det A \cdot \lambda = 1\).

\begin{eg}
  Let \(G = \SL_2\) and let
  \[
    g =
    \begin{pmatrix}
      1 & \\
      & 1
    \end{pmatrix}
    + \varepsilon
    \begin{pmatrix}
      a & b \\
      c & d
    \end{pmatrix}
    + \cdots
  \]
  so
  \[
    \det g = 1 + \varepsilon(a + d) + \text{higher terms}
  \]
  so \(\det g = 1\) if and only if \(a + d = 0\) if we pretend to be physicists for a second. Now introduce the dual numbers
  \[
    E = \C[\varepsilon]/(\varepsilon^2) = \{a + b \varepsilon: a, b \in \C\}.
  \]
  If \(G\) is an algebraic group then we define
  \[
    G(E) = \{A \in \Mat_n(E): A \text{ satisfies the defining equations of } G\}.
  \]
  Then
  \[
    \SL_2(E) = \{
    \begin{pmatrix}
      \alpha & \beta \\
      \gamma & \delta
    \end{pmatrix}
    : \alpha, \beta, \gamma, \delta \in E, \alpha \delta - \beta \gamma = 1\}
  \]
  Now the map \(E \to \C, \varepsilon \mapsto 0\) defines a map \(\pi: G(E) \to G\). We define the \emph{Lie algebra}\index{Lie algebra} of \(G\) to be
  \[
    \Lie g \cong \pi^{-1}(I) \cong \{X \in \Mat_n(\C): I + \varepsilon X \in G(E)\}.
  \]
  In particular,
  \[
    \SL_2 = \{
    \begin{pmatrix}
      a & b \\
      c & d
    \end{pmatrix}
    \in \Mat_2(\C): a + d = 0\}.
  \]
\end{eg}

\begin{ex}
  Show that \(G(E) = TG\) is the tangent bundle of \(G\) and \(\Lie g\) is the tangent space at \(1\), \(I + X \varepsilon\) is the germ of a curve through \(1 \in G\).
\end{ex}

\begin{eg}
  Let \(G = \GL_n\). Then
  \begin{align*}
    G(E) &= \{\tilde A \in \Mat_n(E): \tilde A^{-1} \text{ exists}\} \\
         &= \{A + B \varepsilon: A, B \in \Mat_n(\C), A^{-1} \text{ exists}\}
  \end{align*}
  where the second equality is because
  \[
    (A + B \varepsilon) (A^{-1} - A^{-1}B A^{-1} \varepsilon) = I.
  \]
  So there is no condition on \(B\) so \(\Lie{gl}_n = \Mat_n(\C)\). Another explantion for this result is that \(\det\) does not vanish in a neighbourhood of the identity matrix so we get all matrices in the Lie algebra.
\end{eg}

\begin{ex}
  Let \(G = \SL_n\). Show that
  \[
    \det (I + \varepsilon X) = 1 + \varepsilon \tr X
  \]
  and hence
  \[
    \Lie{sl}_n = \{X \in \Mat_n(\C): \tr X = 0\}.
  \]
\end{ex}

\begin{eg}
  Let
  \[
    G = \OO_n = \{A \in \Mat_n: AA^T = I\}.
  \]
  Then
  \begin{align*}
    \Lie g &= \{X \in \Mat_n(\C): (I + \varepsilon X)(I + \varepsilon X)^T = I\} \\
                &= \{X \in \Mat_n(\C): X + X^T = 0\}
  \end{align*}
  Note \(\tr X^T = \tr X\) so \(\tr X = \tr X^T = 0\). Thus \(\SO_n\) has the same Lie algebra. In other words, by just looking into the Lie algebras we cannot distinguish the groups \(\OO_n\) and \(\SO_n\). This is because \(\OO_n\) has two connected component, and the component of the identity is \(\SO_n\). Of course the tangent space at the identity doesn't tell us anything in the other component. Thus this undesirable situation can be remedied by restricting to connected Lie groups.
\end{eg}

What structure does \(\Lie g\) have that it inherits from \(G\)? It is not a (multiplicative) group as
\[
  (I + A \varepsilon) (I + B \varepsilon) = I + \varepsilon (A + B)
\]
has nothing to do with multiplication. Instead, we can consider the commutator
\begin{align*}
  G \times G &\to G \\
  (P, Q) &\mapsto PQP^{-1}Q^{-1}
\end{align*}
This sends \((I, I) \mapsto I\) so by differentiating at the origin we get a map \(\Lie g \times \Lie g \to \Lie g\). Actually, we want a bilinear map \(\Lie g \times \Lie g \to \Lie g\), so differentiate in each variable separately: fix \(P\) and differentiate \(f_P: Q \mapsto PQP^{-1}Q^{-1}\) to get \(df_P: \Lie g \to \Lie g\). Then we differentiate it as a function of \(P\).

Explicitly, write
\begin{align*}
  P &= I + \varepsilon A \\
  Q &= I + \delta B
\end{align*}
where \(\varepsilon^2 = \delta^2 = 0, \varepsilon \delta = \delta \varepsilon \neq 0\). Then
\[
  PQP^{-1}Q^{-1} = I + (AB - BA) \varepsilon\delta
\]
so the map constructed out of the commutators is
\begin{align*}
  \Lie g \times \Lie g &\to \Lie g \\
  (A, B) &\mapsto AB - BA
\end{align*}
This is called the \emph{Lie bracket} of \(A\) and \(B\).

\begin{ex}\leavevmode
  \begin{enumerate}
  \item Show by differentiation that
    \[
      (PQP^{-1}Q^{-1})^{-1} = QPQ^{-1}P^{-1}
    \]
    implies that
    \[
      [B, A] = -[A, B]
    \]
    so the Lie bracket is anti-symmetric.
  \item Show associativity of multiplication implies that
    \[
      [[X, Y], Z] + [[Y, Z], X] + [[Z, X], Y] = 0.
    \]
    This is the \emph{Jacobi identity}.

    Also show this is true from the definition \([A, B] = AB - BA \in \Mat_n\).
  \end{enumerate}
\end{ex}

\begin{definition}[Lie algebra]\index{Lie algebra}
  Let \(k\) be a field, \(\cha k \neq 2\). A \emph{Lie algebra} \(\Lie g\) is a \(k\)-vector space equipped with a bilinear map \([\cdot, \cdot]: \Lie g \times \Lie g \to \Lie g\) that
  \begin{enumerate}
  \item is anti-symmetric: \([X, Y] = - [Y, X]\),
  \item satisfies the Jacobi identity
    \[
      [[X, Y], Z] + [[Y, Z], X] + [[Z, X], Y] = 0.
    \]
  \end{enumerate}
\end{definition}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item \(\Lie{gl}_n = \Mat_n\) with \([A, B] = AB - BA\). More generally, if \(V\) is a vector space, write \(\Lie{gl}(V) = \End(V)\).
  \item \(\Lie{so}_n = \{A \in \Lie{gl}_n: A + A^T = 0\}\).
  \item \(\Lie{sl}_n = \{A \in \Lie{gl}_n: \tr A = 0\}\).
  \item \(\Lie{sp}_{2n} = \{A \in \Lie{gl}_{2n}: JA^TJ^{-1} + A = 0\}\) where
    \[
      J =
      \begin{psmallmatrix}
        & & & & & 1 \\
        & & & & 1 \\
        & & & \cdots \\
        & -1 \\
        -1
      \end{psmallmatrix}
    \]
  \item \(\Lie{b}_n = \{
    \begin{psmallmatrix}
      * & \cdots & * \\
      & \ddots & * \\
      0 & & *
    \end{psmallmatrix}
    \}
    \) of upper triangular matrices.
  \item \(\Lie u_n\) of strictly upper triangular matrices.
  \item If \(V\) is any vector space, let \([\cdot, \cdot]: V \times V \to V\) be the zero map. This is a Lie algebra, called \emph{abelian Lie algebra}.
  \end{enumerate}
\end{eg}

\begin{ex}\leavevmode
  \begin{enumerate}
  \item Show \(\Lie{gl}_n\) is a Lie algebra.
  \item Show examples 2 - 7 are sub-Lie algebras of \(\Lie{gl}_n\).
  \item Find algebraic groups whose Lie algebras are the examples above.
  \item Show \(\{
    \begin{psmallmatrix}
      * & * \\
      * & 0
    \end{psmallmatrix}
    \} \subseteq \Lie{gl}_2\) is not a Lie algebra.
  \end{enumerate}
\end{ex}

\begin{eg}
  Any \(1\)-dim Lie algebra is abelian by anti-symmetry.
\end{eg}

\begin{ex}
  Classify all Lie algebras of dimension \(3\).
\end{ex}

\begin{definition}[representation]\index{representation}
  A \emph{representation} of a Lie algebra \(\Lie g\) on a vector space \(V\) is a Lie algebra homomorphism \(\Lie g \to \Lie{gl}(V)\). We say \(\Lie g\) acts on \(V\).
\end{definition}

We have the silly example of trivial representation: \(\Lie g\) acts on \(V = k\) by \(x \mapsto 0\).

Less trivially, for any \(x \in \Lie g\), define
\begin{align*}
  \ad x: \Lie g &\to \Lie g \\
  y &\mapsto [x, y]
\end{align*}

\begin{lemma}
  \(\ad: \Lie g \to \End(\Lie g)\) is a representation of \(\Lie g\), i.e.\ \(\Lie g\) acts on it self. This is called the \emph{adjoint representation}\index{adjoint representation}.
\end{lemma}

\begin{proof}
  Must show
  \[
    \ad [x, y] = \ad x \ad y - \ad y \ad x.
  \]
  If \(z \in \Lie g\) then
  \begin{align*}
    (\ad [x, y])(z) &= [[x, y], z] \\
    \text{RHS}(z) &= [x, [y, z]] - [y, [x, z]] = -[[y, z], x] - [[z, x], y]
  \end{align*}
  and they are equal by Jacobi.
\end{proof}

\begin{definition}[center]\index{center}
  The \emph{center} of \(\Lie g\) is
  \[
    \{x \in \Lie g: [x, y] = 0 \text{ for all } y \in \Lie g\} = \ker (\ad: \Lie g \to \Lie{gl}(\Lie g)),
  \]
  which is an abelian Lie algebra.
\end{definition}

In particular, the center of \(\Lie g\) is \(0\) if and only if \(\ad\) is an embedding. Question: does every finite-dimensional Lie algebra \(\Lie g\) have a faithful finite-dimensional representation? In other words, does \(\Lie g \embed \Lie{gl}(V)\) for some \(V\)?

Note: every affine algebraic group has a faithful representation.

\begin{theorem}[Ado]
  Any finite-dimensional Lie algebra \(\Lie g\) over \(k\) has a faithful finite-dimensional rep, i.e.\ \(\Lie g \embed \Lie{gl}_n\) for some \(.\)
\end{theorem}

\begin{eg}
  Let \(\Lie g = \Lie{sl}_2\) with basis
  \[
    e =
    \begin{pmatrix}
      0 & 1 \\
      0 & 0
    \end{pmatrix},
    f =
    \begin{pmatrix}
      0 & 0 \\
      1 & 0
    \end{pmatrix},
    h =
    \begin{pmatrix}
      1 & 0 \\
      0 & -1
    \end{pmatrix}
  \]
  so we have
  \[
    [e, f] = h,
    [h, e] = 2e,
    [h, f] = -2f
  \]
  so a representation of \(\Lie{sl}_2\) is a triple of matrices \(E, F, H \in \Mat_n\) with these relations. How can we find such? The answer, at this moment, is to find reps of the algebraic group \(\SL_2\) and differentiating. Later we will find them just by using linear algebra.
\end{eg}

\begin{definition}[algebraic representation]\index{algebraic representation}
  If \(G\) is an algebraic group. An \emph{algebraic representation} of \(G\) on a vector space \(V\) is a homomorphism \(G \to \GL(V)\) defined by polynomial equations in the matrix coefficients.
\end{definition}

Let \(\rho: G \to \GL(V)\) be an algebraic rep. We have \(\rho(I) = I\). Consider the map \(G(E) \to \GL(V)(E)\). We get
\[
  \rho(I + A\varepsilon) = I + \varepsilon d\rho(A)
\]
for some function \(d\rho(A)\) of \(A\).

\begin{ex}
  \(d \rho\) is the derivative of \(\rho\) at identity.
\end{ex}

\begin{ex}
  \(\rho: G \to \GL(V)\) implies that \(d\rho: \Lie g \to \Lie{gl}(V)\) is a Lie algebra homomorphism, so \(V\) is a representation of \(\Lie g\).
\end{ex}

Let \(G = \SL_2\) and let \(L(n)\) be homogeneous polynomials in \(x, y\) of degree \(n\), with basis \(x^n, x^{n - 1}y, \cdots, y^n\), so has dimension \(n + 1\). \(\GL_2\) acts on \(L(n)\) by change of coordinates: if \(g =
\begin{pmatrix}
  a & b \\
  c & d
\end{pmatrix}
\), \(f \in L(n)\) then
\[
  (\rho_n(g)f)(x, y) = f(ax + cy, bx + dy).
\]
Check that
\begin{enumerate}
\item \(\rho_0\) is the trivial rep.
\item \(\rho_1\) is the usual \(2\)-dim rep.
\item
  \[
    \rho_2
    \begin{pmatrix}
      a & b \\
      c & d
    \end{pmatrix}
    =
    \begin{pmatrix}
      a^2 & ab & b^2 \\
      2ac & ad + bc & 2bd \\
      c^2 & cd & d^2
    \end{pmatrix}
  \]
\end{enumerate}
Differentiate and we get an action of \(\Lie{sl}_2\) on \(L(n)\). Explicitly,
\[
  \rho(I + \varepsilon e) x^iy^j
  = x^i (y + \varepsilon x)^j
  = x^iy^j + \varepsilon jx^{i + 1} y^{j - 1}
\]
and hence
\[
  d\rho(e) x^iy^j = jx^{i + 1} y^{j - 1}.
\]

\begin{ex}\leavevmode
  \begin{enumerate}
  \item The Lie algebra acts by
    \begin{align*}
      e \cdot (x^iy^j) &= jx^{i + 1} y^{j - 1} \\
      f \cdot (x^iy^j) &= ix^{i - 1} y^{j + 1} \\
      h \cdot (x^iy^j) &= (i - j) x^iy^j
    \end{align*}
  \item Check directly this gives a rep of \(\Lie{sl}_2\).
  \item Show \(L(2)\) is isomorphic to the adjoint rep.
  \item Show that
    \[
      e = x \frac{\partial  }{\partial y}, f = y \frac{\partial  }{\partial x}, h = x \frac{\partial  }{\partial x} - y \frac{\partial  }{\partial y}
    \]
    defines an (infinite-dimensional) rep of \(\Lie{sl}_2\) on \(k[x, y]\). Some implication: this can be defined for all characteristics, and the differential operator is suggesting that reps of Lie groups might have something to do with calculus.
  \item Show if \(\cha k = 0\) then \(L(n)\) is irreducible as an \(\Lie{sl}_2\), hence \(\SL_2\)-module.
  \end{enumerate}
\end{ex}

The map \(\rho \mapsto d \rho\) defines a functor from the category of a linear algebraic group \(G\) to the category of Lie algebra reps of \(\Lie g\). However, this is not as nice a map as you might hope.

\begin{eg}
  Let \(G = \C^\times\) so \(\Lie g = \C\) is the abelian Lie algebra. A rep of \(\Lie g\) on a vector space \(V\) is the same as an element \(A \in \End(V)\). A submodule \(W \subseteq V\) is a subspace \(W\) such that \(gW \subseteq W\), i.e.\ \(A \cdot W \subseteq W\), so the same as an \(A\)-subspace of \(V\). Check that \(A\) and \(A'\) in \(\End(V)\) determine isomorphic reps of \(\Lie g\) if and only if \(A, A'\) are conjugate. Hence isomorphism classes of reps of \(\Lie g = \C\) is in bijection with conjugacy classes of matrices, and hence is determined by its Jordan normal form.

  In addition, any \(A \in \End(V)\) has an eigenvector as \(V\) is a vector space over \(\C\). Thus the only irreducible rep of \(\Lie g\) are the 1-dim ones.

  A rep is isomorphic to a direct sum of irred reps if and only if \(A\) is diagonalisable. For example if \(A =
  \begin{psmallmatrix}
    0 & 1 \\
    & 0 & 1 \\
    & & & \ddots \\
    & & & & 1 \\
    & & & & 0
  \end{psmallmatrix}
  \) then the associated rep is \emph{indecomposable}, i.e.\ it does not split into a direct sum, as the only \(A\)-subspaces are \(\langle e_1, \rangle, \langle e_1, e_2 \rangle, \cdots, \langle e_1, \dots, e_n \rangle\).

  Now in constrast consider reps of \(G = \C^\times\). It is a theorem that the irred algebraic reps of \(\C^\times\) are the 1-dim reps where \(z \in \C^\times\) acts on \(\C\) by \(z \cdot v = z^n v\) for \(n \in \Z\). In other words they are given by \(G \to \GL_1, z \mapsto z^n\). Moreover, any finite-dimensional rep of \(G\) is a direct sum of irreducible (this is similar to the proof that the only irred reps of the compact group \(S^1\) are given by \(z \mapsto z^n\), once we set up the theory of algebraic groups).
\end{eg}

\begin{ex}
  Show \(\rho \mapsto d\rho\) sends \(z \mapsto z^n\) to the algebraic rep \(n \in \C\).
\end{ex}

The rep of Lie algebra \(\C\) is continuous while that of the algebraic group \(\C^\times\) is discrete. This has something to do with \(S^1\) and its topology. Later we'll see that the functor \(d\) gives an equivalence of category when restricted to simply connected Lie groups.

\begin{note}
  Notice \(\Lie g\) is also the Lie algebra of the additive group \((\C, +)\), whose algebraic reps resemble the reps of \(\Lie g\).
\end{note}

Less distressingly, if \(Z \subseteq G\) is a finite central subgroup then \(T_1(G/Z) = T_1G\) so the Lie algebras of \(G\) and \(G/Z\) agree.

\begin{ex}
  Let \(G_n = \C^* \ltimes \C\) where \(\C^*\) acts on \(\C\) by \(t \cdot \lambda = t^n \lambda\) so
  \[
    (t, \lambda) (t', \lambda') = (tt', t'^n \lambda + \lambda').
  \]
  Show that \(G_n \cong G_m\) if and only if \(n = \pm m\), but
  \[
    \operatorname{Lie} G_n = \operatorname{Lie} G_m = \C x + \C y
  \]
  where \([x, y] = y\), so the functor is not faithful.
\end{ex}
As a side note, the functor is not surjective either.

\section{Representations of \(\Lie{sl}_2\)}

Recall that \(\Lie{sl}_2\) has basis
\[
  e =
  \begin{pmatrix}
    0 & 1 \\
    0 & 0
  \end{pmatrix}
  , f =
  \begin{pmatrix}
    0 & 0 \\
    1 & 0
  \end{pmatrix}
  , h =
  \begin{pmatrix}
    1 & 0 \\
    0 & -1
  \end{pmatrix}
\]
so we have
\[
  [e, f] = h, [h, e] = 2e, [h, f] = -2f
\]
 
We would like to prove
\begin{theorem}\leavevmode
  \begin{enumerate}
  \item For each \(n \geq 0\) there is a unique irreducible rep of \(\Lie{sl}_2\) of dimension \(n + 1\).
  \item Every finite-dimensional rep of \(\Lie{sl}_2\) is a direct sum of irred reps.
  \end{enumerate}
\end{theorem}

\begin{definition}[weight space]\index{weight space}
  Let \(V\) be a rep of \(\Lie{sl}_2\). If \(\lambda \in \C\), the \emph{\(\lambda\)-weight space} of \(V\) is
  \[
    V_\lambda = \{v \in V: h v = \lambda v\},
  \]
  the eigenspace of \(h\).
\end{definition}

\begin{eg}
  \(L(n)_\lambda = \C x^i y^j\) if \(i - j = \lambda\).
\end{eg}

Let \(v \in V_\lambda\) and we have
\[
  h \cdot ev = (he - eh + eh) v = ([h, e] + eh) v = 2ev + e \lambda v = (\lambda + 2) ev
\]
so if \(v \in V_\lambda\) then \(ev \in V_{\lambda + 2}\), if and only if \(ev \neq 0\). Similarly \(fv \in V_{\lambda - 2}\). Thus \(f\) and \(e\) shifts between a string of spaces \(V_{\lambda + 2}, V_\lambda, V_{\lambda - 2}, \dots\)
\[
  \begin{tikzcd}
    \cdots \ar[r,  shift left] & V_{\lambda - 2} \ar[l, shift left] \ar[r, "e", shift left] & V_\lambda \ar[l, "f", shift left] \ar[r, "e", shift left] & V_{\lambda + 2} \ar[l, "f", shift left] \ar[r, shift left] & \cdots \ar[l, shift left]
  \end{tikzcd}
\]

If \(v \in V_\lambda \cap \ker e\), that is \(ev = 0, hv = \lambda v\) we say \(v\) is a \emph{highest weight vector with highest weight \(\lambda\)}.

\begin{lemma}
  Let \(V\) be a rep of \(\Lie{sl}_2\), \(v \in V_\lambda\) a highest weight vector of weight \(\lambda\) then \(W = \langle v, fv, f^2v, \cdots \rangle\) is an \(\Lie{sl}_2\)-invariant subspace, that is a subrep of \(V.\)
\end{lemma}

\begin{proof}
  We must show the image of \(W\) under \(f, h, e\) are contained in \(W\). \(fW \subseteq W\) by construction. As \(v \in V_\lambda\), we see that \(f^kv \in V_{\lambda - 2k}\) and so \(hW \subseteq W\). Finally \(ev = 0 \in W\) and
  \begin{align*}
    e \cdot fv &= (ef - fe + fe) v = hv = \lambda v \in W \\
    e \cdot f^2 v &= ([e, f] + fe) fv = (\lambda - 2) fv + f\cdot \lambda v = (2\lambda - 2) fv \in W \\
    e \cdot f^3 v &= ([e, f] + fe) f^2 = (\lambda - 4) f^2v + f(2\lambda - 2) fv = (3\lambda - 6)f^2 v \in W
  \end{align*}
  and so on. It is an exercise to show by induction
  \[
    e\cdot f^n v = n (\lambda - n + 1) f^{n - 1} v.
  \]
\end{proof}

We have a surprising result:
\begin{lemma}
  Let \(V\) be a finite-dimensional \(\C\)-space and a rep of \(\Lie{sl}_2\) and \(v \in V\) a highest weight vector with highest weight \(\lambda\) then \(\lambda \in \{0, 1, \dots \} = \Z_{\geq 0}\).
\end{lemma}

\begin{proof}
  Note that all \(f^k v\) lie in different eigenspaces for \(h\) so if non-zero they are linearly independent. But \(V\) is finite dimensional so exists \(k\) such that \(f^k v \neq 0, f^{k + 1} v = 0\). The exercise shows
  \[
    0 = ef^{k + 1}v = (k + 1)( \lambda - k)f^k v
  \]
  so \(k + 1 \neq 0\) so \(\lambda = k\).
\end{proof}

\begin{lemma}
  If \(V\) is a finite-dimensional rep of \(\Lie{sl}_2\) then it has a highest weight vector.
\end{lemma}

\begin{proof}
  As \(V\) is a \(\C\)-space \(h\) has an eigenvector. Apply \(e\) to it get \(v, ev, e^2v, \dots\) which are eigenvectors with different eigenvectors so if nonzero are linearly independent so exists \(k\) such that \(e^kv = 0\), so \(e^kv\) is a highest weight eigenvector.
\end{proof}

\begin{corollary}
  Let \(k = \C\). If \(V\) is an irreducible finite dimensional representation of \(\Lie{sl}_2\) then \(\dim V = n + 1\) and \(V\) has basis \(v_0, v_1, \dots, v_n\) with
  \begin{align*}
    hv_i &= (n - 2i) v_i \\
    fv_i &= v_{i + 1} \\
    ev_i &= i (n - i + 1) v_{i - 1}
  \end{align*}
  In particular there is a unique irreducible representation of dimension \(n + 1\), which is isomorphic to \(L(n)\).
\end{corollary}

(Picture of string)

\begin{ex}\leavevmode
  \begin{enumerate}
  \item Find the explicit relation between this basis and the \(x^ay^b\) basis earlier, where \(a + b = n\).
  \item Recall \(\C[x, y] = \bigoplus_{n \geq 0} L(n)\) as a representation of \(\Lie{sl}_2\) where \(e, h, f\) acts as differential operators. Show that the same operators give a rep of \(\Lie{sl}_2\) on \(x^\lambda y^\mu \C[x/y, y/x]\) for all \(\lambda, \mu \in \C\). Determine the submodules of this rep.
  \end{enumerate}
\end{ex}

Now we show that all reps can be written as direct sum of the irreducible ones. This is one of the more difficult theorem but will lead us towards the general result later. We will show strings of different lengths don't interact, then strings of the same lengths do not interact.

\begin{definition}
  Let \(V\) be a rep of \(\Lie{sl}_2\). Define \(\Omega \in \End(V)\) by
  \[
    \Omega = ef + fe + \frac{1}{2}h^2,
  \]
  the \emph{Casimir} of \(\Lie{sl}_2\).
\end{definition}

\begin{lemma}
  \(\Omega\) is central, that is \(e\Omega = \Omega e, f \Omega = \Omega f, h \Omega = \Omega h\).
\end{lemma}

\begin{proof}
  We will later show a slick proof. For now this is left as an exercise. For example
  \begin{align*}
    e \Omega
    &= e (ef +fe + \frac{1}{2} h^2) \\
    &= e(ef - fe) + 2efe \\
    &+ \frac{1}{2} (eh - he) h + \frac{1}{2} heh \\
    &= 2efe + \frac{1}{2} heh \\
    &= \cdots \\
    &= \Omega e
  \end{align*}
\end{proof}

\begin{corollary}
  If \(V\) is an irreducible rep of \(\Lie{sl}_2\), then \(\Omega\) acts on \(V\) by a scalar.
\end{corollary}

\begin{proof}
  Similar to Schur's lemma.
\end{proof}

\begin{lemma}
  \(\Omega\) acts on \(L(n)\) as multiplication by \(\frac{1}{2} n^2 + n\).
\end{lemma}

\begin{proof}
  We can choose any nonzero element and use the above corollary. Alternatively we can do it by hand. Let \(v\) be the highest weight vector of \(L(n)\) so \(ev = 0, hv = nv\). Then
  \[
    \Omega = (ef - fe) + 2fe + \frac{1}{2} h^2 = (\frac{1}{2} h^2 + h) + 2fe
  \]
  so
  \begin{align*}
    \Omega v &= (\frac{1}{2} n^2 + n) v \\
    \Omega (f^k v) &= f^k \Omega v = (\frac{1}{2} n^2 + n) f^k v
  \end{align*}
\end{proof}

This immediately implies ``strings of different lengths don't interact'', which we shall make sense of now.

Let \(V\) be a finite dimensional rep of \(\Lie{sl}_2\). Let
\[
  V^\lambda = \{v \in V: (\Omega - \lambda)^{\dim V} v = 0\}
\]
be the generalised eigenspace for \(\Omega\) with eigenvalue \(\lambda\). By linear algebra, \(V = \bigoplus_\lambda V^\lambda\). Claim that each \(V^\lambda\) is a subrep, i.e.\ preserved by \(\Lie{sl}_2\), so this is a direct sum decomposition of \(V\) as reps of \(\Lie{sl}_2\).

\begin{proof}
  Let \(x \in \Lie{sl}_2, v \in V^\lambda\). Then
  \[
    (\Omega - \lambda)^{\dim V} xv = x(\Omega - \lambda)^{\dim V} v = 0
  \]
  as \(\Omega\) is central so \(xv \in V^\lambda\).
\end{proof}

Claim that if \(V^\lambda \neq 0\) then \(\lambda = \frac{1}{2} n^2 + n\) for a unique \(n \in \Z_{\geq 0}\), and ``\(V^\lambda\) is glued together from copies of \(L(n)\)''. Formally, ``gluing'' refers to the following:

\begin{definition}[composition series]\index{composition series}
  Let \(W\) be a finite dimensional representation of \(\Lie g\). A \emph{composition series} for \(W\) is a sequence of submodules
  \[
    0 = W_0 \subseteq W_1 \subseteq W_2 \subseteq \cdots \subseteq W_r = W
  \]
  such that each \(W_i/W_{i - 1}\) is a non-zero irreducible module.
\end{definition}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Let \(\Lie g = \C, W = \C^r\) where \(1 \in \Lie g\) acts as \(
    \begin{psmallmatrix}
    0 & 1 \\
    & 0 & 1 \\
    & & & \ddots \\
    & & & & 1 \\
    & & & & 0
  \end{psmallmatrix}
  \). Then there is a unique composition series for \(W\), namely
  \[
    0 \subseteq \langle e_1 \rangle \subseteq \langle e_1, e_2 \rangle \subseteq \cdots \subseteq \langle e_1, \dots, e_r \rangle.
  \]
\item Let \(\Lie g = \C, W = \C^r\) and \(1 \in \Lie g\) acts as \(0\). Then any chain of subspaces
  \[
    W_0 \subseteq W_1 \subseteq \cdots \subseteq W_r
  \]
  with \(\dim W_i = i\) is a composition series.
  \end{enumerate}
\end{eg}

The intuition is that by choosing a suitable basis, we can put each element of \(\Lie g\) into \emph{block triangular form}, with the diagonal blocks \(A_i\) the action on the subquotient \(W_i/W_{i - 1}\), which we require to be irreducible.
\[
  \begin{pmatrix}
    A_1 & & & * \\
    & A_2 \\
    & & \ddots \\
    0 & & & A_r
  \end{pmatrix}
\]

\begin{lemma}
  Composition series always exist.
\end{lemma}

\begin{proof}
  Induct on \(\dim W\). Take an irreducible subrep of \(W\) (why does it always exist?), call it \(W_1\). Then \(W/W_1\) has smaller dimension than \(W\) so has a composition series. Take the preimage of this in \(W\) and stick \(W_1\) in the front.
\end{proof}

\begin{remark}
  The subquotients \(W_i/W_{i - 1}\) are unique (up to reordering). This requires proof in general, but will follow for Lie algebras from what we show in a bit.
\end{remark}

Now we can rephrase the claim as follow: if \(V^\lambda \neq 0\) then \(\lambda = \frac{1}{2} n^2 + n\) for a unique \(n \in \Z_{\geq 0}\), and \(V^\lambda\) has a composition series where all of the subquotients \(W_i/W_{i - 1}\) are isomorphic to \(L(n)\). This proves the slogan ``strings of different lengths don't interact''.

\begin{proof}
  First observe that if \(n \neq m\) then \(\Omega\) acts on \(L(n)\) and \(L(m)\) by different numbers, as \(n \mapsto \frac{1}{2} n^2 + n\) is an increasing function for \(n \geq -1\). Thus if \(V^\lambda \neq 0\), let \(L(n)\) be an irreducible submodule of \(V^\lambda\). As \(\Omega\) acts on \(L(n)\) by \(\frac{1}{2} n^2 + n\), we have \(\lambda = \frac{1}{2}n^2 + n\), and then \(\Omega\) acts on \(V^\lambda/L(n)\) with generalised eigenvalue \(\lambda = \frac{1}{2} n^2 + n\), and for the same reason all composition factors of \(V^\lambda\) must be \(L(n)\) for this \(n\).
\end{proof}

Now we have \(V = \bigoplus_{n \geq 0} V^{\frac{1}{2}n^2 + n}\) where each \(V^{\frac{1}{2}n^2 + n}\) has all composition factors \(L(n)\). We now show strings of the same lengths don't interact.

\begin{lemma}\leavevmode
  \begin{enumerate}
  \item \(hf^k = f^k (h - 2k)\) for all \(k \geq 0\).
  \item \(ef^{k + 1} = f^{k + 1} e + (k + 1) f^k (h - k)\) for all \(k \geq 0\).
  \end{enumerate}
\end{lemma}

\begin{proof}
  Exercise.
\end{proof}

If \(W' \subseteq W\) and \(h\) preserves \(W'\) then the set of generalised eigenvalues of \(h\) on \(W\) is the union on that of \(h\) on \(W'\) and \(W/W'\). As a result, \(h\) acts on \(V^\lambda\) with generalised eigenvalues in \(\{-n, -n + 2, \dots, n - 2, n\}\). Also the only generalised eigenvalue of \(h\) on \(\ker (e: V^\lambda \to V^\lambda)\) is \(n\), that is \((h - n)^{\dim V^\lambda} \cdot x = 0\) for all \(x \in V^\lambda \cap \ker e\).

\begin{proposition}
  \(h\) acts diagonally on \(\ker (e: V^\lambda \to V^\lambda)\), that is it acts by multiplication by \(n\). Thus
  \[
    \ker (e: V^\lambda \to V^\lambda) = (V^\lambda)_n = \{x \in V^\lambda: hx = nx\}.
  \]
\end{proposition}

\begin{proof}
  If \(hx = nx\) then \(ex \in (V^\lambda)_{n + 2} = 0\) so \(x \in \ker e\). Conversely let \(x \in \ker e\). We know \((h - n)^{\dim V^\lambda} x = 0\). By exercises
  \[
    (h - n + 2k)^{\dim V^\lambda} f^kx = f^k (h - n)^{\dim V^\lambda} x = 0
  \]
  so \(f^n x\) is in the generalised eigenspace of \(h\) with eigenvalue \(n - 2k\). Claim that on the other hand, for any \(0 \neq y \in \ker e\), \(f^n y \neq 0\).
  \begin{proof}
    Let \(0 = W_0 \subseteq W_1 \subseteq \cdots \subseteq W_r = V^\lambda\) be a composition series of \(V^\lambda\) such that \(W_i/W_{i - 1} \cong L(n)\) for all \(i\). Then exists \(i\) such that \(y \in W_i, y \neq W_{i - 1}\). Then \(\overline y = y + W_{i - 1} \in W_i/W_{i - 1} \cong L(n)\). Then \(\overline y\) is a highest weight vector of \(L(n)\), so \(f^n(\overline y) \neq 0 \in W_i/W_{i - 1}\) so \(f^n y \neq 0 \in W_i \subseteq V^\lambda\).
  \end{proof}
  
  Now \(f^{n + 1}x\) belongs to the generalised eigenspace of \(h\) with eigenvalue \(-n - 2\), which must be \(0\) by the observation above. Thus \(0 = ef^{n + 1}x\). By exercise this equals to
  \[
    0 = ef^{n + 1}x = (n + 1)f^n (h - n)x + \underbrace{f^{n + 1} ex}_{= 0}
  \]
  so \((n + 1) f^n (h - n)x = 0\). As \(e(h - n)x = (h - n - 2)ex = 0\), we have \((h - n) x \in \ker e\) so if \((h - n)x \neq 0\) then \(f^n (h - n)x \neq 0\). As we are over \(\C\), \(n + 1 \neq 0\) and we just showed \(y \ne o, y \in \ker e\) but \(f^ny \neq 0\), impossible. Thus \((h - x)x = 0\) so \(hx = nx\).
\end{proof}

To show complete reducibility, do the following exercise:

\begin{ex}
  Take a basis \(w_1, \dots, w_k\) of \(\ker e\) and consider the string generated by each \(w_i\), that is \(w_i, fw_i, \dots, f^n w_i\). Show that these give a basis of \(V^\lambda\), each such string is a subrep isomorphic to \(L(n)\) and this gives a direct sum decomposition. In particular \(h\) acts diagonally on all of \(V\) for \(V\) a finite-dimensional rep.
\end{ex}

\begin{ex}
  Show all of this is false in characteristic \(p\). More precisely, show the irreducible reps of \(\Lie{sl}_2\) over \(\overline F_p\) are \emph{not} parameterised by \(n \in \Z_{\geq 0}\). Find a rep of \(\Lie{sl}_2(\overline F_p)\) which does not decompose as a direct sum.
\end{ex}

\subsection{Consequences}

\begin{definition}[tensor product]\index{tensor product}
  Let \(V\) and \(W\) be \(\Lie g\)-reps. Then the \emph{tensor product} of \(V\) and \(W\) is a rep via the map
  \begin{align*}
    \Lie g &\to \End(V \otimes W) = \End(V) \otimes \End(W) \\
    x &\mapsto x \otimes 1 + 1 \otimes x
  \end{align*}
\end{definition}

\begin{ex}\leavevmode
  \begin{enumerate}
  \item Show the above map is a homomorphism of Lie algebras.
  \item Suppose \(G\) acts on \(V\) and \(W\). Show it acts on \(V \otimes W\) by \(g \mapsto g \otimes g\) and the above action is obtained by differentiating this action.
  \end{enumerate}
\end{ex}

Take \(\Lie g = \Lie{sl}_2\). Then by complete reducibility we know \(L(n) \otimes L(m) \cong \bigoplus_{a \geq 0} m_a L(a)\) for some \(m_a\)'s.

\begin{ex}
  Find the highest weight vectors in \(L(1) \otimes L(n)\) and \(L(2) \otimes L(n)\) and hence decompose these.
\end{ex}

To start, let \(v_a\) be a highest weight vector in \(L(a)\). Claim that \(v_n \otimes v_m\) is a highest weight vector in \(L(n) \otimes L(m)\):
\begin{align*}
  h(v_n \otimes v_m &= (hv_n) \otimes v_m + v_n \otimes (hv_m) = (n + m) (v_n \otimes v_m) \\
  e(v_n \otimes v_m) &= (ev_n) \otimes v_m + v_n \otimes (ev_m) = 0
\end{align*}
so \(L(n) \otimes L(m) = L(n + m) \oplus \text{ other stuff}\).

\begin{definition}[character]\index{character}
  Let \(V\) be a finite-dimensional rep of \(\Lie{sl}_2\). Define the \emph{character} of \(V\) to be
  \[
    \ch V = \sum_{n \in \Z} \dim V_n \cdot z^n \in \N[z, z^{-1}].
  \]
\end{definition}

It has the following properties:
\begin{enumerate}
\item \(\ch V|_{z = 1} = \dim V\). This is a consequence of the fact that \(h\) is diagonalisable with integer eigenvalues.
\item \(\ch L(n) = z^n + z^{n - 2} + \dots + z^{2 - n} + z^{-n} = \frac{z^{n + 1} - z^{-{n + 1}}}{z - z^{-1}}\).
\item \(\ch V = \ch W\) if and only if \(V \cong W\) as \(\Lie{sl}_2\) reps.
  \begin{proof}
    Notice that
    \begin{align*}
      \ch L(0) &= 1 \\
      \ch L(1) &= z + z^{-1} \\
      \ch L(2) &= z^2 + 1 + z^{-1} \\
               &\cdots
    \end{align*}
    form a basis of \(\Z[z, z^{-1}]^{S_2}\), the space of symmetric Laurent polynomials with integer coefficients. Now by complete reducibility if \(V \cong \bigoplus _{a\geq 0} n_a L(a), W \cong \bigoplus_{a \geq 0} m_a L(a)\) then \(V \cong W\) if and only if \(n_a = m_a\) for all \(a \geq 0\). As \(\{\ch L(n): n \geq 0\}\) is a basis of \(\Z[z, z^{-1}]^{S_2}\), \(\ch V = \sum m_a \ch L(n)\) determines \(V\).
  \end{proof}
\item \(\ch (V \otimes W) = \ch V \cdot \ch W\). This follows from the exercise: show that \(V_n \otimes W_m \subseteq (V \otimes W)_{n + m}\) and hence \((V \otimes W)_p = \bigoplus_{n + m = p} V_n \otimes W_m\). This is exactly how we multiply polynomials.
\end{enumerate}

\begin{eg}
  \begin{align*}
    \ch (L(1) \otimes L(3))
    &= \ch L(1) \cdot \ch L(3) \\
    &= (z + z^{-1})(z^3 + z + z^{-1} + z^{-3}) \\
    &= (z^4 + z^2 + 1 + z^{-2} + z^{-4}) + (z^2 + 1 + z^{-2})
  \end{align*}
  so complete reducibility and the fact that \(\ch L(n)\) form a basis immediately tell us that \(L(3) \otimes L(1) = L(4) \otimes L(2)\), which is a lot easier than finding highest weight vectors in the tensor product!
\end{eg}

\begin{corollary}[Clebsch-Gordon]\index{Clebsch-Gordon}
  \[
    L(n) \otimes L(m) = \bigoplus_{\substack{k = |n - m| \\ k = n - m \pmod 2}}^{n + m} L(k).
  \]
\end{corollary}

\begin{proof}
  Induction. Also pictorially,
\end{proof}

Purpose of this course: \(\Lie{sl}_n, \Lie{so}_n, \Lie{sp}_{2n}\) etc are simple Lie algebras and the category of their \(\C\)-representations are semisimple, and are parameterised by positive cones in the lattice \(\Z_{\geq 0}^\ell\). Also we can write down their characters parameterised by the lattice. Finally, we are going to draw more pictures like above.

\section{Structure and Classification of simple Lie algebras}

Let's do some warm up exercises in linear algebras. Let \(k\) be a field.

\begin{definition}[simple Lie algebra]\index{simple}
  Let \(\Lie g\) be a Lie algebra over \(k\). \(\Lie g\) is \emph{simple} if \(\dim \Lie g > 1\) and the only ideals of \(\Lie g\) are \(0\) and \(\Lie g\).
\end{definition}

1 dimensional Lie algebras are excluded because they are abelian and as we have seen, their representations do not form a discrete family so they tend to break results we are going to state for nonabelian simple algebras.

In order to describe simple Lie algebras, we will need some Lie algebras which are very far from simple.

\begin{definition}[derived subalgebra]\index{derived subalgebra}
  The \emph{derived subalgebra} of \(\Lie g\), denoted \([\Lie g, \Lie g]\), is the linear span of \([x, y]\) for \(x, y \in \Lie g\).
\end{definition}

\begin{ex}\leavevmode
  \begin{enumerate}
  \item Show \([\Lie g, \Lie g]\) is an ideal.
  \item Show \(\Lie g/[\Lie g, \Lie g]\) is abelian.
  \end{enumerate}
\end{ex}

\begin{definition}[central/derived series]\index{central series}\index{derived series}
  The \emph{central series} for \(\Lie g\) is the sequence of subalgebras
  \[
    \Lie g \supseteq [\Lie g, \Lie g] \supseteq [[\Lie g, \Lie g], \Lie g] \supseteq \cdots
  \]
  or more formally,
  \[
    \Lie g^0 = \Lie g, \quad \Lie g^n = [\Lie g^{n - 1}, \Lie g] \text{ for } n \geq 1.
  \]

  The \emph{derived series} for \(\Lie g\) is the sequence
   \[
    \Lie g \supseteq [\Lie g, \Lie g] \supseteq [[\Lie g, \Lie g], [\Lie g, \Lie g]] \supseteq \cdots
  \]
  or more formally
  \[
    \Lie g^{(0)} = \Lie g, \quad \Lie g^{(n)} = [\Lie g^{(n - 1)}, \Lie g^{(n - 1)}] \text{ for } n \geq 1.
  \]
\end{definition}

Note that \(\Lie g^{(n)} \subseteq \Lie g^n\).

\begin{definition}[nilpotent/solvable Lie algebra]\index{nilpotent}\index{solvable}
  \(\Lie g\) is \emph{nilpotent} if \(\Lie g^n = 0\) for some \(n > 0\), that is if the central series terminates.

  \(\Lie g\) is \emph{solvable} if \(\Lie g^{(n)} = 0\) for some \(n > 0\), that is if the derived series terminates.
\end{definition}

Note that \(\Lie g\) nilpotent implies \(\Lie g\) solvable.

\begin{ex}\leavevmode
  \begin{enumerate}
  \item \(\Lie u\) of strictly upper triangular matrices is nilpotent.
  \item \(\Lie b\) of upper triangular matrices is solvable.
  \item The two dimensional Lie algebra with basis \(x, y\) and \([x, y] = y\) is solvable but not nilpotent.
  \end{enumerate}
\end{ex}

\begin{ex}
  Compute the central and derived series for \(\Lie u, \Lie b\) and show they are nilpotent and solvable repsectively.

  Compute the centre of these Lie algebras.
\end{ex}

\begin{eg}
  Let \(W\) be a symplectic vector space, that is \(W\) is a \(k\)-vector space with a non-degenerated alternating form \(\langle \cdot, \cdot \rangle: W \times W \to k\). For example \(L\) be a finite dimensional vector space and let \(W = L \oplus L^*\) with symplectic form
  \[
    \langle L, L \rangle = \langle L^*, L^* \rangle = 0, \langle v^*, w\rangle = - \langle w, v^* \rangle = v^*(w).
  \]
  \(L\) is a maximal Lagrangian space and by basic linear algebra all examples are of this form. 

  Define the \emph{Heisenberg Lie algebra}\index{Heisenberg Lie algebra} \(H_W = W \oplus k.c\) with Lie brackets
  \begin{align*}
    [w, w'] &= \langle w, w' \rangle . c \\
    [c, w] &= 0
  \end{align*}
\end{eg}

\begin{ex}\leavevmode
  \begin{enumerate}
  \item Show \(H_W\) is a Lie algebra.
  \item Show \(H_W\) is nilpotent. Do we have to do any extra work?
  \end{enumerate}
\end{ex}

Differentiating the Heisenberg group

This is the most important nilpotent Lie algebra that arises in nature For example take \(k = \C, L = \C\). \(H_W\) has basis \(p, q, c\) with \([p, q] = c, [c, *] = 0\).

\begin{ex}
  Show \(\C[x]\) is a rep of \(H_W\) where \(q\) acts by multiplication by \(x\), \(p = \frac{\partial  }{\partial x}\) and \(c\) is identity.
\end{ex}

For a general vector space \(L\) with basis \(v_1, \dots, v_n\) and \(L^*\) with dual basis \(v_1^*, \dots, v_n^*\). Then \(H_W\) acts \(\C[x_1, \dots, x_n]\) with \(v_i^* \mapsto \frac{\partial }{\partial x_i}, v_i \mapsto x_i, c \mapsto 1\).

\begin{ex}\leavevmode
  \begin{enumerate}
  \item Subalgebras and quotient Lie algebras of a solvable Lie algebra are solvable.
  \item Subalgebras and quotient Lie algebras of a nilpotent Lie algebra are nilpotent.
  \item Let \(\Lie g\) be a Lie algebra and \(\Lie h \subseteq \Lie g\) an ideal. Then \(\Lie g\) is solvable if and only if \(\Lie h\) and \(\Lie g/\Lie h\) are solvable. In particular solvable Lie algebras are built out of one-dimensional abelian Lie algebras, i.e.\ there is a refinement of the derived series such that all subquotients are one-dimensional (and hence abelian).
  \item \(\Lie g\) is nilpotent if and only if centre of \(\Lie g\) is non-zero and the quotient of \(\Lie g\) by its centre is nilpotent.
    
    For only if, indeed if \(\Lie g\) is nilpotent we have central series
    \[
      \Lie g \supsetneq \Lie g^1 \supsetneq \cdots \supsetneq \Lie g^n = 0
    \]
    and since \(\Lie g^n = [\Lie g^{n - 1}, \Lie g] = 0\) so must have \(\Lie g^{n - 1}\) contained in the centre of \(\Lie g\).
  \item \(\Lie g\) is nilpotent if and only if \(\ad (\Lie g) \subseteq \Lie{gl}(\Lie g)\) is a nilpotent Lie algebra. This is immediate from 4 as we have a short exact sequence of Lie algebras
    \[
      \begin{tikzcd}
        0 \ar[r] & \text{centre of } \Lie g \ar[r] & \Lie g \ar[r] & \ad(\Lie g) \ar[r] & 0
      \end{tikzcd}
    \]
  \end{enumerate}
\end{ex}

We will use but not prove

\begin{theorem}[Lie]\index{Lie's theorem}
  Let \(k = \overline k\) and \(\cha k = 0\). Let \(\Lie g\) be a solvable Lie algebra over \(k\) and \(\Lie g \subseteq \Lie{gl}(V)\) for some \(V\). Then there exists a basis \(v_1, \dots, v_n\) of \(V\) with respect to which all element of \(\Lie g\) are upper triangular, i.e.\ \(\Lie g \subseteq \Lie b\).
\end{theorem}
Note that \(\Lie g \subseteq \Lie{gl}(V)\) is automatic by Ado.

Equivalently, there exists a linear function \(\lambda: \Lie g \to k\) and an element \(v \in V\) such that \(xv = \lambda(x) v\) for all \(x \in \Lie g\), that is \(\Lie g\) has a one dimensional subrep. In particular the only irreducible finite dimensional reps of \(\Lie g\) are one dimensional.

\begin{ex}
  Show these two formulations are equivalent.
\end{ex}

\begin{ex}\leavevmode
  \begin{enumerate}
  \item Show the theorem is false if \(k \neq \overline k\).
  \item Show the theorem is false if \(\cha k = p > 0\). Hint: consider the 3 dimensional Heisenberg Lie algebra \(H\) and show that \(k[x]/(x^p)\) is an irreducible rep of \(H\) of dimension larger than \(1\).
  \end{enumerate}
\end{ex}

\begin{corollary}
  If \(\cha k = 0\) and \(\Lie g\) is solvable then \([\Lie g, \Lie g]\) is nilpotent.
\end{corollary}

\begin{proof}
  It is an exercise to show that \(\Lie b\) solvable over \(k\) if and only if \(\Lie b \otimes_k \overline k\) is solvable over \(\overline k\), and similarly for nilpotents (?), so we may assume \(k = \overline k\). Now apply Lie's theorem to the adjoint rep \(\Lie g \to \End \Lie g\) so there exists a basis where \(\ad \Lie g\) are upper triangular. Then \([\ad \Lie g, \ad \Lie g]\) is strictly upper triangular. As \(\ad: \Lie g \to \Lie{gl}(\Lie g)\) is a rep, \([\ad \Lie g, \ad \Lie g] = \ad [\Lie g, \Lie g]\) so \(\ad [\Lie g, \Lie g]\) is nilpotent, hence \([\Lie g, \Lie g]\) is nilpotent since a Lie algebra \(\Lie h\) is nilpotent if and only if \(\ad \Lie h\) is nilpotent. (?)
\end{proof}

\begin{ex}
  Show this is false in characteristic \(p\).
\end{ex}

\begin{theorem}[Engel]\index{Engel's theorem}
  \(\Lie g\) is nilpotent if and only if for all \(x \in \Lie g\), \(\ad(x)\) is nilpotent. Equivalently, if \(V\) is a finite dimensional rep of a Lie algebra \(\Lie g\) and for all \(x \in \Lie g\), \(x\) acts on \(V\) as a nilpotent operator, then there exists \(v \in V\) such that \(xv = 0\) for all \(x \in \Lie g\). In otherwords, \(V\) has a 1 dimensional subrep which is the trivial rep. Equivalently, there exists a basis of \(V\) if \(\Lie g \subseteq \Lie{gl}(V)\) with respect to which all matrices in \(\Lie g\) are strictly upper triangular.
\end{theorem}

\begin{ex}
  Show these are all equivalent.
\end{ex}

Engel says \(V\) is built out of trivial reps. That is \(V\) has a composition series whose subquotients are trivial reps.

Warning: Engel says is \(\Lie g\) consists of nilpotent matrices then \(\Lie g\) is a nilpotent Lie algebra. The converse is \emph{false}, for example the abelian Lie algebra of scalar matrices. The correct converse is: \(\Lie g\) is nilpotent then \(\Lie g/\text{center}\) (which is isomorphic to \(\ad \Lie g\)) consists of nilpotent matrices.

\begin{definition}[invariant symmetric bilinear form]\index{invariant symmetric bilinear form}
  A symmetric bilinear form \((\cdot, \cdot): \Lie g \times \Lie g \to k\) is \emph{invariant} if \(([x, y], z) = (x, [y, z])\) for all \(x, y, z \in \Lie g\).
\end{definition}

\begin{ex}
  Show if \(G\) is an algebraic group actions on a vector space \(V\) and \((gx, gy) = (x, y)\) for all \(g \in G, x, y \in V\) then this defines an invariant form on \(V\).
\end{ex}

\begin{ex}
  If \(\mathfrak a \subseteq \Lie g\) is an ideal and \((\cdot, \cdot)\) is an invariant symmetric bilinear form then \(\mathfrak a^\perp\) is an ideal.
\end{ex}

\begin{definition}[trace form]\index{trace form}
  If \(\rho: \Lie g \to \Lie{gl}(V)\) is a rep, define the \emph{trace form} of \(V\) to be
  \[
    (x, y)_V = \tr (\rho(x) \rho(y)).
  \]
\end{definition}

\begin{ex}
  Show \((\cdot, \cdot)_V\) is an invariant symmetric bilinear form.
\end{ex}

\begin{definition}[Killing form]\index{Killing form}
  The \emph{Killing form} of a Lie algebra \(\Lie g\) is the trace form of the adjoint rep, i.e.
  \[
    (x, y)_{\ad} = \tr (\ad x \ad y).
  \]
\end{definition}

The third theorem that we are not going to prove:

\begin{theorem}[Cartan's criteria]\index{Cartan's criteria}
  Suppose \(\cha k = 0\) and \(\Lie g \subseteq \Lie{gl}(V)\). Then \(\Lie g\) is solvable if and only if for all \(x \in \Lie g, y \in [\Lie g, \Lie g]\), the trace form \((x, y)_V = 0\). That is \([\Lie g, \Lie g] \subseteq \Lie g^\perp\).
\end{theorem}

\begin{ex}
  Show only if is immediate from Lie's theorem. Idea: if \(\Lie g\) is solvable then we have a basis with \(x\) upper triangular and \(y\) strictly upper triangular, so \(xy\) has \(0\) entries on the diagonals and so has trace \(0\).
\end{ex}

\begin{corollary}
  If \(\cha k = 0\) then \(\Lie g\) is solvable if and only if \((\Lie g, [\Lie g, \Lie g])_{\ad} = 0\).
\end{corollary}

\begin{proof}
  If \(\Lie g\) is solvable then Lie's theorem says that \((\Lie g, [\Lie g, \Lie g])_{\ad} = 0\). Conversely Cartan says \(\ad \Lie g = \Lie g/\text{centre}\) is solvable so \(\Lie g\) is solvable.
\end{proof}

\begin{ex}
  Show now every invariant symmetric bilinear form on \(\Lie g\) is a trace form. More precisely, let \(\Lie g = \tilde H\) where \(\tilde H\) has basis \(c, p, q, d\) with
  \[
    [c, \tilde H] = 0, [p, q] = c, [d, p] = p, [d, q] = -q.
  \]
  \begin{enumerate}
  \item Show \(\tilde H\) is solvable.
  \item Construct a non-degenerate invariant form on \(\tilde H\).
  \item Why couldn't we just write use \(H\)?
  \item Extend the rep of \(H\) on \(k[x]\) to a rep of \(\tilde H\).
  \end{enumerate}
\end{ex}

Now we can use the theorems.

\begin{definition}[semisimplicity]\index{semisimplicity}
  \(\Lie g\) is \emph{semisimple} if is a sum of simple (non-abelian) Lie algebras.
\end{definition}

\begin{definition}[radical]\index{radical}
  The \emph{radical} of \(\Lie g\), \(R(\Lie g)\), is the maximal solvable ideal in \(\Lie g\).
\end{definition}

\begin{ex}\leavevmode
  \begin{enumerate}
  \item Show the sum of solvable ideals in \(\Lie g\) is solvable and hence \(R(\Lie g)\) is just the sum of all solvable ideals in \(\Lie g\).
  \item Show \(R(\Lie g/R(\Lie g)) = 0\).
  \end{enumerate}
\end{ex}

\begin{theorem}
  \label{thm:semisimplicity criteria}
  Suppose \(\cha k = 0\). Then TFAE:
  \begin{enumerate}
  \item \(\Lie g\) is semisimple.
  \item \(R(\Lie g) = 0\).
  \item Killing criterion: the Killing form is non-degenerate.
  \end{enumerate}
  Moreover if \(\Lie g\) is semisimple then every derivation \(D: \Lie g \to \Lie g\) is inner.
\end{theorem}
The converse of the last statement is \emph{false}.

\begin{definition}[derivation]\index{derivation}
  A \emph{derivation} is a linear map \(D: \Lie g \to \Lie g\) such that
  \[
    D[x, y] = [Dx, y] + [x, Dy].
  \]
\end{definition}

\begin{eg}
  If \(x \in \Lie g\) then \(\ad x\) is a derivation. Derivations of this form are called \emph{inner}\index{derivation!inner}.
\end{eg}

More generally if \(V\) is a rep of \(\Lie g\) then \(D: \Lie g \to V\) is a derivation if
\[
  D[x, y] = x Dy - y Dx
\]
and if \(v \in V\), \(x \mapsto xv\) is a derivation. Such a derivation is called inner. We define \(H^1(\Lie g, V)\) to be the quotient \(\operatorname{Der}(\Lie g, V)\) by the inner derivations. Thus the theorem says that \(\Lie g\) is semisimple implies that \(H^1(\Lie g, \Lie g) = 0\), but the converse is false. This is the subject of Lie algebra cohomology.

\begin{remark}
  If \(\Lie g\) is a Lie algebra over \(k\) where \(\cha k = 0\) then consider the SES
  \[
    \begin{tikzcd}
      0 \ar[r] & R(\Lie g) \ar[r] & \Lie g \ar[r] & \Lie g/R(\Lie g) \ar[r] & 0
    \end{tikzcd}
  \]
  The theorem says that \(\Lie g/R(\Lie g)\) is semisimple as its radical is \(0\). We are going to classify all the semisimple Lie algebras. As \(R(\Lie g)\) is solvable, this makes the theory particularly nice.
\end{remark}

It's helpful to mention that

\begin{theorem}[Levi]
  The above exact sequence splits, that is there exists a subalgebra \(\Lie h \subseteq \Lie g\) with \(\Lie h \to \Lie g/R(\Lie g)\). This subalgebra is not canonical, i.e.\ not an ideal, but his does say semidirect product.
\end{theorem}

\begin{ex}
  Show Levi's theorem fails in characteristic \(p\). Let \(\Lie g = \Lie{sl}_p(\overline F_p)\). Show \(R(\Lie g) = \overline F_p \cdot I\) but there is no complement to \(R(\Lie g)\) which is a subalgebra.
\end{ex}

\begin{proof}[Proof of \Cref{thm:semisimplicity criteria}]
  Claim \(\R(Lie g) = 0\) if and only if \(\Lie g\) has non-zero abelian ideals.
  \begin{proof}
    Only if is easy as an abelian ideal is solvable. For if, the derived series of \(R(\Lie g)\) is (defines?) a sequence of ideals of \(\Lie g\) and the last term is abelian.
  \end{proof}

  \(3 \implies 2\): we show that if \(\Lie a \subseteq \Lie g\) is an abelian ideal then \(\Lie a \subseteq \Lie g^\perp\) where the perp is with respect to the Killing form.
  \begin{proof}
    Take a vector space complement \(\Lie h\) to \(\Lie a\) in \(\Lie g\) so \(\Lie g = \Lie a \oplus \Lie h\). If \(x \in \Lie g\) then \(\ad x\) is block upper triangular and if \(a \in \Lie a\) then as \([\Lie a, \Lie a] = 0\) so \(\ad a\) ia block strictly upper triangular so \((a, x)_\ad = \tr \ad_a \ad_x = 0\).
  \end{proof}

  \(2 \implies 3\): let \(\Lie r \subseteq \Lie g^\perp\) be an ideal of \(\Lie g\) (for example \(\Lie r = \Lie g^\perp\) and  suppose \(\Lie r \neq 0\). Then \(R(\Lie g) = 0\) implies that centre of \(\Lie g\) is zero (?) so \(\Lie r \subseteq \Lie{gl}(\Lie g)\) as \(\ad: \Lie g \to \Lie{gl}(\Lie g)\) is injection and as \(Lie r \subseteq \Lie g^\perp\), \((x, y)_\ad = 0\) for all \(x, y \in \Lie g\). In particular for all \(y \in [\Lie r, \Lie r]\), so Carton's criteria implies that \(\Lie r\) is solvable, contradiction. Thus \(R(\Lie g) = 0\).

  \begin{ex}
    Show \(R(\Lie g) \supseteq \Lie g^\perp \supseteq [R(\Lie g), R(\Lie g)]\) in general for \(\cha k = 0\).
  \end{ex}

  \(2, 3 \implies 1\): Assume the Killing form is nondegenerate and let \(\Lie s \subseteq \Lie g\) be a minimal non-zero ideal. Observe that \((\cdot, \cdot)_\ad|_{\Lie s}\) is either non-degenerate or \(0\): the kernel is \(\{x \in \Lie s: (x, s)_\ad = 0\} = \Lie s \cap \Lie s^\perp\) which is an intersection of ideals, and we assumed \(\Lie s \neq 0\) is minimal. But if it is zero then by Cartan \(\Lie s\) is solvable so \(R(\Lie g) \neq 0\), contradiction. Thus \((\cdot, \cdot)_\ad|_{\Lie s}\) is non-degenerate and hence we get a direct sum decomposition \(\Lie g = \Lie s \oplus \Lie s^\perp\). Note \(\Lie s\) is not abelian as \(R(\Lie g) = 0\) and \(\Lie s\) is minimal implies \(\Lie s\) is simple. Moreover \(R(\Lie g) = 0\) implies \(R(\Lie s^\perp) = 0\) (exercise) and we can conclude by induction on \(\dim \Lie g\) as \(\Lie s^\perp\) is a Lie algebra of smaller dimension with \(R(\Lie s^\perp) = 0\).

  \(1 \implies 2\): exercise: show that if \(\Lie g\) is semisimple then \(\Lie g\) is a direct sum of minimal ideals in a unique way. In particular show if \(\Lie g = \bigoplus_{i = 1}^r \Lie s_i\) where \(\Lie s_i\)'s are minimal ideals of \(\Lie g\) and if \(\Lie b\) is a minimal ideal of \(\Lie g\), show \(\Lie b = \Lie s_i\) for some \(i\) (hint: consider \(\Lie b \cap \Lie s_i\) for all \(i\)). Derive as a corollary \(1 \implies 2\).

  Finally let \(D: \Lie g \to \Lie g\) be a derivation with \(\Lie g\) semisimple. Consider the linear map
  \begin{align*}
    \ell: \Lie g &\to k \\
    x &\mapsto \tr (d \ad x: \Lie g \to \Lie g)
  \end{align*}
  As \((\cdot, \cdot)_\ad\) is non-degenerate, exists \(y \in \Lie g\) such that \(\ell(x) = (y, x)_\ad\) for all \(x \in \Lie g\). Would like to show \(E = D - \ad y = 0\): enough to show \((Ex, z)_\ad = 0\) for all \(x, z \in \Lie g\). But
  \[
    \ad(Ex) = E \ad x - \ad x E = [E, \ad x]
  \]
  as
  \[
    \ad(Ex)(z) = [Ex, z] = E[x, z] - [x, Ez]
  \]
  since \(E\) is a derivation. Hence
  \begin{align*}
    (Ex, z)_\ad
    &= \tr(\ad(Ex) \ad(z)) \\
    &= \tr([E, \ad x], \ad z) \\
    &= \tr(E, [\ad x, \ad z]) \\
    &= \tr(E, \ad[x, z]) \\
    &= (E, [x, z])_\ad
  \end{align*}
  But by the definition of \(E\), \((E, a)_\ad = 0\) for all \(a \in \Lie g\), proving the result.
\end{proof}

\begin{ex}\leavevmode
  \begin{enumerate}
  \item If \(\Lie n\) is a nilpotent Lie algebra then there exists a non-inner derivation \(D: \Lie n \to \Lie n\).
  \item Let \(\Lie g = \langle x, y\rangle, [x, y] = y\). Show this has only inner derivations (so this doesn't characterise semisimple Lie algebras).
  \end{enumerate}
\end{ex}

\section{Structure theory of semisimple Lie algebras}

\begin{ex}\leavevmode
  \begin{enumerate}
  \item Let \(\Lie g\) be a simple Lie algebra with two nondegenerate symmetric bilinear forms \((\cdot, \cdot)_1, (\cdot, \cdot)_2\). SHow exists \(\lambda \in k^*\) such that \((\cdot, \cdot)_1 = \lambda (\cdot, \cdot)_2\) (\(\cha k = 0, k = \overline k\)).
  \item Let \(\Lie g = \Lie{sl}_n(\C)\). Then there are two such forms: the Killing form and \((A, B) \mapsto \tr AB\). Find \(\lambda\).
  \end{enumerate}
\end{ex}

\begin{definition}[torus]\index{torus}\index{maixmal torus}
  Let \(\Lie g\) be a Lie algebra. A \emph{torus} \(\Lie t \subseteq \Lie g\) is an abelian subalgebra such that for all \(t \in \Lie t\), \(\ad t: \Lie g \to \Lie g\) is a diagonalisable linear map. A \emph{maximal torus} is a torus not contained in any strictly bigger torus.
\end{definition}

\begin{eg}
  Let \(G\) be an algebraic group, \(T = (\C^*)^r \subseteq G\) a subgroup. Then \(\mathrm{Lie}(T)\) is a torus in \(\mathrm{Lie}(G)\).
\end{eg}

\begin{ex}\leavevmode
  \begin{enumerate}
  \item If \(\Lie g = \Lie{gl}_n\) then \(\Lie t\) of diagonal matrices in \(\Lie g\) is a maximal torus. Show the same for \(\Lie{sl}_n\).
  \item Show \(
    \begin{psmallmatrix}
      0 & * \\
      0 & 0
    \end{psmallmatrix}
    \subseteq \Lie{sl}_2\) is not a torus.
  \end{enumerate}
\end{ex}

If \(V\) is a vector space, \(t_1, \dots, t_r: V \to V\) are pairwise commuting linear maps, \(\lambda_1, \dots, \lambda_r \in \C^r\). Define
\[
  V_{(\lambda_1, \dots, \lambda_r)} = \{v \in V: t_i v = \lambda_i v \text{ for all } i\},
\]
the simultaneous eigenspace.

\begin{lemma}
  If each \(t_i\) is diagonalisable then \(V = \bigoplus_{\lambda \in \C^r} V_\lambda\).
\end{lemma}

\begin{proof}
  Induction on \(R\). If \(r = 1\) this is the assumption \(t_1\) is diagonalisable. For \(r > 1\), induction gives
  \[
    V = \bigoplus_{(\lambda_1, \dots, \lambda_{r - 1}) \in \C^{r - 1}} V_{(\lambda_1, \dots, \lambda_{r - 1})}
  \]
  and now \(t_r\) commutes with each of \(t_1, \dots, t_{r - 1}\) so preserves this eigenspace decomposition, so decomposes each \(V_{(\lambda_1, \dots, \lambda_{r - 1})}\) into eigenspaces for \(t_r\).
\end{proof}

Recap: let \(\Lie t\) be an abelian Lie algebra with basis \(t_1, \dots, t_n\), \(k = \overline k\). Then
\begin{enumerate}
\item a rep \(V\) of \(\Lie t\) is irreducible if and only if \(\dim V = 1\), exists \(\lambda \in \Lie t^* = \Hom(\Lie t, k)\), \(t v = \lambda(t) v\). \(\lambda_i = \lambda(t_i)\) is the eigenvalue of \(t_i\).
\item \(V\) is a direct sum of irreducible reps if and only if each \(t_i\) is diagonalisable.
\end{enumerate}

Define \(\Lie t \subseteq \Lie g\) to be the maximal torus. If \(V\) is a rep of \(\Lie t^*\). Write \(V_\lambda\) for the \(\lambda\)-weight space of \(V\).

\begin{corollary}
  \(\Lie g = \Lie g + \bigoplus_{\lambda \in \Lie t^*} \Lie g_\lambda\)
\end{corollary}

\begin{definition}
  We define the roots of \(\Lie g\) to be \(R = \{\lambda \in \Lie t^*: \Lie g_\lambda \neq 0\}\).
\end{definition}

\begin{eg}
  Let \(\Lie g = \Lie{sl}_n\), \(\Lie t\) the diagonal matrices, i.e.\ the trace \(0\) diagonal matrices. Let \(t\) be the diagonal matrix with diagonal entries \(t_1, \dots, t_n\), \(E_{ij}\) with \(1\) at \(ij\)th entry and \(0\) elsewhere (matrix units). Then \([t, E_{ij}] = (t_i - t_j) E_{ij}\). Define linear maps \(\varepsilon_i: \Lie t \to k, ... \mapsto t_i\). Then \(\varepsilon_i\) span \(\Lie t^*\). Also as \(\Lie t \subseteq k^n\), we have \((k^n)^* \surj \Lie t^*\). \((k^n)^*\) has basis \(\varepsilon_i\), so this is quotient by \(\varepsilon_1 + \dots + \varepsilon_n = 0\).

  Also \(\Lie g_0 = \Lie t, \Lie g_{\varepsilon_i - \varepsilon_j} = k \cdot E_{ij}\) and so \(\Lie{sl}_n\) has root space decomposition
  \[
    \Lie{sl}_n = \Lie t \oplus \bigoplus_{i \neq j} \Lie g_{\varepsilon_i - \varepsilon_j}.
  \]
\end{eg}

\begin{ex}
  Essential exercise. Suppose \(k = \overline k, \cha k \neq 2\) (can take \(k = \C\)). Compute the root space decomposition for \(\Lie g = \Lie{sl}_n, \Lie{so}_{2n + 1}, \Lie{so}_{2n}, \Lie{sp}_{2n}\) with \(\Lie t\) the diagonal matrices in \(\Lie g\). Note we use the bilinear form defining \(\Lie{so}_n\) to be \(\Lie{so}_n = \{A: JA + A^TJ = 0\}\) where \(J\) is the antidiagonal matrix with entries \(1\). Check that \(\Lie t\) is indeed a maximal torus.

  Subexercise: show this \(\Lie{so}_n\) is the same as \(\{A + A^T = 0\}\) by showing all nondegenerate orthogonal forms are equivalent.
\end{ex}

\begin{proposition}
  \(\Lie{sl}_n \C\) is a simple Lie algebra.
\end{proposition}

\begin{proof}
  Suppose \(\Lie r \subseteq \Lie{sl}_n \C = \Lie t \oplus \bigoplus_{\alpha \in R} \Lie g_\alpha\) is a nonzero ideal. We must show \(\Lie r = \Lie g\). Choose \(r \neq 0, r \in \Lie r\) such that \(\Lie r = \Lie t + \sum_\alpha e_\alpha\) with \(e_\alpha \in \Lie g_\alpha\) with the minimal number of non-zero terms. First suppose \(\Lie t \neq 0\). Choose \(\alpha \in \Lie t\) such that \(\alpha(t_0) neq 0\) for all \(\alpha \in R\), that is choosing a diagonal matrix with disinct eigenvalues. Consider \([t_0, r] \in \Lie r\), \([t_0, r] = \sum \alpha(t_0) e_\alpha\). If nonzero this has fewer terms than \(r\), absurd. Thus \(e_\alpha = 0\) for all \(\alpha \in R\), i.e.\ \(r = t \in \Lie t\). But \(t \neq 0\) so exists \(\alpha \in R\) with \(\alpha(t) \neq 0\). (as \(\alpha(t) = 0\) for all \(\alpha = \varepsilon_i - \varepsilon_j\) is saying \(t\) is \(\lambda I\), but \(\tr \lambda I = n\lambda \neq 0\). Phrase in another way: \(R\) spans \(\Lie t^*\))

  Thus \([t, e_\alpha] = \alpha(t) e_\alpha \neq 0 \in \Lie r\) so \(e_\alpha \in \Lie r\). But \(\alpha = \varepsilon_i - \varepsilon_j\) for some \(i \neq j\), so this says \(E_{ij} \in \Lie r\). But \([E_{ij}, E_{jk}] = E_{ik}\) if \(k \neq i\) and \([E_{si}, E_{ij}] = E_{sj}\) if \(s \neq j\). Hence \(E_{ab} \in \Lie r\) for all \(a \neq b\). Finally
  \[
    [E_{i, i + 1}, E_{i + 1, i}] = E_{ii} - E_{i + 1, i + 1} \in \Lie r
  \]
  so we've just seen a basis for \(\Lie{sl}_n\) is in \(\Lie r\).

  Finally if \(r = t + \sum e_\alpha\) and \(t = 0\). If there is one term in this expression, i.e.\ \(r = c E_{ij}\) for some \(c \neq 0\), we are done as above. Otherwise
  \[
    r = e_\alpha + e_\beta + \sum_{\gamma \in R\setminus\{\alpha, \beta\}} e_\alpha
  \]
  for some \(\alpha \neq \beta\). Choose \(t_0 \in \Lie t\) such that \(\alpha(t_0) \neq \beta(t_0)\). Then some linear combination of \([t_0, r]\) and \(r\) is nonzero with fewer terms, absurd.

  Key ingedient: \([E_{ij}, E_{jk}] = \dots\) Combinatorial.
\end{proof}

\begin{proposition}
  Let \(\Lie g\) be a semisimple Lie algebra over \(\C\). Then
  \begin{enumerate}
  \item non-zero maximal tori \(\Lie t\) exist.
  \item \(\Lie t = \Lie g_0 = \{x \in \Lie g: [t, x] = 0 \text{ for all } t \in \Lie t\}\), that is, such \(\Lie t\) are maximal abelian.
  \item Will state more precisely later: any two such \(\Lie t\) are conjugate by an element of algebraic group \(G\) of automorphisms of \(\Lie g\).
  \end{enumerate}
\end{proposition}

\begin{proof}
  Omitted, for lack of time.
\end{proof}

Hence \(\Lie g = \Lie g \oplus \bigoplus_{\alpha \in R} \Lie g_\alpha\), as we have seen by hand for the classical Lie algebras \(\Lie{sl}_n, \Lie{so}_n, \Lie{so}_{2n}\).

\begin{theorem}[structure theorem for semisimple Lie algebras, part 1]
  Let \(\Lie g\) be a semisimple Lie algebra over \(\C\), \(\Lie g = \Lie t \oplus \bigoplus_{\alpha \in R} \Lie g_\alpha\). Then
  \begin{enumerate}
  \item the roots span \(\Lie t^*\).
  \item \(\dim \Lie g_\alpha = 1\) for all \(\alpha \in R\).
  \item If \(\alpha, \beta \in R\) and \(\alpha, \beta \in R\) then \([\Lie g_\alpha, \Lie g_\beta] = \Lie g_{\alpha + \beta}\). If \(\alpha + \beta \notin R\) and \(\alpha \ne -\beta\) then \([\Lie g_\alpha, \Lie g_\beta] = 0\).
  \item \([\Lie g_\alpha, \Lie g_{-\alpha}] \subseteq \Lie t\) is one-dimensional and \(\Lie g_\alpha + [\Lie g_\alpha, \Lie g_{-\alpha}] + \Lie g_{-\alpha}\) is a Lie subalgebra of \(\Lie g\), isomorphic to \(\Lie{sl}_2\). In particular if \(\alpha \in R\) then \(-\alpha \in R\).
  \end{enumerate}
\end{theorem}

\begin{ex}
  Check this for classical Lie algebras.
\end{ex}

\begin{proof}
  Suppose not. Then there exists \(t \in \Lie t^*\) such that \(\alpha(t) = 0\) for all \(\alpha \in R\). But then if \(x \in \Lie g_\alpha\), \([t, x] = \alpha(t) x = 0\) so \([t, \Lie g] = 0\), i.e.\ \(t\) is in the centre of \(\Lie g\). But \(\Lie g\) is semisimple so has no nontrivial abelian ideals.

  We now prove a sequence of results which implies most of them. If \(\lambda, \mu \in \Lie t^*\) then \([\Lie g_\lambda, \Lie g_\mu] \subseteq \Lie g_{\lambda + \mu}\).

  \begin{proof}
    If \(x \in \Lie g_\lambda, y \in \Lie g_\mu, t \in \Lie t\) then
    \begin{align*}
      [t, [x, y]] &= [[t, x], y] + [x, [t, y]] \\
                  &= \lambda(t) [x, y] + \mu(t) [x, y] \\
                  &= (\lambda + \mu) (t) [x, y]
    \end{align*}
    Hence if \(\alpha, \beta \in R\) but \(\alpha + \beta \neq 0\) and \(\alpha + \beta \notin R\) (so \(\Lie g_{\alpha + \beta} = 0\)) then \([\Lie g_\alpha, \Lie g_\beta] = 0\) and if \(\alpha + \beta \in R\) then \([\Lie g_\alpha, \Lie g_\beta] \subseteq \Lie g_{\alpha + \beta}\). If \(\alpha + \beta = 0\) then \([\Lie g_\alpha, \Lie g_\beta] \subseteq \Lie t\). Note we will not show \([\Lie g_\alpha, \Lie g_\beta] = \Lie g_{\alpha + \beta}\) for a while.

  \end{proof}
  Secondly claim \((g_\lambda, g_\mu)_\ad = 0\) if \(\lambda + \mu \neq 0\) and \((\cdot, \cdot)_\ad|_{g_\lambda + g_{-\lambda}}\) is nondegenerate.

  \begin{proof}
    Let \(x \in g_\lambda, y \in g_\mu\). To show this is \(0\), it is enough to show \(\ad x \ad y\) is nilpotent (?). But
    \[
      (\ad x \ad y)^N g_\alpha \subseteq g_{\alpha + N(\lambda + \mu)}
    \]
    by the previous part. So if \(\lambda + \mu \neq 0\) then as \(g\) is finite dimensional, \(g_{\alpha + N(\lambda + \mu)} = 0\) for \(N >> 0\), showing \((x, y)_\ad = 0\).

    On the other hand, the Killing form is nondegenerate and \(g = \bigoplus_\lambda (g_\lambda + g_{-\lambda})\) is an orthogonal decomposition by what we just showed, so \((\cdot, \cdot)_\ad|_{g_\lambda + g_{-\lambda}}\) is nondegenerate.
  \end{proof}

  In particular take \(\lambda = 0\). Get \((\cdot, \cdot)_\ad|_{\Lie t}\) is non-degenerate. Hence we get an isomorphism \(v: \Lie t \to \Lie t^*\) by \(v(t)(t') = (t, t')_\ad\). Moreover this defines a symmetric bilinear form on \(\Lie t^*\) by \((v(t), v(t')) = (t, t')_\ad\) (make \(v\) an isometry).

  Claim if \(\alpha \in R\) then \(-\alpha \in R\): \((g_\alpha, g_\alpha)_\ad = 0\) as \(\alpha \neq 0\) implies \(2\alpha \neq 0\). But \((\cdot, \cdot)_\ad|_{g_\alpha + g_{-\alpha}}\) is non-degenerate (in particular so Killing form gives an isomorphism \(g_\alpha \cong g_{-\alpha}^*\)).

  Let \(x \in g_\alpha, y \in g_{-\alpha}\). Claim \([x, y] = (x, y)_\ad v^{-1}(\alpha)\).

  \begin{proof}
    \begin{align*}
      (t, [x, y])_\ad
      &= ([t, x], y)_\ad \\
      &= \alpha(t) (x, y)_\ad
    \end{align*}
  \end{proof}

  Pick \(e_\alpha \in g_\alpha, e_\alpha \ne 0\) and \(e_{-\alpha} \in g_{-\alpha}\) such that \((e_\alpha, e_{\alpha})_\ad \ne 0\). and consider \(M_:a = \langle e_\alpha, e_{-\alpha}, v^{-1}(\alpha) \rangle\). This is a 3 dimensional Lie algebra as
  \[
    [v^{-1}(\alpha), e_\alpha] = \alpha(v^{-1}(\alpha)) e_\alpha = (\alpha, \alpha) e_\alpha
  \]
  and similarly \([v^{-1}(\alpha), e_{-\alpha}] = -(\alpha, \alpha) e_\alpha\). So if \((\alpha, \alpha) \neq 0\) then define \(h_\alpha = \frac{2}{(\alpha, \alpha)} v^{-1}(\alpha)\) and rescale \(e_{-\alpha}\) so that \((e_\alpha, e_{-\alpha})_\ad = \frac{2}{(\alpha, \alpha)}\). It is an exercise to show that \(M_\alpha \to \Lie{sl}_2, e_\alpha, h, e_{-\alpha} \mapsto e, h, f\).

  Now we show if \(\alpha \in R\) then \((\alpha, \alpha) \neq 0\). Suppose otherwise, then \([M_\alpha, M_\alpha] = \C v^{-1}(\alpha)\) (or did we merely prove containment?), i.e.\ \(M_\alpha\) is a solvable Lie algebra. Hence by Lie's theorem, \(\ad[M_\alpha, M_\alpha]\) acts as nilpotent operators on \(\Lie g\), i.e.\ \(\ad v^{-1}(\alpha)\) is nilpotent. But \(v^{-1}(\alpha) \in \Lie t\) and hence diagonalisable. Together this implies \(\nu^{-1}(\alpha) = 0\). But \(\alpha \in R\) means \(\alpha \neq 0\), contradiction.

  Claim \(\dim \Lie g_{-\alpha} = 1\) for all \(\alpha \in R\).

  \begin{proof}
    Fix \(\alpha\). Pick \(\Lie m_\alpha \subseteq \Lie g\) so \(\Lie m_\alpha \cong \Lie{sl}_2\). If \(\dim \Lie g_{-\alpha} > 1\) then the map \(g_{-\alpha} \to \C \nu^{-1}(\alpha), x \mapsto \ad e_\alpha \cdot x\) has a non-zero kernel. So exists \(v \in g_{-\alpha}\) such that
    \begin{align*}
      \ad(e_\alpha) v &= 0 \\
      \ad(h_\alpha) v &= -\alpha(h_\alpha) . v = -2v
    \end{align*}
    Claim \(v\) is a highest weight vector for \(\Lie{sl}_2\) with negative highest weight. Hence the \(\Lie{sl}_2\)-submodule of \(\Lie g\) generated by \(v\) is infinite dimensional, conradiction.
  \end{proof}
\end{proof}

Guaranteed question on exam: explain everything about each classical Lie algebra.

\begin{theorem}[structure theorem, part II]\leavevmode
  \begin{enumerate}
  \item \(\frac{2(\alpha, \beta)}{(\alpha, \alpha)} \in \Z\) for all \(\alpha, \beta \in R\).
  \item If \(\alpha \in R\) and \(k \alpha \in R\) then \(k = \pm 1\).
  \item \(\bigoplus_{k \in \Z} \Lie g_{\beta + k \alpha}\). This is an irreducible module for \((\Lie{sl}_2)_\alpha = \Lie m_\alpha\). In particular
    \[
      \{k\alpha + \beta: k \in \Z, k \alpha + \beta \in \R \cup \{0\}\}
    \]
    is of the form \(\beta - p \alpha, \beta - (p - 1) \alpha, \dots, \beta + (p - 1)\alpha, \beta + q \alpha\) where \(p - q = \frac{2(\alpha, \beta)}{(\alpha, \alpha)}\). This is called the \emph{\(\alpha\) string through \(\beta\)}.
  \end{enumerate}
\end{theorem}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item Let \(q = \max \{k \in \Z: p + k \alpha \in R\}\) and let \(v \in \Lie g_{\beta + q \alpha} \setminus \{0\}\). Then \(\ad e_\alpha v \in \Lie g_{\beta + (q + 1)\alpha} = 0\) and
    \[
      \ad h_\alpha . v = (\beta + q\alpha) (h_\alpha) . v = \left( \frac{2(\beta, \alpha)}{(\alpha, \alpha)} + 2q \right) \cdot v
    \]
    Hence is a highest weight vector for \(\Lie{sl}_2\) with weight ... and this is a non-negative integer as \(\Lie g\) is finite dimensional.
  \item
  \item Structure of \(\Lie{sl}_2\)-modules implies that \((\ad e_{\alpha})^r v \neq 0\) for \(0 \leq r \leq N\) where \(N = \frac{2(\beta, \alpha)}{(\alpha, \alpha)} + 2q\) and \((\ad e_{-\alpha})^{N + 1}v = 0\). Hence
  \[
    \{\beta + (q - k) \alpha: 0 \leq k \leq N\}
  \]
  are all in \(R \cup \{0\}\) (in particular, non-zero eigenspaces). We need to show no other roots of the form \(\beta + k\alpha\). Repeat same construction from bottom up: \(p = \max \{k: \beta - k\alpha \in R \cup \{0\}\}\), \(w \in \Lie g_{\beta - p \alpha} \setminus \{0\}\) implies \(\ad e_{-\alpha} w = 0\).
  ... diagram and the strings coincide.
  \end{enumerate}
  For 2, apply 1 to \(\{\alpha, \beta\} = \{\alpha, k\alpha\}\) to get
  \[
    \frac{2(\alpha, k\alpha)}{k \alpha, k \alpha} = \frac{2}{k} \in \Z, \frac{2(k\alpha, \alpha)}{\alpha, \alpha)} = 2k \in \Z.
  \]
  Take \(\alpha = \beta\) in 2 (?) as \((sl_2)_\alpha = g_\alpha + [g_\alpha, g_\alpha] + g_{-\alpha}\) is an irreducible \((sl_2)_\alpha\)-module, 2 says it is a string though \(\alpha\) so \(g_{2\alpha} = 0 = g_{-2\alpha}\).

  Finally if \(\alpha, \beta, \alpha + \beta \in R\), we need to show \([g_\alpha, g_\beta] = g_{\alpha + \beta}\). But \(\bigoplus_{k \in \Z} g_{\beta + k \alpha}\) is an irreducible \(sl_2\)-module, so \(ad e_k: g_{\beta + k\alpha} \to g_{\beta + (k + 1)\alpha}\) is an iso if \(k < q\). But \(q \geq 1\) so in particular \(\ad e_\alpha: g_\beta \to g_{\beta + \alpha}\) is an iso.
\end{proof}

The statement of 3 is messsy. Here is a much cleaner consequence.

Given \(\alpha \in t^*\), define ``reflection''
\begin{align*}
  s_\alpha: t^* &\to t^* \\
  v &\mapsto v -\frac{2 (\alpha, v)}{(\alpha, \alpha)} \alpha
\end{align*}
Claim that 3 implies \(s_\alpha \beta \in R\) if \(\alpha, \beta \in R\).

\begin{proof}
  Let \(r = \frac{2(\alpha, \beta)}{(\alpha, \alpha)}\). If \(r \geq 0\) then \(p = q + r \geq r\). If \(r \leq 0\) then \(q = p - r \geq -r\). In either case \(\beta - r \alpha\) is the \(\alpha\)-string through \(\beta\). Exercise: show drawing is accurate (reflection sends \(\beta\) to \(s_\alpha \beta\).
\end{proof}

\begin{proposition}\leavevmode
  \begin{enumerate}
  \item If \(\alpha, \beta \in R\) then \((\alpha, \beta) \in \Q\).
  \item If we pick a basis \(\beta_1, \dots \beta_r\) of \(t^*\) with each \(\beta_i \in R\) then any \(\beta \in R\) is of the form \(\sum q_i \beta_i\) with \(q_i \in \Q\), that is the \(\Q\)-span of \(R\) has dimension equal to \(\dim_\C t\).
  \item \((\cdot, \cdot)\) is positive definite on \(\Q R\).
  \end{enumerate}
\end{proposition}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item As \(\frac{2(\alpha, \beta)}{(\alpha, \alpha)} \in \Z\) it is enough to show \((\beta, \beta) \in \Q\) for all \(\beta \in R\). Let \(t, t' \in t\), then
    \[
      (t, t')_\ad = \tr(\ad t \ad t': g \to g) = \sum_{\alpha \in R} \alpha(t) \alpha(t')
    \]
    by weight space decomposition. So if \(\lambda, \beta \in t^*\) then
    \begin{align*}
      (\lambda, \mu) &= (\nu^{-1}(\lambda), \nu^{-1}(\mu)) \\
                     &= \sum_{\alpha \in R} \alpha(\nu^{-1}(\lambda)) \alpha (\nu^{-1}(\mu)) \\
      &= \sum_{\alpha \in R} (\lambda, \alpha)(\mu, \alpha)
    \end{align*}
    In particular \((\beta, \beta) = \sum_{\alpha \in R} (\beta, \alpha)^2\). Multiply by \(\frac{4}{(\beta, \beta)^2}\), get
    \[
      \frac{4}{(\beta, \beta)} = \sum_{\alpha \in R} \left( \frac{2(\alpha, \beta)}{(\beta, \beta)} \right)^2 \in \Z.
    \]
  \item Let \(B\) be the grand matrix of \((\cdot, \cdot)\) on \(t^*\) with respect to basis \(\beta_i\), meaning \(B = [(\beta_i, \beta_j)]_{ij}\). It is an exercise to check \((\cdot, \cdot)\) is nondegenerate implies \(\det B \neq 0\). Let \(\beta = \sum c_i \beta_i \in R\) so \((\beta, \beta_i) = \sum_j c_j (\beta_j, \beta_i)\), that is
    \[
      \begin{pmatrix}
        (\beta, \beta_1) \\
        \vdots \\
        (\beta, \beta_r)
      \end{pmatrix}
      = B
      \begin{pmatrix}
        c_1 \\
        \vdots \\
        c_r
      \end{pmatrix}
    \]
    and as \(\det B \neq 0\) we can invert this. Then \(c_i \in \Q\).
  \item Let \(\lambda = \sum c_i \beta_i\) with \(c_i \in \Q\), so \((\lambda, \alpha) \in \Q\) for all \(\alpha \in R\). But \((\lambda, \lambda) = \sum_{\alpha \in R}(\lambda, \alpha)^2 \geq 0\) and \((\lambda, \lambda) = 0\) implies \((\lambda, \alpha) = 0\) for all \(\alpha \in R\). But \(R\) spans \(t^*\) and \((\cdot, \cdot)\) is nondegenerate so \(\lambda = 0\).
  \end{enumerate}
\end{proof}

\section{Root systems}

Let \(V\) be a vector space over \(\R\), let \((\cdot, \cdot): V \times V \to \R\) be an inner product, i.e.\ a positive definite symmetric bilinear form. If \(\alpha \in V, \alpha \neq 0\) let \(\alpha^\vee = \frac{2\alpha}{(\alpha, \alpha)}\) so \((\alpha, \alpha^\vee) = 2\). Define
\begin{align*}
  s_\alpha: V &\to V \\
  v &\mapsto v - (v, \alpha^\vee) \alpha
\end{align*}

\begin{lemma}
  \(s_\alpha\) is the reflection in the hyperplane orthogonal to \(\alpha\). In particular \(s_\alpha \alpha = -\alpha\) and all other eigenvectors of \(s_\alpha\) have eigenvalue \(1\). Moreover
  \[
    s_\alpha^2 = 1, \quad (s_\alpha + 1)(s_\alpha - 1) = 0
  \]
  and \(s_\alpha \in O(V, (\cdot, \cdot))\), the orthogonal group of \(V\) with respect to \((\cdot, \cdot)\), which is in particular an algebraic group.
\end{lemma}

\begin{proof}
  \(V = \R\alpha \oplus \alpha^\perp\) and if \(v \in \alpha^\perp\) then \(s_\alpha v = v\).
\end{proof}

\begin{definition}[root system]\index{root system}
  A \emph{root system} \(R\) in \(V\) is a finite set \(R \subseteq V\) such that
  \begin{enumerate}
  \item \(0 \notin R, \R R = V\),
  \item for all \(\alpha, \beta \in R\), \((\alpha, \beta^\vee) \in \Z\),
  \item for all \(\alpha \in R\), \(s_\alpha R \subseteq R\). In particular \(s_\alpha \alpha = -\alpha \in R\).
  \end{enumerate}
  Moreover \(R\) is \emph{reduced} if in addition \(\alpha, k \alpha \in R\) implies \(k = \pm 1\).
\end{definition}

\begin{eg}
  Let \(g\) be a semisimple Lie algebra over \(\C\). Then it has weight space decomposition \(g = t \oplus \bigoplus_{\alpha \in R} g_\alpha\). Then \(R\) is a root system.
\end{eg}

\begin{definition}[Weyl group]\index{Weyl group}
  Let \(W\) be the group generated by the reflection \(s_\alpha\) for \(\alpha \in R\). This is the \emph{Weyl group of \(R\)}.
\end{definition}

Claim that \(W\) is finite.

\begin{proof}
  \(W\) acts on \(R\) by permutations and as \(\R R = V\), this action is faithful (?), so \(W \subseteq \operatorname{Sym}(R)\) so finite.
\end{proof}

\begin{definition}
  The \emph{rank} of \(R\) is the dimension of \(V\).
\end{definition}

\begin{definition}
  An isomorphism of root systems between \((V, R)\) and \((V', R')\) is a linear bijection \(\phi: V \to V'\) such that \(\phi(R) = R'\).
\end{definition}
Note that we do not require this to be an isometry.

\begin{ex}
  If \((R, V), (R', V')\) are two root systems then so is \((R \amalg R', V \oplus V')\).
\end{ex}

A root system not isomorphic to a direct sum is called \emph{irreducible}.

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Rank 1: take \(V = \R, (x, y) = xy, R = \{\alpha, - \alpha\}\) with \(\alpha \in R, \alpha \neq 0\). \(W = \Z/2\). Exercise: this is the only rank \(1\) root system.
  \item rank 2
    \begin{enumerate}
    \item \(V = \R^2\) with usual inner product is a root system. This is called \(A_1 \times A_1\) and is not irreducible. \(W = \Z/2 \times \Z/2\).
    \item \((\alpha, \beta) = -1, \alpha = \alpha^\vee, \beta = \beta^\vee\) . \(W = S_3\). This is the root system for \(sl_3\). This is \(A_2\).
    \item \(B_2\). \(W = D_8\).
    \item \(\alpha = e_1, \beta = e_2 - e_1, (\alpha, \alpha) = 1, (\beta, \beta) = 1\). This is \(G_2\).
    \end{enumerate}
  \end{enumerate}
\end{eg}

\begin{ex}\leavevmode
  \begin{enumerate}
  \item Show these are root systems.
  \item Show they are all the rank 2 root systems.
  \item Show \(A_2, B_2, G_2\) are irreducible.
  \end{enumerate}
\end{ex}

\begin{ex}
  If \((R, V)\) is a root system then so is \((R^\vee, V)\) where \(R^\vee = \{\alpha^\vee: \alpha \in R\}\).
\end{ex}

\begin{definition}[simply laced]\index{simply laced}
  \(R\) is simply laced if all the roots have the samle length.
\end{definition}

\begin{ex}
  If \(R\) is a simply laced root system then \(R\) is isomorphic to a root system with \((\alpha, \alpha) = 2\) for all \(\alpha \in R\).
\end{ex}

\begin{definition}[lattice]\index{lattice}\index{root}
  A \emph{lattice} \(L\) is a finitely generated free abelian group (i.e. isomoprhic to \(\Z^\ell\) for some \(\ell\)) equipped with a form \((\cdot, \cdot): L \otimes L \to \Z\) such that the induced form \((\cdot, \cdot): L_\R \times L_\R \to \R\) is a positive definite symmetric bilinear form, where \(L_\R = L \otimes_\Z \R \cong \R^\ell\).

  A \emph{root} of \(L\) is a vector \(\alpha \in L\) with \((\alpha, \alpha) = 2\). We denote the set of roots of \(L\) by \(R_L\).
\end{definition}

\begin{ex}
  If \(\alpha \in R_L\) then \(s_\alpha(L) \subseteq L\).
\end{ex}

\begin{lemma}
  \(R_L\) is a simply laced root system in \(\R R_L\).
\end{lemma}

\begin{proof}
  Obvious except finiteness of \(R_L\). But \(R_L\) is the intersection of a compact set (the sphere \(\{\alpha \in \R L: (\alpha, \alpha) = 2\}\)) and a discrete set (\(L\)), so finite.
\end{proof}

\begin{definition}
  \(L\) is \emph{generated by roots} if \(\Z R_L = L\).
\end{definition}

Note if so, \(L\) is an ``even lattice'', i.e.\ \((\ell, \ell) \in 2\Z\) for all \(\ell \in L\).

\begin{eg}
  \(L = \Z \alpha\) with \((\alpha, \alpha) = 2\). If \(\lambda = 2\) then \(R_L = \{\pm \alpha\}\) and \(L = \Z R_L\). If \(\frac{k^2\lambda}{2} \neq 1\) for all \(k \in \Z\) then \(R_L = \emptyset\).
\end{eg}

We will now meet all simply laced lattices generated by roots.
\begin{enumerate}
\item \(A_n\) Consider \(\Z^{n + 1} = \bigoplus_{i = 1}^{n + 1} \Z e_i\), \((e_i, e_j) = \delta_{ij}\). This is the square lattice. Define
  \[
    L = \{\ell \in \Z^{n + 1}: (\ell, e_1 + \dots + e_{n + 1}) = 0\}
    = \{\sum a_i e_i: \sum a_i = 0\} \cong \Z^n
  \]
  then \(R_L = \{e_i - e_j: i \neq j, \# R_L = n(n + 1), \Z R_L = L\). If \(\alpha = e_i - e_j\) then \(s_\alpha\) waps \(i\)th and \(j\)th coordinate, i.e.
  \[
    s_\alpha(x_1e_1 + \dots + x_{n + 1} e_{n + 1}) = x_1e_1 + \dots + x_je_i + \dots + x_ie_j + \dots + x_{n + 1} e_{n + 1}
  \]
  so \(W = \langle s_{e_i - e_j}: i \neq j\rangle \cong S_{n + 1}\).

  \((R_L, L)\) is the root system of \(\Lie{sl}_{n + 1}\).

  \begin{ex}\leavevmode
    \begin{enumerate}
    \item Check all these statement, especially the one about root of \(\Lie{sl}_{n + 1}\).
    \item Draw \(L\) and \(R_L\) for \(n = 1, 2\). Check \(A_2, A_2\) are as produced earlier.
    \end{enumerate}
  \end{ex}
\item \(D_n\). Consider the square lattice \(\Z^n\) and define
  \begin{align*}
    R_L &= \{\pm e_i \pm e_j: i \neq j\} \\
    L &= \Z R_L = \{\sum a_i e_i: \sum a_i \text{ even}\}
  \end{align*}
  and \(s_{e_i - e_j}\) as before, and \(s_{e_i + e_j}\) flips signs of \(i\)th and \(j\)th coordinate. \(\# R_L = 2n(n + 1)\). Then \(W = (\Z/2)^{n - 1} \rtimes S_n\).

  \begin{ex}\leavevmode
    \begin{enumerate}
    \item Check all the claims.
    \item Show \(D_n\) is irreducible if \(n \geq 3\).
    \item \(D_3 \cong A_3, D_2 \cong A_1 \times A_1\).
    \item Roots of \(\Lie{so}_{2n}\) are of type \(D_n\).
    \end{enumerate}
  \end{ex}
\item \(E_8\). Let
  \[
    \Gamma_n = \{(k_1, \dots, k_n): \sum k_i \in 2\Z \text{ and either } k_i \in \Z \text{ or } k_i \in \Z + \frac{1}{1} \text{ for all } i\}
  \]
  with the usual inner product of \(\R^n\). Consider \(\alpha = (\frac{1}{2}, \dots, \frac{1}{2})\). Note \((\alpha, \alpha) = \frac{n}{4}\) so if \(\alpha \in \Gamma_n\) and \(\Gamma_n\) is an even lattice then \(8 \divides n\).

  \begin{ex}\leavevmode
    \begin{enumerate}
    \item Show \(\Gamma_{8n}\) is an even lattice.
    \item If \(n >1\), roots of \(\Gamma_{8n}\) are a root system of type \(D_n\).
    \item Show the roots of \(\Gamma_8\) are \(\{\pm e_i \pm e_j: i \neq j\} \cup \{\frac{1}{2}(\pm e_i  \pm \dots \pm e_8): \text{ even number of minus signs}\}\). Roots of \(\Gamma_8\) are called \emph{root system of type \(E_8\)}. \(\# R_{E_8} = \binom{8}{2} \cdot 4 + 128 = 240\), so by classification of semisimple Lie algebras the associated Lie algebra has dimension \(248\).
    \item Can you compute \(\# W_{E_8}\)? The answer is \(2^{14} \cdot 3^5 \cdot 5^2 \cdot 7\).
    \end{enumerate}
  \end{ex}
\end{enumerate}

\begin{ex}
  If \(R\) is a root system, \(\alpha \in R\) then \(\alpha^\perp \cap R \) is a root system.
\end{ex}













\printindex
\end{document}
