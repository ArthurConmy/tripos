\documentclass[a4paper]{article}

\def\npart{III}

\def\ntitle{Analytic Number Theory}
\def\nlecturer{T.\ F.\ Bloom}

\def\nterm{Lent}
\def\nyear{2019}

\input{header}

\usepackage{cancel}

\newtheorem*{fact}{Fact}

\begin{document}

\input{titlepage}

\tableofcontents

\setcounter{section}{-1}

\section{Introduction}

Analytic number theory is the study of numbers using analysis. In particular it answers quantitative questions. ``Numbers'' means natural numbers in this course, which excludes \(0\).

\begin{eg}\leavevmode
  \begin{enumerate}
  \item How many primes are there? We know there are infinitely many but can we have a more precise answer? Let \(\pi(x)\) be the number of primes smaller than or equal to \(x\). Then by the famous prime number theorem, \(\pi(x) \sim \frac{x}{\log x}\).
  \item How may twin primes are there? It is not known whethere there are infinitely many. From 2014 Zhang, Maynard, Polymath, there are infinitely many primes at most \(246\) apart. It's been conjectured that the asymptotic bound is \(\sim \frac{x}{(\log x)^2}\).
  \item How many primes are there congruent to \(a\) mod \(q\) where \((a, q) = 1\)? There are infinitely many by Dirichlet's theorem. The guess is \(\frac{1}{\varphi(q)}\frac{x}{\log x}\). This is known for small \(q\).
  \end{enumerate}
\end{eg}

The course is divided into four parts:
\begin{enumerate}
\item elementary techniques (using real analysis),
\item sieve methods,
\item Riemann zeta function/Prime number theorem (using complex analysis),
\item primes in arithmetic progression. 
\end{enumerate}

\section{Elementary techniques}

Review of asymptotic notations:
\begin{itemize}
\item Landau notation: \(f(x) = O(g(x))\) if there is \(C > 0\) such that \(|f(x)| \leq C|g(x)|\) for all large enough \(x\).
\item Vinogradov notation: \(f \ll g\) is the same as \(f = O(g)\).
\item \(f \sim g\) if \(\lim_{x \to \infty} \frac{f(x)}{g(x)} = 1\), i.e.\ \(f = (1 + o(1)) g\).
\item \(f = o(g)\) if \(\lim_{x \to \infty} \frac{f(x)}{g(x)} = 0\).
\end{itemize}

\subsection{Arithmetic functions}

These are just functions \(f: \N \to \C\). An important operation for multiplicative number theory is \emph{multiplicative convolution}\index{multiplicative convolution}
\[
  f * g(n) = \sum_{ab = n} f(a)g(b)
\]

\begin{eg}\leavevmode
  \begin{enumerate}
  \item \(1(n) = 1\) for all \(n\). Caution: this is not identity on \(\N\).
  \item \emph{Möbius function}\index{Möbius function}
    \[
      \mu(n) =
      \begin{cases}
        (-1)^k & \text{ if } n = p_1 \dots p_k \\
        0 & \text{ if \(n\) is divisible by a square}
      \end{cases}
    \]
  \item \emph{Liouville function}
    \[
      \lambda(n) = (-1)^k
    \]
    if \(n = p_1 \dots p_k\) where \(p_i\)'s are not necessarily distinct.
  \item \emph{divisor function}
    \[
      \tau(n) = \#d \text{ such that } d \divides n = \sum_{ab = n} 1 = 1 * 1(n).
    \]
    This is sometimes also denoted by \(d(n)\).
  \end{enumerate}
\end{eg}

\begin{definition}[multiplicative function]\index{multiplicative function}
  An arithmetic function \(f\) is \emph{multiplicative} if
  \[
    f(nm) = f(n)f(m)
  \]
  whenever \((n, m) = 1\).
\end{definition}

In particular a multiplicative function is determined by its values on prime powers \(f(p^k)\).

\begin{fact}
  If \(f\) and \(g\) are multiplicative then so is \(f * g\).
\end{fact}

\begin{eg}
  \(1, \mu, \lambda, \tau\) are multiplicative. \(\log n\) is not multiplicative.
\end{eg}

\begin{fact}[Möbius inversion]\index{Möbius inversion}
  \(1 * f = g\) if and only if \(\mu * g = f\). That is,
  \[
    \sum_{d \divides n} f(d) = g(n)
  \]
  if and only if
  \[
    \sum_{d \divides n} g(d) \mu(\frac{n}{d}) = f(n).
  \]

  For example
  \[
    \sum_{d \divides n} \mu(d) =
    \begin{cases}
      1 & n = 1 \\
      0 & \text{otherwise}
    \end{cases}
    = 1 * \mu (n)
  \]
  is multiplicative so enough to check the identity for prime powers. If \(n = p^k\) then \(\{d: d \divides n\} = \{1, p, \dots, p^k\}\) so LHS equals to \(1 - 1 + 0 + \dots = 0\) unless \(k = 1\) when LHS equals to \(\mu(1) = 1\).
\end{fact}

Our goal is to study primes. Our first might be that we shall work with
\[
  1_p(n) =
  \begin{cases}
    1 & n \text{ prime} \\
    0 & \text{otherwise}
  \end{cases}
\]
as then \(\pi(x) = \sum_{1 \leq n \leq x} 1_p(n)\). But this is very awkward to work with, as to begin with, this is not multiplicative. Instead, we are going to work almost exclusively with \emph{von Mangoldt function}\index{von Mangoldt function}
\[
  \Lambda(n) =
  \begin{cases}
    \log p & n = p^k \\
    0 & \text{otherwise}
  \end{cases}
\]
``assign weight \(\log p\) to prime power \(n\)''

\begin{lemma}
  \[
    1 * \Lambda = \log.
  \]
  and
  \[
    \mu * \log = \Lambda.
  \]
\end{lemma}

\begin{proof}
  The second part follows from Möbius inversion. Thus if \(n = p_1^{k_1} \dots p_r^{k_r}\),
  \begin{align*}
    1 * \Lambda(n)
    &= \sum_{d \divides n} \Lambda(d)
    = \sum_{i = 1}^r \sum_{j = 1}^{k_i} \Lambda(p_i^j) \\
    &= \sum_{i = 1}^r \sum_{j = 1}^{k_i} \log(p_i)
    = \sum_{i = 1}^r k_i \log p_i \\
    &= \sum_{i = 1}^r \log (p_i^{k_i})
    = \log n
  \end{align*}
\end{proof}

Therefore
\begin{align*}
  \Lambda(n)
  &= \sum_{d \divides n} \mu(d) \log(\frac{n}{d}) \\
  &= \log n \sum_{d \divides n} \mu(d) - \sum_{d \divides n} \mu(d) \log d \\
  &= - \sum_{d \divides n} \mu(d) \log d
\end{align*}
For example
\[
  \sum_{1 \leq n \leq x} \Lambda(n)
  = - \sum_{1 \leq n \leq x} \sum_{d \divides n} \mu(d) \log d
  = - \sum_{d \leq x} \mu(d) \log d \left(\sum_{1 \leq n \leq x, d \divides n} 1\right)
\]
by reversing summation. But now the term in the inner summation is very easy to understand:
\[
  \sum_{1 \leq n \leq x, d \divides n} 1 = \floor*{\frac{x}{d}} = \frac{x}{d} + O(1).
\]
Thus
\[
  \sum_{1 \leq n \leq x} \Lambda(n)
  = -x \sum_{d \leq x} \mu(d) \frac{\log d}{d} + O\left(\sum_{d \leq x} \mu(d) \log d\right).
\]
We'll see more of these examples.

\subsection{Summation}

Given an arithmetic function \(f\), we can ask for estimates of \(\sum_{1 \leq n \leq x} f(n)\). We say that \(f\) has \emph{average order}\index{average order} \(g\) if
\[
  \sum_{1 \leq n \leq x} f(n) \sim x g(x).
\]
``average size of \(f\) is \(g\)''.

\begin{eg}\leavevmode
  \begin{enumerate}
  \item \(f = 1\) then
    \[
      \sum_{1 \leq n \leq x} f(x) = \floor{x} = x + O(1) \sim x
    \]
    so average order of \(1\) is \(1\).
  \item \(f(n) = n\):
    \[
      \sum_{1 \leq n \leq x} n \sim \frac{x^2}{2}
    \]
    so average of \(n\) is \(\frac{n}{2}\).
  \end{enumerate}
\end{eg}

\begin{lemma}[partial summation]\index{partial summation}
  If \((a_n)\) is a sequence of complex numbers and \(f\) is such that \(f'\) is continuous. Then
  \[
    \sum_{1 \leq n \leq x} a_n f(n) = A(x) f(x) - \int_1^x A(t)f'(t) dt
  \]
  where \(A(x) = \sum_{1 \leq n \leq x} a_n\).
\end{lemma}

This is the discrete analogus of integration by parts.

\begin{proof}
  Suppose \(x = N\) is an integer. Note that \(a_n = A(n) - A(n - 1)\), so
  \begin{align*}
    \sum_{1 \leq n \leq N} a_nf(n)
    &= \sum_{1 \leq n \leq N} f(n) (A(n) - A(n - 1)) \\
    &= A(N)f(N) - \sum_{n = 1}^{N - 1} A(n) (f(n + 1) - f(n))
  \end{align*}
  Now
  \[
    f(n + 1) - f(n) = \int_n^{n + 1} f'(t) dt
  \]
  so
  \begin{align*}
    \sum_{1 \leq n \leq N} a_n f(n)
    &= A(N)f(N) - \sum_{n = 1}^{N - 1} A(n) \int_n^{n + 1} f'(t) dt \\
    &= A(N)f(N) - \int_1^N A(t) f'(t) dt
  \end{align*}
  where the last step is because \(A(n) = A(t)\) for \(t \in [n, n + 1)\).

  If \(N = \floor x\) then
  \[
    A(x)f(x)
    = A(N)f(x)
    = A(N) f(N) + \int_N^x f'(t) dt.
  \]
\end{proof}

As a simple application

\begin{lemma}
  \[
    \sum_{1 \leq n \leq x} \frac{1}{n} = \log x + \gamma + O(\frac{1}{x}).
  \]
\end{lemma}

\begin{proof}
  Partial summation with \(f(x) = \frac{1}{x}\) and \(a_n = 1\), so \(A(x) = \floor x\). Therefore
  \begin{align*}
    \sum_{1 \leq n \leq x} \frac{1}{n}
    &= \frac{\floor x}{x} + \int_1^x \frac{\floor t}{t^2} dt
    \intertext{Write \(\floor t= t - \{t\}\),} \\
    &= 1 + O(\frac{1}{x}) + \int_1^x \frac{1}{t} dt - \int_1^x \frac{\{t\}}{t^2} dt \\
    &= 1 + O(\frac{1}{x}) + \log x - \int_1^\infty \frac{\{t\}}{t^2} dt + \underbrace{\int_x^\infty \frac{\{t\}}{t^2} dt}_{\leq \int_x^\infty \frac{1}{t^2} dt \leq \frac{1}{x}} \\
    &= \gamma + O(\frac {1}{x}) + \log x + O(\frac{1}{x}) \\
    &= \log x + \gamma + O(\frac{1}{x})
  \end{align*}
\end{proof}

This is an amazing result and the only thing we did is to replace the discrete summation by the continuous analogue to it. In essence this is the whole reason analytic number theory works.

\(\gamma\) can be seen as a measure of the difference between between \(\log\) and its discrete approximation. It is called \emph{Euler-Mascheroni constant}\index{Euler-Mascheroni constant}. 
Surprisingly little is known about \(\gamma\). It is approximately \(0.577\dots\). We don't even know if \(\gamma\) is rational or not.

\begin{lemma}
  \[
    \sum_{1 \leq n \leq x} \log n = x \log x - x + O(\log x).
  \]
\end{lemma}

\begin{proof}
  Partial summation with \(f(x) = \log x, a_n = 1\) so \(A(x) = \floor{x}\). As a side note, in the previous example, most error comes from the integral term (the mass is evenly distributed). By constrast in this example most error comes from the ``sum'' term.
  \begin{align*}
    \sum_{1 \leq n \leq x} \log n
    &= \floor{x} \log x - \int_1^x \frac{\floor{t}}{t} dt \\
    &= x \log x + O(\log x) - \int_1^x dt + O(\int_1^x \frac{1}{t}dt) \\
    &= x \log x + O(\log x) - x + O(\log x)
  \end{align*}
\end{proof}

\subsection{Divisor function}

Recall that
\[
  \tau(n) = 1 * 1(n) = \sum_{ab = n} 1 = \sum_{d \divides n} 1.
\]

\begin{theorem}
  \[
    \sum_{1 \leq n \leq x} \tau(n) = x \log x + (2 \tau - 1) x + O(x^{1/2})
  \]
  so in particular average order of \(\tau\) is \(\log\).
\end{theorem}

\begin{proof}
  First attempt:
  \begin{align*}
    \sum_{1 \leq n \leq x} \tau(n)
    &= \sum_{1 \leq n \leq x} \sum_{d \divides n} 1
    = \sum_{1 \leq d \leq x} \sum_{1 \leq n \leq x, d \divides n} 1 \\
    &= \sum_{1 \leq d \leq x} \floor*{\frac{x}{d}} \\
    &= \sum_{1 \leq d \leq x} \frac{x}{d} + O(x)
    = x \sum_{1 \leq d \leq x} \frac{1}{d} + O(x) \\
    &= x\log x + \gamma x + O(x)
  \end{align*}
  This is not a very good bound (the error might be as large as one of the terms!) but shows that at least the first term is correct. The main drawback is we used the estimate
  \[
    \sum_{1 \leq d \leq x} O(1) = O(x).
  \]

  To reduce the error term, we use \emph{(Dirichlet's) hyperbola trick}\index{hyperbola trick}
  \[
    \sum_{1 \leq n \leq x} \tau(n)
    = \sum_{1 \leq n \leq x} \sum_{ab = n} 1
    = \sum_{ab \leq x} 1
    = \sum_{a \leq x} \sum_{b \leq x/a} 1
  \]
  The intuition is like this: \(\sum_{1 \leq n \leq x} \tau(n)\) counts the number of integral points below the hyperbola \(k_1k_2 = x\) in the first quadrant. The old methods amounts to an estimation by integral, while in the new method we count the number of points lying below the line \(k_2 = x^{1/2}\), add the number of points to the left of \(k_1 = x^{1/2}\), and finally subtract those points in the box \([0, x^{1/2}]^2\) which are double counted.

  Thus when summing over \(ab \leq x\), we can sum over \(a \leq x^{1/2}\) and \(b \leq x^{1/2}\) respectively, and then minus pairs \(a, b \leq \sqrt x\). Thus
  \begin{align*}
    \sum_{1 \leq n \leq x} \tau(n)
    &= \sum_{a \leq x^{1/2}} \sum_{b \leq x/a} 1 + \sum_{b \leq x^{1/2}} \sum_{a \leq x/b} 1 - \sum_{a, b \leq x^{1/2}} 1 \\
    &= 2 \sum_{a \leq x^{1/2}} \floor*{\frac{x}{a}} - \floor{x^{1/2}}^2 \\
    &= 2 \sum_{a \leq x^{1/2}} \frac{x}{a} + O(x^{1/2}) - x + O(x^{1/2}) \\
    &= 2x\log x^{1/2} + 2 \gamma x - x + O(x^{1/2}) \\
    &= x \log x + (2\gamma - 1) x + O(x^{1/2})
  \end{align*}
\end{proof}

\begin{remark}
  Improving this \(O(x^{1/2})\) error term is a famous and hard problem. Probably \(O(x^{1/4 + \varepsilon})\)? The best result so far is \(O(x^{0.3149})\).
\end{remark}

A note on average order: \(\tau\) has average order \(\log\) does not mean \(\tau(n) \ll \log n\), i.e.\ average order does not imply individual values.

\begin{theorem}
  For all \(n\)
  \[
    \tau(n) \leq n^{O(\frac{1}{\log \log n})}.
  \]
  In particular \(\tau(n) \ll_\varepsilon n^\varepsilon\) for all \(\varepsilon > 0\) where \(\ll_\varepsilon\) means that \(|\tau(n)| \leq C_\varepsilon |n^\varepsilon|\) eventually where \(C_\varepsilon\) is a constant depending on \(\varepsilon\).
\end{theorem}

As a side note, asymptotic bounds such as \(\log \log n\) are quite common in analytic number theory and here is how to reason with them: as \(n \to \infty\), \(\log n\) grows slower than any polynomial, so \(\log \log n\) grows slower than \(\log P(n)\) for any polynomial. Another way is to write \(n = e^{\log n}\) and then
\[
  n^{O(\frac{1}{\log \log n})} = \exp (O(\frac{\log n}{\log \log})).
\]

\begin{proof}
  \(\tau\) is multiplicative so enough to calculate at prime powers. \(\tau(p^k) = k + 1\) so if \(n = p_1^{k_1} \cdots p_r^{k_r}\) then \(\tau(n) = \prod_{i = 1}^r (k_i + 1)\). Let \(\varepsilon > 0\) to be chosen later and consider the ratio
  \[
    \frac{\tau(n)}{n^\varepsilon}
    = \prod_{i = 1}^r \frac{k_i + 1}{p^{k_i\varepsilon}}.
  \]
  Now enters the trick: split into big and small cases. Note as \(p\) goes large, \(\frac{k + 1}{p^{k \varepsilon}} \to 0\). In particular if \(p \geq 2^{1/\varepsilon}\) then
  \[
    \frac{k + 1}{p^{k\varepsilon}} \leq \frac{k + 1}{2^k} \leq 1.
  \]
  What about small \(p\)? It is important to remind ourselves that we're dealing with primes and \(p\) can't run below \(2\). In this case
  \[
    \frac{k + 1}{p^{k\varepsilon}} \leq \frac{k + 1}{2^{k\varepsilon}} \leq \frac{1}{\varepsilon}
  \]
  this is because \(x + \frac{1}{2} \leq 2^x\) for \(x \geq 0\) so \(\varepsilon h + \varepsilon \leq 2^{k \varepsilon}\) if \(\varepsilon \leq \frac{1}{2}\) (the details are not so important compared to the conclusion that this can be bounded). Therefore
  \[
    \frac{\tau(n)}{n^\varepsilon}
    \leq \prod_{i = 1, p_i < 2^{1/\varepsilon}}^r \frac{k_i + 1}{p^{k_i \varepsilon}}
    \leq \left( \frac{1}{\varepsilon} \right)^{\pi(2^{1/\varepsilon})}
    \leq \left( \frac{1}{\varepsilon} \right)^{2^{1/\varepsilon}}\footnote{Behold what a wasteful bound we give in the last inequality! But that almost has no effect in the final result.}.
  \]
  Now we need to choose an optimal \(\varepsilon\). Another trick: if we want to minimise \(f(x) + g(x)\), choose \(x\) such that \(f(x) = g(x)\). Have
  \[
    \tau(n)
    \leq n^\varepsilon \varepsilon^{-2^{1/\varepsilon}}
    = \exp (\varepsilon \log n + 2^{1/\varepsilon} \log (1/\varepsilon)).
  \]
  Choose \(\varepsilon\) such that \(\log n \approx 2^{1/\varepsilon}\) (again, only a rough guess is needed), i.e.\ \(\varepsilon \approx \frac{1}{\log \log n}\) and get
  \begin{align*}
    \tau(n)
    &\leq n^{\frac{1}{\log \log n}} (\log \log n)^{2^{\log \log n}} \\
    &= n^{\frac{1}{\log \log n}} \exp ((\log n)^{\log 2} \log \log \log n) \\
    &\leq n^{O(\frac{1}{\log \log n})}.
  \end{align*}
\end{proof}

\subsection{Estimates for the primes}

Recall that
\begin{align*}
  \pi(x) &= \# \{\text{primes } \leq x\} = \sum_{1 \leq n \leq x} 1_p(n) \\
  \psi(x) &= \sum_{1 \leq n \leq x} \Lambda(n)
\end{align*}
The second one is sometimes known as \emph{Chebyshev's function}\index{Chebyshev's function}. Prime number theorem asserts that \(\pi(x) \sim \frac{x}{\log x}\) or equivalently \(\psi(x) \sim x\) (this equivalence will be shown later).

Although Euclid's prove in 300 BC the infinitude of prime, It was 1850 before the correct magnitude of \(\pi(x)\) was proved. Chebyshev showed that
\[
  \pi(x) \asymp \frac{x}{\log x}
\]
where \(f \asymp g\) means that \(g \ll f \ll g\).

\begin{theorem}[Chebyshev]
  \[
    \psi(x) \asymp x.
  \]
\end{theorem}

\begin{proof}
  First we'll prove the lower bound, i.e.\ \(\psi(x) \gg x\). Recall that \(1 * \Lambda = \log\). Here comes in a genuine\footnote{Read unmotivated.} trick: find something that equals \(1\). Then \(\psi(x) = \sum_{1 \leq n \leq x} \Lambda(n) \cdot 1\) can be rearranged. We'll use the identity
  \[
    \floor{x} = 2 \floor*{\frac{x}{2}} + 1
  \]
  for \(x \geq 0\). Either see it directly or a simple verification: if \(\frac{x}{2} = n + \theta\) where \(\theta \in [0, 1)\) then \(\floor*{\frac{x}{2}} = n\) and \(\floor{x} = \floor{2n + 2\theta} = 2n \text{ or } 2n + 1\). Then
  \begin{align*}
    \psi(x)
    &\geq \sum_{1 \leq n \leq x} \Lambda(x) \left( \floor*{\frac{x}{n}} - 2 \floor*{\frac{x}{2n}} \right)\\
    \intertext{Note that \(\floor*{\frac{x}{n}} = \sum_{m \leq x/n} 1\),}
    &=\sum_{n \leq x} \Lambda(n) \sum_{m \leq x/n} 1 - 2 \sum_{n \leq x} \Lambda(n) \sum_{m \leq x/2n} 1 \\
    &=  \sum_{nm \leq x} \Lambda(n) - 2 \sum_{nm \leq x/2} \Lambda(n) \\
    \intertext{Write \(d = nm\),}
    &= \sum_{d \leq x} 1 * \Lambda(d) - 2 \sum_{d \leq x/2} 1 * \Lambda(d) \\
    &= \sum_{d \leq x} \log d - 2 \sum_{d \leq x/2} \log d \\
    &= x \log x - x + O(\log x) - 2 \left( \frac{x}{2} \log \frac{x}{2} - \frac{x}{2} + O(\log x) \right) \\
    &= (\log 2) x + O(\log x) \\
    &\gg x
  \end{align*}

  For the upper bound,
  \[
    \floor{x} = 2 \floor*{\frac{x}{2}} + 1
  \]
  for \(x \in (1, 2)\) so
  \begin{align*}
    \psi(x) - \psi(\frac{x}{2})
    &= \sum_{x/2 < n < x} \Lambda(n) \\
    &\leq \sum_{1 \leq n \leq x} \Lambda(n) \left( \floor*{\frac{x}{n}} - 2 \floor*{\frac{x}{2n}} \right) \\
    & \leq (\log 2) x + O(\log x)
  \end{align*}
  Thus
  \begin{align*}
    \psi(x)
    &= (\psi(x) - \psi(x/2)) + (\psi(x/2) - \psi(x/4)) + \dots \\
    &\leq \log 2 \cdot (x + x/2 + x/4 + \dots ) \\
    &= 2 \log 2 \cdot x
  \end{align*}
  Thus we have shown
  \[
    (\log 2) x \leq \psi(x) \leq (\log 4) x.
  \]
\end{proof}

\begin{lemma}
  \[
    \sum_{p \leq x} \frac{\log p}{p} = \log x + O(1).
  \]
\end{lemma}

\begin{proof}
  Recall that \(\log = 1 * \Lambda\) so
  \begin{align*}
    \sum_{n \leq x} \log n
    &= \sum_{ab \leq x} \Lambda(a)
    = \sum_{a \leq x} \Lambda(a) \sum_{b \leq x/a} 1 \\
    &= \sum_{a \leq x} \Lambda(a) \floor*{\frac{x}{a}} \\
    &= x \sum_{a \leq x} \frac{\Lambda(a)}{a} + O(\psi(x)) \\
    &= x \sum_{a \leq x} \frac{\Lambda(a)}{a} + O(x) \\
  \end{align*}
  Note where we used Chebyshev's bound. Since
  \[
    \sum_{n \leq x} \log x = x\log x - x + O(\log x),
  \]
  have
  \[
    \sum_{n \leq x} \frac{\Lambda(n)}{n}
    = \log x - 1 + O(\frac{\log x}{x}) + O(1)
    = \log x + O(1)
  \]
  Remain to note the contribution from prime powers \(\geq 2\) are ``small'':
  \begin{align*}
    \sum_{p \leq x} \sum_{n = 2}^\infty \frac{\log p}{p^n}
    &= \sum_{p \leq x} \log p \sum_{n = 2}^\infty \frac{1}{p^n} \\
    &= \sum_{p \leq x} \frac{\log p}{p^2 - p} \\
    &\leq \sum_{p = 2}^\infty \frac{1}{p^{3/2}} \\
    &= O(1)
  \end{align*}
  so
  \[
    \sum_{n \leq x} \frac{\Lambda(n)}{n} = \sum_{p \leq x} \frac{\log p}{p} + O(1).
  \]
\end{proof}

\begin{lemma}
  \[
    \pi(x) = \frac{\psi(x)}{\log x} + O(\frac{x}{(\log x)^2}).
  \]
  In particular \(\pi(x) \asymp \frac{x}{\log x}\) and prime number theorem \(\pi(x) \sim \frac{x}{\log x}\) is equivalent to \(\psi(x) \sim x\).
\end{lemma}

\begin{proof}
  Idea is to use partial summation: let
  \[
    \theta(x)
    = \sum_{p \leq x} \log p
    = \pi(x) \log x - \int_1^x \frac{\pi(t)}{t} dt.
  \]
  First problem: \(\psi(x)\) sums over not only primes but also prime powers. We can use a previous trick to remove contributions from prime powers:
  \begin{align*}
    \psi(x) - \theta(x)
    &= \sum_{k = 2}^\infty \sum_{p^k \leq x} \log p
    = \sum_{k = 2}^\infty \theta(x^{1/k}) \\
    &\leq \sum_{k = 2}^{\log x} \psi(x^{1/k})
    \leq \sum_{k = 2}^{\log x} x^{1/k} \\
    &\leq x^{1/2} \log x
  \end{align*}
  Therefore
  \begin{align*}
    \psi(x)
    &= \pi(x) \log x + O(x^{1/2} \log x) - \int_1^x \frac{\pi(t)}{t} dt \\
    \intertext{As \(\pi(t) \leq \frac{t}{\log t}\),}
    &= \pi(x) \log x + O(x^{1/2} \log x) + O(\int_1^x \frac{1}{\log t} dt) \\
    &= \pi(x) \log x + O(\frac{x}{\log x})
  \end{align*}
  For \(\pi(t) < \frac{t}{\log t}\), note the trivial bound \(\pi(t) \leq t\) so
  \[
    \psi(x) = \pi(x) \log x + O(x^{1/2} \log x) + O(x)
  \]
  so \(\pi(x)\log x = O(x)\). Thus we used the trivial bound to get a better bound and use that to do actual work.
\end{proof}

\begin{lemma}
  \[
    \sum_{p \leq x} \frac{1}{p} = \log \log x + b + O(\frac{1}{\log x})
  \]
  where \(b\) is some constant.
\end{lemma}

Compare to \(\sum_{1 \leq n \leq x} \frac{1}{n}\).

\begin{proof}
  Partial summation. Let
  \[
    A(x) = \sum_{p \leq x} \frac{\log p}{p} = \log x + R(x)
  \]
  where \(R(x) = O(1)\). Then (summing from \(2\) to prevent \(\log t = 0\))
  \begin{align*}
    \sum_{2 \leq p \leq x} \frac{1}{p}
    &= \frac{A(x)}{\log x} + \int_2^x \frac{A(t)}{t (\log t)^2} dt \\
    &= 1 + O(\frac{1}{\log x}) + \int_2^x \frac{1}{t \log t} dt + \int_2^x \frac{R(t)}{t(\log t)^2} dt
  \end{align*}
  Note that \(\int_2^\infty \frac{R(t)}{t (\log t)^2} dt\) exists, say \(C\). Then
  \begin{align*}
    \sum_{2 \leq p \leq x} \frac{1}{p}
    &= 1 + C + O(\frac{1}{\log x}) + \log \log x - \log \log 2 + O(\int_x^\infty \frac{1}{t (\log t)^2} dt) \\
    &= \log \log x + b + O(\frac{1}{\log x})
  \end{align*}
  It turns out \(b\) can be expressed in terms of \(\gamma\).
\end{proof}

\begin{theorem}[Chebyshev]
  If \(\pi(x) \sim c \frac{x}{\log x}\) then \(c = 1\).
\end{theorem}

Note that this does not prove prime number theorem. Historically this is a surprise: a following corollary says that if \(\pi(x) \sim \frac{x}{\log x - A(x)}\) then \(A \sim 1\). But Legendre and Gauss et al have conjectured that \(A \approx 1.08 \dots\), just by looking up the prime table.

\begin{proof}
  Partial summation on \(\sum_{p \leq x} \frac{1}{p}\):
  \begin{align*}
    \sum_{p \leq x} \frac{1}{p}
    &= \frac{\pi(x)}{x} + \int_1^x \frac{\pi(t)}{t^2} dt \\
    \intertext{If \(\pi(x) = (c + o(1)) \frac{x}{\log x}\) then}
    &= \frac{c}{\log x} + o(\frac{1}{\log x}) + (c + o(1)) \int_1^x \frac{1}{t \log t} dt \\
    &= O(\frac{1}{\log x}) + (c + o(1)) \log \log x
  \end{align*}
  But
  \[
    \sum_{p \leq x} \frac{1}{p} = (1 + o(1)) \log \log x
  \]
  so \(c = 1\).
\end{proof}

\begin{lemma}
  \[
    \prod_{p \leq x} \left( 1 - \frac{1}{p} \right)^{-1} = c\log x + O(1)
  \]
  where \(c\) is some constant.
\end{lemma}

\begin{proof}
  We have only dealt with summations so far so take \(\log\),
  \begin{align*}
    \log \prod_{p \leq x} \left( 1 - \frac{1}{p} \right)^{-1}
    &= - \sum_{p \leq x} \log (1 - \frac{1}{p}) \\
    &= \sum_{p \leq x} \sum_k \frac{1}{k p^k} \\
    &= \sum_{p \leq x} \frac{1}{p} + \sum_{k \geq 2} \sum_{p \leq x} \frac{1}{kp^k} \\
    &= \log \log x + c' + O(\frac{1}{\log x}).
  \end{align*}
  using \(\log (1 - t) = - \sum_k \frac{t^k}{k}\).

  To undo the \(\log\), note that \(e^x = 1 + O(x)\) for \(|x| \leq 1\) so
  \begin{align*}
    \prod_{p \leq x} \left( 1 - \frac{1}{p} \right)^{-1}
    &= c \log x \exp (O(\frac{1}{\log x})) \\
    &= c \log x (1 + O(\frac{1}{\log x})) \\
    &= c \log x + O(1)
  \end{align*}
  It turns out that \(c = e^\gamma \approx 1.78 \dots\).
\end{proof}

\subsubsection{Aside: Why is prime number theorem so hard?}

It seems that we've made quite a progress without too much effort. But how far are we from prime number theorem and if the answer is ``quite far'', what makes it so resistant to elementary methods?

Probabilistic heuristic: fix \(p\) prime, ``probability'' that a random \(n\) satisfies \(p \divides n\) is \(\frac{1}{p}\). What is the ``probability'' that \(n\) is prime then? \(n\) is a prime if and only if \(n\) has no prime divisors \(p \leq n^{1/2}\). Guess that the events ``divisble by \(p\)'' are independent, then ``probability'' that \(n\) is prime is roughly
\[
  \prod_{p \leq n^{1/2}} \left( 1 - \frac{1}{p} \right)
  \approx \frac{1}{c \log n^{1/2}}
  = \frac{2}{c} \frac{1}{\log n}.
\]
Thus use some questionable squiggles,
\[
  \pi(x)
  = \sum_{n \leq x} 1_{n \text{ prime}}
  \approx \frac{2}{c} \sum_{n \leq x} \frac{1}{\log n}
  \approx \frac{2}{c} \frac{x}{\log x}
  \approx 2e^{-\gamma} \frac{x}{\log x}
\]
This constant is approximately \(1.122\dots\), which contradicts Chebyshev's theorem. Therefore somehow the heuristics is wrong: it gives 12\% more prime than should.

One reason is that the error terms are so close to the main term that when we do \(\approx\) they accummulate and excees the main term. Another reason is of course that the ``independence'' of primes are completely false. From an analytic point of view, this can be seen as saying that the ``interference terms'' are not so small that they are be instead.

This may explain why heuristics don't work. But can we bound \(\pi\) by elementary methods? Recall that \(\mu * \log = \Lambda\) so
\begin{align*}
  \psi(x)
  &= \sum_{n \leq x} \Lambda(n) \\
  &= \sum_{ab \leq x} \mu(a) \log b \\
  &= \sum_{a \leq x} \mu(a) \left( \sum_{b \leq x/a} \log b \right)
\end{align*}
Recall that
\[
  \sum_{m \leq x} \log m = x \log x - x + O(\log x),
\]
but if we just plug this in we will get a trouble. Instead use another trick: consider
\[
  \sum_{m \leq x} \tau(m) = x \log x + (2 \psi - 1) x + O(x^{1/2}).
\]
Thus
\[
  \psi(x)
  = \sum_{a \leq x} \mu(a) \left( \sum_{b \leq x/a} \tau(b) - 2\gamma \frac{x}{a} + O(\frac{x^{1/2}}{a^{1/2}}) \right).
\]
The first term is (essentially \(\mu * \tau = 1\))
\begin{align*}
  \sum_{ab \leq x} \mu(a)\tau(b)
  &= \sum_{abc \leq x} \mu(a) \\
  &= \sum_{b \leq x} \sum_{ac \leq x/b} \mu(a) \\
  &= \sum_{b \leq x} \sum_{d \leq x/b} \mu * 1(d) \\
  &= \floor x \\
  &= x + O(1)
\end{align*}
and the first error term is
\[
  -2\gamma \sum_{a \leq x} \mu(a) \frac{x}{a} = O(x\sum_{a \leq x} \frac{\mu(a)}{a})
\]
so still need to show that
\[
  x \sum_{a \leq x} \frac{\mu(a)}{a} = O(1).
\]
Well it turns out that this is equivalent to prime number theorem! This constant can be shown to be \(1/\zeta(1)\). As \(\zeta\) has a pole at \(z = 1\), this is indeed true.

\subsection{Selberg's identity and on elementary proof of prime number theorem}

Define Selberg's function
\[
  \Lambda_2(n) = \mu* (\log )^2(n) = \sum_{ab = n} \mu(a) (\log b)^2.
\]
The idea is to prove ``prime number theorem for \(\Lambda_2\)'' with elementary methods. The intuition is that \(\Lambda_2\) is like \(\Lambda\) multiplied by \(\log\) and if we do the same expansion as before, hopefully we can get
\[
  \sum_{n \leq x} \Lambda_2(n) = \text{main term} + O(x),
\]
but now this is now an acceptable error!

\begin{lemma}\leavevmode
  \begin{enumerate}
  \item \(\Lambda_2(n) = \Lambda(n) \log n + \Lambda * \Lambda (n)\).
  \item \(0 \leq \Lambda_2(n) \leq (\log n)^2\).
  \item If \(\Lambda_2(n) \neq 0\) then \(n\) has at most 2 distinct prime divisors.
  \end{enumerate}
\end{lemma}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item Use Möbius inversion suffices to show
    \[
      \sum_{d \divides n} (\Lambda(d) \log d + \Lambda * \Lambda(d)) = (\log n)^2.
    \]
    Start by expanding out,
    \begin{align*}
      \sum_{d \divides n} (\Lambda(d) \log d + \Lambda * \Lambda(d))
      &= \sum_{d \divides n} \Lambda(d) \log d + \sum_{ab \divides n} \Lambda(a) \Lambda(b) \\
      &= \sum_{d \divides n} \log d + \sum_{a \divides n} \Lambda(a) \underbrace{\sum_{b \divides \frac{n}{a}} \Lambda(b)}_{= \log (n/a)} \\
      &= \sum_{d \divides n} \log d + \sum_{d \divides n} \Lambda(d) \log \frac{n}{d} \\
      &= \log n \sum_{d \divides n} \Lambda(d) \\
      &= (\log n)^2
    \end{align*}
  \item \(\Lambda_2(n) \geq 0\) since both terms on RHS in 1 are nonnegative. Since
    \[
      \sum_{d \divides n} \Lambda_2(d) = (\log n)^2,
    \]
    \(\Lambda_2(n) \leq (\log n)^2\).
  \item Note that if \(n\) is divisible by 2 distinct primes then \(\Lambda(n) = 0\), and
    \[
      \Lambda * \Lambda(n) = \sum_{ab \divides n} \Lambda(a) \Lambda(b) = 0
    \]
    since at least one of \(a\) or \(b\) has \(\geq 2\) distinct prime divisors.
  \end{enumerate}
\end{proof}

As such while \(\Lambda\) can be thought as the indicator function for numbers with exactly 1 prime divisor, weighted by \(\log\), \(\Lambda_2\) can be thought as the indicator function for numbers with a pair of prime divisors, weighted by \((\log^2)\).

\begin{theorem}[Selberg]
  \[
    \sum_{n \leq x} \Lambda_2(n) = 2x \log x + O(x).
  \]
\end{theorem}

\begin{proof}
  \begin{align*}
    \sum_{n \leq x} \Lambda_2(n)
    &= \sum_{n \leq x} \mu * (\log)^2 (n) \\
    &= \sum_{ab \leq x} \mu(a) (\log b)^2 \\
    &= \sum_{a \leq x} \mu(a) \left( \sum_{b \leq x/a} (\log b)^2 \right)
  \end{align*}
  By partial summation,
  \[
    \sum_{m \leq x} (\log m)^2
    = x (\log x)^2 - 2x \log x + 2x + O((\log x)^2).
  \]
  We want to use the same trick and substitute sum of divisor function for the leading term. First we have to manufacture a \(x (\log x)^2\) term. By partial summation with
  \[
    A(t) = \sum_{n \leq t} \tau(n) = t \log t + Ct + O(t^{1/2}),
  \]
  have
  \begin{align*}
    \sum_{m \leq x} \frac{\tau(m)}{m}
    &= \frac{A(x)}{x} + \int_1^x \frac{A(t)}{t^2} dt \\
    &= \log x + C + O(x^{-1/2}) + \int_1^x \frac{\log t}{t} dt + C \int_1^x \frac{1}{t} dt + O(\int_1^x \frac{1}{t^{3/2}} dt) \\
    &= \frac{(\log x)^2}{2} + C_1 \log x + C_2 + O(x^{-1/2})
  \end{align*}
  Since we dislike \(\log\), we replace it by \(\sum_{m \leq x} \tau(m)\) to get
  \[
    \frac{x (\log x)^2}{2}
    = \sum_{m \leq x} \tau(m) \frac{x}{m} + C_1' \sum_{m \leq x} \tau(m) + C_2'x + O(x^{1/2}).
  \]
  Substituting back,
  \[
    \sum_{m \leq x} (\log m)^2
    = 2 \sum_{m \leq x} \tau(m) \frac{x}{m} + C_3 \sum_{m \leq x} \tau(m) + C_4 x + O(x^{1/2})
  \]
  so
  \begin{align*}
    \sum_{n \leq x} \Lambda_2(n)
    &= 2 \sum_{a \leq x} \mu(a) \sum_{b \leq x/a} \frac{\tau(b)x}{ab} + C_5 \sum_{a \leq x} \mu(a) \sum_{b \leq x/a} \tau(b) \\
    &+ C_6 \sum_{a \leq x} \mu(a) \frac{x}{a} + O(\sum_{a \leq x} \frac{x^{1/2}}{a^{1/2}}).
  \end{align*}
  We analyse the error terms one by one, starting from the back. First note that
  \[
    x^{1/2} \sum_{a \leq x} \frac{1}{a^{1/2}} = O(x).
  \]
  Secondly
  \begin{align*}
    x \sum_{a \leq x} \frac{\mu(a)}{a}
    &= \sum_{a \leq x} \mu(a) \floor*{\frac{x}{a}} + O(x) \\
    &= \sum_{a \leq x} \mu(a) \sum_{b \leq x/a} 1 + O(x) \\
    &= \sum_{d \leq x} \mu * 1(d) + O(x) \\
    &= 1 + O(x) \\
    &= O(x)
  \end{align*}
  Thirdly, (again essentially \(\mu * \tau = 1\))
  \begin{align*}
    \sum_{a \leq x} \mu(a) \sum_{b \leq x/a} \tau(b)
    &= \sum_{a \leq x} \mu(a) \sum_{b \leq x/a}\sum_{cd = b} 1 \\
    &= \sum_{a \leq x} \mu(a) \sum_{cd \leq x/a} 1 \\
    &= \sum_{acd \leq x} \mu(a)
    = \sum_{d \leq x} \sum_{ac \leq x/d} \mu(a) \\
    &= \sum_{d \leq x} \sum_{e \leq x/d} \mu * 1(e) \\
    &= \sum_{d \leq x} 1 \\
    &= O(x)
  \end{align*}
  Collecting what we've done,
  \begin{align*}
    \sum_{n \leq x} \Lambda_2(n)
    &= 2 \sum_{a \leq x} \mu(a) \sum_{b \leq x/a} \frac{\tau(b) x}{ab} + O(x) \\
    &= 2x \sum_{d \leq x} \frac{1}{d} \mu * \tau(d) + O(x) \\
    \intertext{Recall that \(\tau = 1 * 1\) so \(\mu * \tau = \mu * 1 * 1 = 1\),}
    &= 2x \sum_{d \leq x} \frac{1}{d} + O(x) \\
    &= 2x \log x + O(x)
  \end{align*}
\end{proof}

\subsubsection{*A 14-point plan to prove prime number theorem from Selberg's identity}

Let
\[
  r(x) = \frac{\psi(x)}{x} - 1.
\]
Then prime number theorem is the statement that
\[
  \lim_{x \to \infty} |r(x)| = 0.
\]
We will demonstrate how to count from 1 to 14 below. When you finished counting, you will get prime number theorem as a byproduct.

\begin{enumerate}
\item[1] Show that Selberg's identity implies
  \[
    r(x) \log x = - \sum_{n \leq x} \frac{\Lambda(n)}{n} r (\frac{x}{n}) + O(1).
  \]
\item[2] Consider 1 with \(x\) replaced by \(\frac{x}{m}\), summing over \(m\), show
  \[
    |r(x)| (\log x)^2 \leq \sum_{n \leq x} \frac{\Lambda_2(n)}{n} \left|r(\frac{x}{n})\right| + O(\log x).
  \]
\item[3]
  \[
    \sum_{n \leq x} \Lambda_2(n) = 2 \int_1^{\floor x} \log t dt + O(x).
  \]
\item[4 - 6]
  \[
    \sum_{n \leq x} \frac{\Lambda_2(n)}{n} |r(\frac{x}{n})|
    = 2 \int_1^x \frac{r(x/2)}{t \log t} dt + O(\log x).
  \]
\item[7] Let \(V(u) = r(e^u)\). Show that
  \[
    u^2 |V(u)| \leq 2 \int_0^u \int_0^v |V(t)| dt dv + O(u).
  \]
\item[8] Show
  \[
    \alpha = \limsup_{x \to \infty} |r(x)| \leq \limsup_{u \to \infty} \frac{1}{u} \int_0^u |V(t)| dt = \beta.
  \]
\item[9 - 14] If \(\alpha > 0\) then can show from 7 that \(\beta < \alpha\), contradiction. So \(\alpha = 0\). Prime number theorem.
\end{enumerate}

\section{Sieve methods}

Motivation example sieve of Eratosthenes: given natural numbers below \(20\), let's cross out all multiples of \(2\) to get
\[
  \begin{array}{cccccccccc}
    1 & \cancel 2 & 3 & \cancel 4 & 5 & \cancel 6 & 7 & \cancel 8 & 9 & \cancel{10} \\
    11 & \cancel{12} & 13 & \cancel{14} & 15 & \cancel{16} & 17 & \cancel{18} & 19 & \cancel{20}
  \end{array}
\]
Next we cross out all multiples of \(3\) to get
\[
  \begin{array}{cccccccccc}
    1 & \cancel 2 & \bcancel 3 & \cancel 4 & 5 & \xcancel 6 & 7 & \cancel 8 & \bcancel 9 & \cancel{10} \\
    11 & \xcancel{12} & 13 & \cancel{14} & \bcancel{15} & \cancel{16} & 17 & \xcancel{18} & 19 & \cancel{20}
  \end{array}
\]
As \(\sqrt{20} < 5\), we know that the numbers left on the list are prime (with the exception of \(1\)). Our interest is in using the sieve to \emph{count} things: we can find how many numbers are left, which by definition are those primes below \(20\) that are not used as sieves, by inclusion-exclusion principle:
\begin{align*}
  \pi(20) + 1 - \pi(\sqrt{20})
  &= 20 - \floor*{\frac{20}{2}} - \floor*{\frac{20}{3}} + \floor*{\frac{20}{6}} \\
  &= 20 - 10 - 6 + 3 \\
  &= 7 
\end{align*}
By the way if there are more sieves then we naturally include more terms in the inclusion-exclusion expansion. Note that the coefficient/sign in front of each term is precisely the Möbius function of the denominator.

\subsection{Setup}

Consider \(A \subseteq \N\) finite, which is the set to be sifted. Let \(P\) be a set of primes, which are those we sift out by. Usually \(P\) is the set of all primes. Let \(z\) be a sifting limit: we sift all primes in \(P\) that are smaller than \(z\). A \emph{sifting function}
\[
  S(A, P; z) = \sum_{n \in A} 1_{(n, P(z)) = 1}
\]
where \(P(z) = \prod_{p \in P, p < z} p\). The goal is to estimate \(S(A, P; z)\).

For \(d\), let
\[
  A_d = \{n \in A: d \divides n\}.
\]
Write
\[
  |A_d| = \frac{f(d)}{d} X + R_d
\]
where \(f\) is completely multiplicative (\(f(mn) = f(m)f(n)\) for all \(m, n\)) and \(f(d) \geq 0\) for all \(d\). Note that
\[
  |A| = \frac{f(1)}{1} X + R_1 = X + R_1
\]
Think of \(R_1\) as the remainder term, \(X\) is roughly the size of \(A\). Extending this analogy, for general \(d\), \(R_d\) is the ``error'' term and \(\frac{X}{d}\) measures the number of elements in the \(0\) residue class of \(d\), assuming they are distributed uniformly. Then \(f(d)\) is a factor that says how the residue class is actually distributed.

We choose \(f\) so that \(f(p) = 0\) if \(p \notin P\) (so \(R_p = |A_p|\)). Finally let
\[
  W_P(z) = \prod_{\substack{p \in P \\ p < z}} \left( 1 - \frac{f(p)}{p} \right),
\]
the probability that it is not divisible by any of the \(p\).

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Sieve of Eratosthenes: \(A = (x, x + y] \cap \N\) and \(P\) is the set of all primes. Then
    \[
      |A_d|
      = \floor*{\frac{x + y}{d}} - \floor*{\frac{x}{d}}
      = \frac{x + y}{d} - \frac{x}{d} + O(1)
      = \frac{y}{d} + O(1)
    \]
    so \(f(d) = 1\) and \(R_d = O(1)\). Have
    \[
      S(A, P; z) = \#\{x < n \leq x + y: \text{ if } p \divides n \text{ then } p \geq z\}.
    \]
    For example if \(z \approx (x + y)^{1/2}\) then
    \[
      S(A, P; z) = \pi(x + y) - \pi(x) + O((x + y)^{1/2}).
    \]
  \item Let \(A = \{1 \leq n \leq y: n = a \mod q\}\). Then
    \[
      A_d = \{1 \leq m \leq \frac{y}{d}: dm = a \mod q\}.
    \]
    The congruence has solutions if and only if \((d, q) \divides a\). Thus
    \[
      |A_d| =
      \begin{cases}
        \frac{(d, q)}{dq} y + O((d, q)) & (d, q) \divides a \\
        O((d, q)) & \text{otherwise}
      \end{cases}
    \]
    So here \(X = \frac{y}{q}\) and
    \[
      f(d) =
      \begin{cases}
        (d, q) & (d, q) \divides a \\
        0 & \text{otherwise}
      \end{cases}
    \]
  \item Count twin primes: let \(A = \{n (n + 2): 1 \leq n \leq x\}\) and let \(P\) be all primes except \(2\). So \(p \divides n(n + 2)\) if and only if \(n = 0 \text{ or } 2 \mod p\). Thus
    \[
      |A_p| = \frac{2x}{p} + O(1).
    \]
    Thus \(f(p) = 2\). By complete multiplicity, \(f(d) = 2^{\omega(d)}\) if \(2 \ndivides d\). Have
    \begin{align*}
      S(A, P; x^{1/2})
      &= \#\{1 \leq p \leq x: p, p + 2 \text{ both primes}\} + O(x^{1/2}) \\
      &= \pi_2(x) + O(x^{1/2})
    \end{align*}
    We would expect \(\pi_2(x) \approx \frac{x}{(\log x)^2}\). We'll prove upper bound using sieves.
  \end{enumerate}
\end{eg}

\begin{theorem}[sieve of Eratosthenes-Legendre]
  \[
    S(A, P; z) = XW_P(z) + O(\sum_{d \divides P(z)} |R_d|).
  \]
\end{theorem}

\begin{proof}
  \begin{align*}
    S(A, P; z)
    &= \sum_{n \in A} 1_{(n, P(z)) = 1} \\
    &= \sum_{n \in A} \sum_{d \divides (n, P(z))} \mu(d) \\
    &= \sum_{n \in A} \sum_{\substack{d \divides n \\ d \divides P(z)}} \mu(d) \\
    &= \sum_{d \divides P(z)} \mu(d) \sum_{n \in A} 1_{d \divides n} \\
    &= \sum_{d \divides P(z)} \mu(d) |A_d| \\
    &= X \sum_{d \divides P(z)} \frac{\mu(d) f(d)}{d} + \sum_{d \divides P(z)} \mu(d) R_d \\
    &= X \prod_{p \in P, p < z} \left(1 - \frac{f(p)}{p} \right) + O(\sum_{d \divides P(z)} |R_d|)
  \end{align*}
\end{proof}

\begin{corollary}
  \[
    \pi(x + y) - \pi(x) \ll \frac{y}{\log \log y}.
  \]
\end{corollary}
By taking \(x = 0\) we see this is much worse bound in \(y\) than Chebyshev. On the other hand, however, we get a uniform bound independent of \(x\)!

\begin{proof}
  In example 1, \(X = y, f = 1\) and \(|R_d| \ll 1\). Thus
  \[
    W_P(z)
    = \prod_{p < z} \left( 1 - \frac{1}{p} \right)
    \ll (\log z)^{-1}
  \]
  and
  \[
    \sum_{d \divides P(z)} |R_d| \ll \sum_{d \divides P(z)} 1 \leq 2^z
  \]
  so
  \[
    \pi(x + y) - \pi(x) \ll \frac{y}{\log z} + 2^z \ll \frac{y}{\log \log y}
  \]
  by letting \(z = \log y\).
\end{proof}

\subsection{Selberg's sieve}

Using sieve of Eratosthenes-Legendre, we only get \(\frac{y}{\log\log y}\) instead of the expected \(\frac{y}{\log y}\). What prevents us from getting the result is that we can't take \(z = y\) --- otherwise the error term will \(O(2^z) = O(2^y)\), which is much bigger than the main term.

The problem is that we have to consider \(2^z\) many divisors of \(P(z)\) so get \(2^z\) many error terms. However, we can design a different sieve, and only consider those divisors which are small, say \(\leq D\). The key part of Eratosthenes-Legendre sieve is
\[
  1_{(n, P(z)) = 1} = \sum_{d \divides (n, P(z))} \mu(d).
\]
However, for an upper bound, it is enough to use \emph{any} function \(F\) such that
\[
  F(n) \geq
  \begin{cases}
    1 & n = 1 \\
    0 & \text{otherwise}
  \end{cases}
\]
Selberg's observation was that if \((\lambda_i)\) is any sequence of reals with \(\lambda_1 = 1\) then
\[
  F(n) = \left( \sum_{d \divides n} \lambda_d \right)^2
\]
works.

We assume that \(0 < f(p) < p\) for \(p \in P\), which is a reasonable assumption for the sieve to be ``nontrivial'' (if \(f(p) = 0\) then the sieve does nothing and may well just remove \(p\) from \(P\). If \(f(p) = p\) then it sifts out everything!) The let us define a new multiplicative function \(g\) such that
\[
  g(p) = \left( 1 - \frac{f(p)}{p} \right)^{-1} - 1 = \frac{f(p)}{p - f(p)}.
\]

\begin{theorem}[Selberg's sieve]\index{Selberg's sieve}
  \label{thm:Selberg's sieve}
  For all \(t\),
  \[
    S(A, P; z) \leq \frac{X}{G(t, z)} + \sum_{\substack{d \divides P(t) \\ d < t^2}} 3^{\omega(d)} |R_d|
  \]
  where
  \[
    G(t, z) = \sum_{\substack{d \divides P(z) \\ d < t}} g(d).
  \]
\end{theorem}

Recall that \(W_P = \prod_{\substack{p \in P \\ p < z}} (1 - \frac{f(p)}{p})\) so expected size of \(S(A, P; z)\) is \(XW_P\). Note that as \(t \to \infty\),
\[
  G(t, z)
  \to \sum_{d \divides P(z)} g(d) 
  = \prod_{p < z} (1 + g(p))
  = \prod_{p < z} \left( 1 - \frac{f(p)}{p} \right)^{-1}
  = \frac{1}{W_P}.
\]

Let's apply our new machinery:
\begin{corollary}
  For all \(x, y\),
  \[
    \pi(x + y) - \pi(x) \ll \frac{y}{\log y}.
  \]
\end{corollary}

\begin{proof}
  Let \(A = \{x < n \leq x + y\}, f(p) = 1, R_d = O(1)\) and \(X = y\). As \(g(p) = \frac{1}{p - 1} = \frac{1}{\varphi(p)}\) so \(g(d) = \frac{1}{\varphi(d)}\),
  \begin{align*}
    G(z, z)
    &= \sum_{\substack{d \divides P(z) \\ d < z}} \prod_{p \divides d} (p - 1)^{-1} \\
    &= \sum_{d = p_1 \cdots p_r < z} \prod_{i = 1}^r \sum_{k = 1}^\infty \frac{1}{p_i^k} \\
    &= \sum_{\substack{p_1 \cdots p_r < z \\ 1 \leq i \leq r}} \sum_{k_i = 1}^\infty \frac{1}{p_1^{k_1} \cdots p_r^{k_r}} \\
    &= \sum \frac{1}{n} \quad \text{sum over \(n\) whose square-free part \(< z\)} \\
    &\geq \sum_{d < z} \frac{1}{d} \\
    &\gg \log z
  \end{align*}
  so the main term \(\ll \frac{y}{\log z}\). Note that
  \[
    3^{\omega(d)} \leq \tau_3(d) \ll_\varepsilon d^\varepsilon
  \]
  from example sheet 1 so the error term is
  \[
    \sum_{\substack{d \divides P(t) \\ d < t^2}} 3^{\omega(d)} |R_d|
    \ll_\varepsilon t^\varepsilon \sum_{d < t^2} 1
    \ll t^{2 + \varepsilon} = z^{2 + \varepsilon}
  \]
  by setting \(t = z\). Thus
  \[
    S(A, P; z) \ll \frac{y}{\log z} + z^{2 + \varepsilon} \ll \frac{y}{\log y}
  \]
  by taking \(z = y^{1/3}\).
\end{proof}

\begin{proof}[Proof of \nameref{thm:Selberg's sieve}]
  Let \((\lambda_i)\) be a sequence of reals with \(\lambda_1 = 1\), to be chosen later. Then
  \begin{align*}
    S(A, P; z)
    &= \sum_{n \in A} 1_{(n, P(z)) = 1} \\
    &\leq \sum_{n \in A} \left( \sum_{d \divides (n, P(z))} \lambda_d \right)^2 \\
    &= \sum_{d, e \divides P(z)} \lambda_d \lambda_e \sum_{n \in A} 1_{d \divides n, e \divides n} \\
    &= \sum_{d, e \divides P(z)} \lambda_d \lambda_e |A_{[d, e]}| \\
    &= X \sum_{d, e \divides P(z)} \lambda_d \lambda_e \frac{f([d, e])}{[d, e]} + \sum_{d, e \divides P(z)} \lambda_d \lambda_e R_{[d, e]}
  \end{align*}
  We'll choose \(\lambda_d\) such that \(|\lambda_d| \leq 1\) and \(\lambda_d = 0\) if \(d \geq t\). Then
  \[
    \left| \sum_{d, e \divides P(z)} \lambda_d \lambda_e R_{[d, e]} \right|
    \leq \sum_{\substack{d, e < t \\ d, e \divides P(z)}} |R_{[d, e]}|
    \leq \sum_{\substack{n \divides P(z) \\ n < t^2}} |R_n| \sum_{d, e} 1_{[d, e] = n}
  \]
  and since \(n\) is square-free,
  \[
    \sum_{d, e} 1_{[d, e] = n} = 3^{\omega(n)}
  \]
  so the error term is settled.

  Now to the main term. Let
  \[
    V = X \sum_{d, e \divides P(z)} \lambda_d \lambda_e \frac{f([d, e])}{[d, e]}.
  \]
  Write \([d, e] = abc\) where \(d = ab, e = bc\) and \((a, b) = (b, c) = (a, c) = 1\).
\end{proof}











\printindex
\end{document}

% www.thomasbloom.org/ant.html