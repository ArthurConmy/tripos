\documentclass[a4paper]{article}

\def\npart{III}

\def\ntitle{Analytic Number Theory}
\def\nlecturer{T.\ F.\ Bloom}

\def\nterm{Lent}
\def\nyear{2019}

\input{header}

\usepackage{cancel}

\newtheorem*{fact}{Fact}

\begin{document}

\input{titlepage}

\tableofcontents

\setcounter{section}{-1}

\section{Introduction}

Analytic number theory is the study of numbers using analysis. In particular it answers quantitative questions. ``Numbers'' means natural numbers in this course, which excludes \(0\).

\begin{eg}\leavevmode
  \begin{enumerate}
  \item How many primes are there? We know there are infinitely many but can we have a more precise answer? Let \(\pi(x)\) be the number of primes smaller than or equal to \(x\). Then by the famous prime number theorem, \(\pi(x) \sim \frac{x}{\log x}\).
  \item How may twin primes are there? It is not known whethere there are infinitely many. From 2014 Zhang, Maynard, Polymath, there are infinitely many primes at most \(246\) apart. It's been conjectured that the asymptotic bound is \(\sim \frac{x}{(\log x)^2}\).
  \item How many primes are there congruent to \(a\) mod \(q\) where \((a, q) = 1\)? There are infinitely many by Dirichlet's theorem. The guess is \(\frac{1}{\varphi(q)}\frac{x}{\log x}\). This is known for small \(q\).
  \end{enumerate}
\end{eg}

The course is divided into four parts:
\begin{enumerate}
\item elementary techniques (using real analysis),
\item sieve methods,
\item Riemann zeta function/Prime number theorem (using complex analysis),
\item primes in arithmetic progression. 
\end{enumerate}

\section{Elementary techniques}

Review of asymptotic notations:
\begin{itemize}
\item Landau notation: \(f(x) = O(g(x))\) if there is \(C > 0\) such that \(|f(x)| \leq C|g(x)|\) for all large enough \(x\).
\item Vinogradov notation: \(f \ll g\) is the same as \(f = O(g)\).
\item \(f \sim g\) if \(\lim_{x \to \infty} \frac{f(x)}{g(x)} = 1\), i.e.\ \(f = (1 + o(1)) g\).
\item \(f = o(g)\) if \(\lim_{x \to \infty} \frac{f(x)}{g(x)} = 0\).
\end{itemize}

\subsection{Arithmetic functions}

These are just functions \(f: \N \to \C\). An important operation for multiplicative number theory is \emph{multiplicative convolution}\index{multiplicative convolution}
\[
  f * g(n) = \sum_{ab = n} f(a)g(b)
\]

\begin{eg}\leavevmode
  \begin{enumerate}
  \item \(1(n) = 1\) for all \(n\). Caution: this is not identity on \(\N\).
  \item \emph{Möbius function}\index{Möbius function}
    \[
      \mu(n) =
      \begin{cases}
        (-1)^k & \text{ if } n = p_1 \dots p_k \\
        0 & \text{ if \(n\) is divisible by a square}
      \end{cases}
    \]
  \item \emph{Liouville function}
    \[
      \lambda(n) = (-1)^k
    \]
    if \(n = p_1 \dots p_k\) where \(p_i\)'s are not necessarily distinct.
  \item \emph{divisor function}
    \[
      \tau(n) = \#d \text{ such that } d \divides n = \sum_{ab = n} 1 = 1 * 1(n).
    \]
    This is sometimes also denoted by \(d(n)\).
  \end{enumerate}
\end{eg}

\begin{definition}[multiplicative function]\index{multiplicative function}
  An arithmetic function \(f\) is \emph{multiplicative} if
  \[
    f(nm) = f(n)f(m)
  \]
  whenever \((n, m) = 1\).
\end{definition}

In particular a multiplicative function is determined by its values on prime powers \(f(p^k)\).

\begin{fact}
  If \(f\) and \(g\) are multiplicative then so is \(f * g\).
\end{fact}

\begin{eg}
  \(1, \mu, \lambda, \tau\) are multiplicative. \(\log n\) is not multiplicative.
\end{eg}

\begin{fact}[Möbius inversion]\index{Möbius inversion}
  \(1 * f = g\) if and only if \(\mu * g = f\). That is,
  \[
    \sum_{d \divides n} f(d) = g(n)
  \]
  if and only if
  \[
    \sum_{d \divides n} g(d) \mu(\frac{n}{d}) = f(n).
  \]

  For example
  \[
    \sum_{d \divides n} \mu(d) =
    \begin{cases}
      1 & n = 1 \\
      0 & \text{otherwise}
    \end{cases}
    = 1 * \mu (n)
  \]
  is multiplicative so enough to check the identity for prime powers. If \(n = p^k\) then \(\{d: d \divides n\} = \{1, p, \dots, p^k\}\) so LHS equals to \(1 - 1 + 0 + \dots = 0\) unless \(k = 1\) when LHS equals to \(\mu(1) = 1\).
\end{fact}

Our goal is to study primes. Our first might be that we shall work with
\[
  1_p(n) =
  \begin{cases}
    1 & n \text{ prime} \\
    0 & \text{otherwise}
  \end{cases}
\]
as then \(\pi(x) = \sum_{1 \leq n \leq x} 1_p(n)\). But this is very awkward to work with, as to begin with, this is not multiplicative. Instead, we are going to work almost exclusively with \emph{von Mangoldt function}\index{von Mangoldt function}
\[
  \Lambda(n) =
  \begin{cases}
    \log p & n = p^k \\
    0 & \text{otherwise}
  \end{cases}
\]
``assign weight \(\log p\) to prime power \(n\)''

\begin{lemma}
  \[
    1 * \Lambda = \log.
  \]
  and
  \[
    \mu * \log = \Lambda.
  \]
\end{lemma}

\begin{proof}
  The second part follows from Möbius inversion. Thus if \(n = p_1^{k_1} \dots p_r^{k_r}\),
  \begin{align*}
    1 * \Lambda(n)
    &= \sum_{d \divides n} \Lambda(d)
    = \sum_{i = 1}^r \sum_{j = 1}^{k_i} \Lambda(p_i^j) \\
    &= \sum_{i = 1}^r \sum_{j = 1}^{k_i} \log(p_i)
    = \sum_{i = 1}^r k_i \log p_i \\
    &= \sum_{i = 1}^r \log (p_i^{k_i})
    = \log n
  \end{align*}
\end{proof}

Therefore
\begin{align*}
  \Lambda(n)
  &= \sum_{d \divides n} \mu(d) \log(\frac{n}{d}) \\
  &= \log n \sum_{d \divides n} \mu(d) - \sum_{d \divides n} \mu(d) \log d \\
  &= - \sum_{d \divides n} \mu(d) \log d
\end{align*}
For example
\[
  \sum_{1 \leq n \leq x} \Lambda(n)
  = - \sum_{1 \leq n \leq x} \sum_{d \divides n} \mu(d) \log d
  = - \sum_{d \leq x} \mu(d) \log d \left(\sum_{1 \leq n \leq x, d \divides n} 1\right)
\]
by reversing summation. But now the term in the inner summation is very easy to understand:
\[
  \sum_{1 \leq n \leq x, d \divides n} 1 = \floor*{\frac{x}{d}} = \frac{x}{d} + O(1).
\]
Thus
\[
  \sum_{1 \leq n \leq x} \Lambda(n)
  = -x \sum_{d \leq x} \mu(d) \frac{\log d}{d} + O\left(\sum_{d \leq x} \mu(d) \log d\right).
\]
We'll see more of these examples.

\subsection{Summation}

Given an arithmetic function \(f\), we can ask for estimates of \(\sum_{1 \leq n \leq x} f(n)\). We say that \(f\) has \emph{average order}\index{average order} \(g\) if
\[
  \sum_{1 \leq n \leq x} f(n) \sim x g(x).
\]
``average size of \(f\) is \(g\)''.

\begin{eg}\leavevmode
  \begin{enumerate}
  \item \(f = 1\) then
    \[
      \sum_{1 \leq n \leq x} f(x) = \floor{x} = x + O(1) \sim x
    \]
    so average order of \(1\) is \(1\).
  \item \(f(n) = n\):
    \[
      \sum_{1 \leq n \leq x} n \sim \frac{x^2}{2}
    \]
    so average of \(n\) is \(\frac{n}{2}\).
  \end{enumerate}
\end{eg}

\begin{lemma}[partial summation]\index{partial summation}
  If \((a_n)\) is a sequence of complex numbers and \(f\) is such that \(f'\) is continuous. Then
  \[
    \sum_{1 \leq n \leq x} a_n f(n) = A(x) f(x) - \int_1^x A(t)f'(t) dt
  \]
  where \(A(x) = \sum_{1 \leq n \leq x} a_n\).
\end{lemma}

This is the discrete analogus of integration by parts.

\begin{proof}
  Suppose \(x = N\) is an integer. Note that \(a_n = A(n) - A(n - 1)\), so
  \begin{align*}
    \sum_{1 \leq n \leq N} a_nf(n)
    &= \sum_{1 \leq n \leq N} f(n) (A(n) - A(n - 1)) \\
    &= A(N)f(N) - \sum_{n = 1}^{N - 1} A(n) (f(n + 1) - f(n))
  \end{align*}
  Now
  \[
    f(n + 1) - f(n) = \int_n^{n + 1} f'(t) dt
  \]
  so
  \begin{align*}
    \sum_{1 \leq n \leq N} a_n f(n)
    &= A(N)f(N) - \sum_{n = 1}^{N - 1} A(n) \int_n^{n + 1} f'(t) dt \\
    &= A(N)f(N) - \int_1^N A(t) f'(t) dt
  \end{align*}
  where the last step is because \(A(n) = A(t)\) for \(t \in [n, n + 1)\).

  If \(N = \floor x\) then
  \[
    A(x)f(x)
    = A(N)f(x)
    = A(N) f(N) + \int_N^x f'(t) dt.
  \]
\end{proof}

As a simple application

\begin{lemma}
  \[
    \sum_{1 \leq n \leq x} \frac{1}{n} = \log x + \gamma + O(\frac{1}{x}).
  \]
\end{lemma}

\begin{proof}
  Partial summation with \(f(x) = \frac{1}{x}\) and \(a_n = 1\), so \(A(x) = \floor x\). Therefore
  \begin{align*}
    \sum_{1 \leq n \leq x} \frac{1}{n}
    &= \frac{\floor x}{x} + \int_1^x \frac{\floor t}{t^2} dt
    \intertext{Write \(\floor t= t - \{t\}\),} \\
    &= 1 + O(\frac{1}{x}) + \int_1^x \frac{1}{t} dt - \int_1^x \frac{\{t\}}{t^2} dt \\
    &= 1 + O(\frac{1}{x}) + \log x - \int_1^\infty \frac{\{t\}}{t^2} dt + \underbrace{\int_x^\infty \frac{\{t\}}{t^2} dt}_{\leq \int_x^\infty \frac{1}{t^2} dt \leq \frac{1}{x}} \\
    &= \gamma + O(\frac {1}{x}) + \log x + O(\frac{1}{x}) \\
    &= \log x + \gamma + O(\frac{1}{x})
  \end{align*}
\end{proof}

This is an amazing result and the only thing we did is to replace the discrete summation by the continuous analogue to it. In essence this is the whole reason analytic number theory works.

\(\gamma\) can be seen as a measure of the difference between between \(\log\) and its discrete approximation. It is called \emph{Euler-Mascheroni constant}\index{Euler-Mascheroni constant}. 
Surprisingly little is known about \(\gamma\). It is approximately \(0.577\dots\). We don't even know if \(\gamma\) is rational or not.

\begin{lemma}
  \[
    \sum_{1 \leq n \leq x} \log n = x \log x - x + O(\log x).
  \]
\end{lemma}

\begin{proof}
  Partial summation with \(f(x) = \log x, a_n = 1\) so \(A(x) = \floor{x}\). As a side note, in the previous example, most error comes from the integral term (the mass is evenly distributed). By constrast in this example most error comes from the ``sum'' term.
  \begin{align*}
    \sum_{1 \leq n \leq x} \log n
    &= \floor{x} \log x - \int_1^x \frac{\floor{t}}{t} dt \\
    &= x \log x + O(\log x) - \int_1^x dt + O(\int_1^x \frac{1}{t}dt) \\
    &= x \log x + O(\log x) - x + O(\log x)
  \end{align*}
\end{proof}

\subsection{Divisor function}

Recall that
\[
  \tau(n) = 1 * 1(n) = \sum_{ab = n} 1 = \sum_{d \divides n} 1.
\]

\begin{theorem}
  \[
    \sum_{1 \leq n \leq x} \tau(n) = x \log x + (2 \gamma - 1) x + O(x^{1/2})
  \]
  so in particular average order of \(\tau\) is \(\log\).
\end{theorem}

\begin{proof}
  First attempt:
  \begin{align*}
    \sum_{1 \leq n \leq x} \tau(n)
    &= \sum_{1 \leq n \leq x} \sum_{d \divides n} 1
    = \sum_{1 \leq d \leq x} \sum_{1 \leq n \leq x, d \divides n} 1 \\
    &= \sum_{1 \leq d \leq x} \floor*{\frac{x}{d}} \\
    &= \sum_{1 \leq d \leq x} \frac{x}{d} + O(x)
    = x \sum_{1 \leq d \leq x} \frac{1}{d} + O(x) \\
    &= x\log x + \gamma x + O(x)
  \end{align*}
  This is not a very good bound (the error might be as large as one of the terms!) but shows that at least the first term is correct. The main drawback is we used the estimate
  \[
    \sum_{1 \leq d \leq x} O(1) = O(x).
  \]

  To reduce the error term, we use \emph{(Dirichlet's) hyperbola trick}\index{hyperbola trick}
  \[
    \sum_{1 \leq n \leq x} \tau(n)
    = \sum_{1 \leq n \leq x} \sum_{ab = n} 1
    = \sum_{ab \leq x} 1
    = \sum_{a \leq x} \sum_{b \leq x/a} 1
  \]
  The intuition is like this: \(\sum_{1 \leq n \leq x} \tau(n)\) counts the number of integral points below the hyperbola \(k_1k_2 = x\) in the first quadrant. The old methods amounts to an estimation by integral, while in the new method we count the number of points lying below the line \(k_2 = x^{1/2}\), add the number of points to the left of \(k_1 = x^{1/2}\), and finally subtract those points in the box \([0, x^{1/2}]^2\) which are double counted.

  Thus when summing over \(ab \leq x\), we can sum over \(a \leq x^{1/2}\) and \(b \leq x^{1/2}\) respectively, and then minus pairs \(a, b \leq \sqrt x\). Thus
  \begin{align*}
    \sum_{1 \leq n \leq x} \tau(n)
    &= \sum_{a \leq x^{1/2}} \sum_{b \leq x/a} 1 + \sum_{b \leq x^{1/2}} \sum_{a \leq x/b} 1 - \sum_{a, b \leq x^{1/2}} 1 \\
    &= 2 \sum_{a \leq x^{1/2}} \floor*{\frac{x}{a}} - \floor{x^{1/2}}^2 \\
    &= 2 \sum_{a \leq x^{1/2}} \frac{x}{a} + O(x^{1/2}) - x + O(x^{1/2}) \\
    &= 2x\log x^{1/2} + 2 \gamma x - x + O(x^{1/2}) \\
    &= x \log x + (2\gamma - 1) x + O(x^{1/2})
  \end{align*}
\end{proof}

\begin{remark}
  Improving this \(O(x^{1/2})\) error term is a famous and hard problem. Probably \(O(x^{1/4 + \varepsilon})\)? The best result so far is \(O(x^{0.3149})\).
\end{remark}

A note on average order: \(\tau\) has average order \(\log\) does not mean \(\tau(n) \ll \log n\), i.e.\ average order does not imply individual values.

\begin{theorem}
  For all \(n\)
  \[
    \tau(n) \leq n^{O(\frac{1}{\log \log n})}.
  \]
  In particular \(\tau(n) \ll_\varepsilon n^\varepsilon\) for all \(\varepsilon > 0\) where \(\ll_\varepsilon\) means that \(|\tau(n)| \leq C_\varepsilon |n^\varepsilon|\) eventually where \(C_\varepsilon\) is a constant depending on \(\varepsilon\).
\end{theorem}

As a side note, asymptotic bounds such as \(\log \log n\) are quite common in analytic number theory and here is how to reason with them: as \(n \to \infty\), \(\log n\) grows slower than any polynomial, so \(\log \log n\) grows slower than \(\log P(n)\) for any polynomial. Another way is to write \(n = e^{\log n}\) and then
\[
  n^{O(\frac{1}{\log \log n})} = \exp (O(\frac{\log n}{\log \log})).
\]

\begin{proof}
  \(\tau\) is multiplicative so enough to calculate at prime powers. \(\tau(p^k) = k + 1\) so if \(n = p_1^{k_1} \cdots p_r^{k_r}\) then \(\tau(n) = \prod_{i = 1}^r (k_i + 1)\). Let \(\varepsilon > 0\) to be chosen later and consider the ratio
  \[
    \frac{\tau(n)}{n^\varepsilon}
    = \prod_{i = 1}^r \frac{k_i + 1}{p^{k_i\varepsilon}}.
  \]
  Now entering the trick: split into big and small cases. Note as \(p\) goes large, \(\frac{k + 1}{p^{k \varepsilon}} \to 0\). In particular if \(p \geq 2^{1/\varepsilon}\) then
  \[
    \frac{k + 1}{p^{k\varepsilon}} \leq \frac{k + 1}{2^k} \leq 1.
  \]
  What about small \(p\)? It is important to remind ourselves that we're dealing with primes and \(p\) can't run below \(2\). In this case
  \[
    \frac{k + 1}{p^{k\varepsilon}} \leq \frac{k + 1}{2^{k\varepsilon}} \leq \frac{1}{\varepsilon}
  \]
  this is because \(x + \frac{1}{2} \leq 2^x\) for \(x \geq 0\) so \(\varepsilon h + \varepsilon \leq 2^{k \varepsilon}\) if \(\varepsilon \leq \frac{1}{2}\) (the details are not so important compared to the conclusion that this can be bounded). Therefore
  \[
    \frac{\tau(n)}{n^\varepsilon}
    \leq \prod_{i = 1, p_i < 2^{1/\varepsilon}}^r \frac{k_i + 1}{p^{k_i \varepsilon}}
    \leq \left( \frac{1}{\varepsilon} \right)^{\pi(2^{1/\varepsilon})}
    \leq \left( \frac{1}{\varepsilon} \right)^{2^{1/\varepsilon}}\footnote{Behold what a wasteful bound we give in the last inequality! But that almost has no effect in the final result.}.
  \]
  Now we need to choose an optimal \(\varepsilon\). Another trick: if we want to minimise \(f(x) + g(x)\), choose \(x\) such that \(f(x) = g(x)\). Have
  \[
    \tau(n)
    \leq n^\varepsilon \varepsilon^{-2^{1/\varepsilon}}
    = \exp (\varepsilon \log n + 2^{1/\varepsilon} \log (1/\varepsilon)).
  \]
  Choose \(\varepsilon\) such that \(\log n \approx 2^{1/\varepsilon}\) (again, only a rough guess is needed), i.e.\ \(\varepsilon \approx \frac{1}{\log \log n}\) and get
  \begin{align*}
    \tau(n)
    &\leq n^{\frac{1}{\log \log n}} (\log \log n)^{2^{\log \log n}} \\
    &= n^{\frac{1}{\log \log n}} \exp ((\log n)^{\log 2} \log \log \log n) \\
    &\leq n^{O(\frac{1}{\log \log n})}.
  \end{align*}
\end{proof}

\subsection{Estimates for the primes}

Recall that
\begin{align*}
  \pi(x) &= \# \{\text{primes } \leq x\} = \sum_{1 \leq n \leq x} 1_p(n) \\
  \psi(x) &= \sum_{1 \leq n \leq x} \Lambda(n)
\end{align*}
The second one is sometimes known as \emph{Chebyshev's function}\index{Chebyshev's function}. Prime number theorem asserts that \(\pi(x) \sim \frac{x}{\log x}\) or equivalently \(\psi(x) \sim x\) (this equivalence will be shown later).

Although Euclid's prove in 300 BC the infinitude of prime, It was 1850 before the correct magnitude of \(\pi(x)\) was proved. Chebyshev showed that
\[
  \pi(x) \asymp \frac{x}{\log x}
\]
where \(f \asymp g\) means that \(g \ll f \ll g\).

\begin{theorem}[Chebyshev]
  \[
    \psi(x) \asymp x.
  \]
\end{theorem}

\begin{proof}
  First we'll prove the lower bound, i.e.\ \(\psi(x) \gg x\). Recall that \(1 * \Lambda = \log\). Here comes in a genuine\footnote{Read unmotivated.} trick: find something that equals \(1\). Then \(\psi(x) = \sum_{1 \leq n \leq x} \Lambda(n) \cdot 1\) can be rearranged. We'll use the identity
  \[
    \floor{x} = 2 \floor*{\frac{x}{2}} + 1
  \]
  for \(x \geq 0\). Either see it directly or a simple verification: if \(\frac{x}{2} = n + \theta\) where \(\theta \in [0, 1)\) then \(\floor*{\frac{x}{2}} = n\) and \(\floor{x} = \floor{2n + 2\theta} = 2n \text{ or } 2n + 1\). Then
  \begin{align*}
    \psi(x)
    &\geq \sum_{1 \leq n \leq x} \Lambda(x) \left( \floor*{\frac{x}{n}} - 2 \floor*{\frac{x}{2n}} \right)\\
    \intertext{Note that \(\floor*{\frac{x}{n}} = \sum_{m \leq x/n} 1\),}
    &=\sum_{n \leq x} \Lambda(n) \sum_{m \leq x/n} 1 - 2 \sum_{n \leq x} \Lambda(n) \sum_{m \leq x/2n} 1 \\
    &=  \sum_{nm \leq x} \Lambda(n) - 2 \sum_{nm \leq x/2} \Lambda(n) \\
    \intertext{Write \(d = nm\),}
    &= \sum_{d \leq x} 1 * \Lambda(d) - 2 \sum_{d \leq x/2} 1 * \Lambda(d) \\
    &= \sum_{d \leq x} \log d - 2 \sum_{d \leq x/2} \log d \\
    &= x \log x - x + O(\log x) - 2 \left( \frac{x}{2} \log \frac{x}{2} - \frac{x}{2} + O(\log x) \right) \\
    &= (\log 2) x + O(\log x) \\
    &\gg x
  \end{align*}

  For the upper bound,
  \[
    \floor{x} = 2 \floor*{\frac{x}{2}} + 1
  \]
  for \(x \in (1, 2)\) so
  \begin{align*}
    \psi(x) - \psi(\frac{x}{2})
    &= \sum_{x/2 < n < x} \Lambda(n) \\
    &\leq \sum_{1 \leq n \leq x} \Lambda(n) \left( \floor*{\frac{x}{n}} - 2 \floor*{\frac{x}{2n}} \right) \\
    & \leq (\log 2) x + O(\log x)
  \end{align*}
  Thus
  \begin{align*}
    \psi(x)
    &= (\psi(x) - \psi(x/2)) + (\psi(x/2) - \psi(x/4)) + \dots \\
    &\leq \log 2 \cdot (x + x/2 + x/4 + \dots ) \\
    &= 2 \log 2 \cdot x
  \end{align*}
  Thus we have shown
  \[
    (\log 2) x \leq \psi(x) \leq (\log 4) x.
  \]
\end{proof}

\begin{lemma}
  \[
    \sum_{p \leq x} \frac{\log p}{p} = \log x + O(1).
  \]
\end{lemma}

\begin{proof}
  Recall that \(\log = 1 * \Lambda\) so
  \begin{align*}
    \sum_{n \leq x} \log n
    &= \sum_{ab \leq x} \Lambda(a)
    = \sum_{a \leq x} \Lambda(a) \sum_{b \leq x/a} 1 \\
    &= \sum_{a \leq x} \Lambda(a) \floor*{\frac{x}{a}} \\
    &= x \sum_{a \leq x} \frac{\Lambda(a)}{a} + O(\psi(x)) \\
    &= x \sum_{a \leq x} \frac{\Lambda(a)}{a} + O(x) \\
  \end{align*}
  Note where we used Chebyshev's bound. Since
  \[
    \sum_{n \leq x} \log x = x\log x - x + O(\log x),
  \]
  have
  \[
    \sum_{n \leq x} \frac{\Lambda(n)}{n}
    = \log x - 1 + O(\frac{\log x}{x}) + O(1)
    = \log x + O(1)
  \]
  Remain to note the contribution from prime powers \(\geq 2\) are ``small'':
  \begin{align*}
    \sum_{p \leq x} \sum_{n = 2}^\infty \frac{\log p}{p^n}
    &= \sum_{p \leq x} \log p \sum_{n = 2}^\infty \frac{1}{p^n} \\
    &= \sum_{p \leq x} \frac{\log p}{p^2 - p} \\
    &\leq \sum_{p = 2}^\infty \frac{1}{p^{3/2}} \\
    &= O(1)
  \end{align*}
  so
  \[
    \sum_{n \leq x} \frac{\Lambda(n)}{n} = \sum_{p \leq x} \frac{\log p}{p} + O(1).
  \]
\end{proof}

\begin{lemma}
  \[
    \pi(x) = \frac{\psi(x)}{\log x} + O(\frac{x}{(\log x)^2}).
  \]
  In particular \(\pi(x) \asymp \frac{x}{\log x}\) and prime number theorem \(\pi(x) \sim \frac{x}{\log x}\) is equivalent to \(\psi(x) \sim x\).
\end{lemma}

\begin{proof}
  Idea is to use partial summation: let
  \[
    \theta(x)
    = \sum_{p \leq x} \log p
    = \pi(x) \log x - \int_1^x \frac{\pi(t)}{t} dt.
  \]
  First problem: \(\psi(x)\) sums over not only primes but also prime powers. We can use a previous trick to remove contributions from prime powers:
  \begin{align*}
    \psi(x) - \theta(x)
    &= \sum_{k = 2}^\infty \sum_{p^k \leq x} \log p
    = \sum_{k = 2}^\infty \theta(x^{1/k}) \\
    &\leq \sum_{k = 2}^{\log x} \psi(x^{1/k})
    \leq \sum_{k = 2}^{\log x} x^{1/k} \\
    &\leq x^{1/2} \log x
  \end{align*}
  Therefore
  \begin{align*}
    \psi(x)
    &= \pi(x) \log x + O(x^{1/2} \log x) - \int_1^x \frac{\pi(t)}{t} dt \\
    \intertext{As \(\pi(t) \leq \frac{t}{\log t}\),}
    &= \pi(x) \log x + O(x^{1/2} \log x) + O(\int_1^x \frac{1}{\log t} dt) \\
    &= \pi(x) \log x + O(\frac{x}{\log x})
  \end{align*}
  For \(\pi(t) < \frac{t}{\log t}\), note the trivial bound \(\pi(t) \leq t\) so
  \[
    \psi(x) = \pi(x) \log x + O(x^{1/2} \log x) + O(x)
  \]
  so \(\pi(x)\log x = O(x)\). Thus we used the trivial bound to get a better bound and use that to do actual work.
\end{proof}

\begin{lemma}
  \[
    \sum_{p \leq x} \frac{1}{p} = \log \log x + b + O(\frac{1}{\log x})
  \]
  where \(b\) is some constant.
\end{lemma}

Compare to \(\sum_{1 \leq n \leq x} \frac{1}{n}\).

\begin{proof}
  Partial summation. Let
  \[
    A(x) = \sum_{p \leq x} \frac{\log p}{p} = \log x + R(x)
  \]
  where \(R(x) = O(1)\). Then (summing from \(2\) to prevent \(\log t = 0\))
  \begin{align*}
    \sum_{2 \leq p \leq x} \frac{1}{p}
    &= \frac{A(x)}{\log x} + \int_2^x \frac{A(t)}{t (\log t)^2} dt \\
    &= 1 + O(\frac{1}{\log x}) + \int_2^x \frac{1}{t \log t} dt + \int_2^x \frac{R(t)}{t(\log t)^2} dt
  \end{align*}
  Note that \(\int_2^\infty \frac{R(t)}{t (\log t)^2} dt\) exists, say \(C\). Then
  \begin{align*}
    \sum_{2 \leq p \leq x} \frac{1}{p}
    &= 1 + C + O(\frac{1}{\log x}) + \log \log x - \log \log 2 + O(\int_x^\infty \frac{1}{t (\log t)^2} dt) \\
    &= \log \log x + b + O(\frac{1}{\log x})
  \end{align*}
  It turns out \(b\) can be expressed in terms of \(\gamma\).
\end{proof}

\begin{theorem}[Chebyshev]
  If \(\pi(x) \sim c \frac{x}{\log x}\) then \(c = 1\).
\end{theorem}

Note that this does not prove prime number theorem. Historically this is a surprise: a following corollary says that if \(\pi(x) \sim \frac{x}{\log x - A(x)}\) then \(A \sim 1\). But Legendre and Gauss et al have conjectured that \(A \approx 1.08 \dots\), just by looking up the prime table.

\begin{proof}
  Partial summation on \(\sum_{p \leq x} \frac{1}{p}\):
  \begin{align*}
    \sum_{p \leq x} \frac{1}{p}
    &= \frac{\pi(x)}{x} + \int_1^x \frac{\pi(t)}{t^2} dt \\
    \intertext{If \(\pi(x) = (c + o(1)) \frac{x}{\log x}\) then}
    &= \frac{c}{\log x} + o(\frac{1}{\log x}) + (c + o(1)) \int_1^x \frac{1}{t \log t} dt \\
    &= O(\frac{1}{\log x}) + (c + o(1)) \log \log x
  \end{align*}
  But
  \[
    \sum_{p \leq x} \frac{1}{p} = (1 + o(1)) \log \log x
  \]
  so \(c = 1\).
\end{proof}

\begin{lemma}
  \[
    \prod_{p \leq x} \left( 1 - \frac{1}{p} \right)^{-1} = c\log x + O(1)
  \]
  where \(c\) is some constant.
\end{lemma}

\begin{proof}
  We have only dealt with summations so far so take \(\log\),
  \begin{align*}
    \log \prod_{p \leq x} \left( 1 - \frac{1}{p} \right)^{-1}
    &= - \sum_{p \leq x} \log (1 - \frac{1}{p}) \\
    &= \sum_{p \leq x} \sum_k \frac{1}{k p^k} \\
    &= \sum_{p \leq x} \frac{1}{p} + \sum_{k \geq 2} \sum_{p \leq x} \frac{1}{kp^k} \\
    &= \log \log x + c' + O(\frac{1}{\log x}).
  \end{align*}
  using \(\log (1 - t) = - \sum_k \frac{t^k}{k}\).

  To undo the \(\log\), note that \(e^x = 1 + O(x)\) for \(|x| \leq 1\) so
  \begin{align*}
    \prod_{p \leq x} \left( 1 - \frac{1}{p} \right)^{-1}
    &= c \log x \exp (O(\frac{1}{\log x})) \\
    &= c \log x (1 + O(\frac{1}{\log x})) \\
    &= c \log x + O(1)
  \end{align*}
  It turns out that \(c = e^\gamma \approx 1.78 \dots\).
\end{proof}

\subsubsection{Aside: Why is prime number theorem so hard?}

It seems that we've made quite a progress without too much effort. But how far are we from prime number theorem and if the answer is ``quite far'', what makes it so resistant to elementary methods?

Probabilistic heuristic: fix \(p\) prime, ``probability'' that a random \(n\) satisfies \(p \divides n\) is \(\frac{1}{p}\). What is the ``probability'' that \(n\) is prime then? \(n\) is a prime if and only if \(n\) has no prime divisors \(p \leq n^{1/2}\). Guess that the events ``divisble by \(p\)'' are independent, then ``probability'' that \(n\) is prime is roughly
\[
  \prod_{p \leq n^{1/2}} \left( 1 - \frac{1}{p} \right)
  \approx \frac{1}{c \log n^{1/2}}
  = \frac{2}{c} \frac{1}{\log n}.
\]
Thus use some questionable squiggles,
\[
  \pi(x)
  = \sum_{n \leq x} 1_{n \text{ prime}}
  \approx \frac{2}{c} \sum_{n \leq x} \frac{1}{\log n}
  \approx \frac{2}{c} \frac{x}{\log x}
  \approx 2e^{-\gamma} \frac{x}{\log x}
\]
This constant is approximately \(1.122\dots\), which contradicts Chebyshev's theorem. Therefore somehow the heuristics is wrong: it gives 12\% more prime than should.

One reason is that the error terms are so close to the main term that when we do \(\approx\) they accummulate and excees the main term. Another reason is of course that the ``independence'' of primes are completely false. From an analytic point of view, this can be seen as saying that the ``interference terms'' are not so small that they can be ignored.

This may explain why heuristics don't work. But can we bound \(\pi\) by elementary methods? Recall that \(\mu * \log = \Lambda\) so
\begin{align*}
  \psi(x)
  &= \sum_{n \leq x} \Lambda(n) \\
  &= \sum_{ab \leq x} \mu(a) \log b \\
  &= \sum_{a \leq x} \mu(a) \left( \sum_{b \leq x/a} \log b \right)
\end{align*}
Recall that
\[
  \sum_{m \leq x} \log m = x \log x - x + O(\log x),
\]
but if we just plug this in we will get a trouble. Instead use another trick: consider
\[
  \sum_{m \leq x} \tau(m) = x \log x + (2 \psi - 1) x + O(x^{1/2}).
\]
Thus
\[
  \psi(x)
  = \sum_{a \leq x} \mu(a) \left( \sum_{b \leq x/a} \tau(b) - 2\gamma \frac{x}{a} + O(\frac{x^{1/2}}{a^{1/2}}) \right).
\]
The first term is (essentially \(\mu * \tau = 1\))
\begin{align*}
  \sum_{ab \leq x} \mu(a)\tau(b)
  &= \sum_{abc \leq x} \mu(a) \\
  &= \sum_{b \leq x} \sum_{ac \leq x/b} \mu(a) \\
  &= \sum_{b \leq x} \sum_{d \leq x/b} \mu * 1(d) \\
  &= \floor x \\
  &= x + O(1)
\end{align*}
and the first error term is
\[
  -2\gamma \sum_{a \leq x} \mu(a) \frac{x}{a} = O(x\sum_{a \leq x} \frac{\mu(a)}{a})
\]
so still need to show that
\[
  x \sum_{a \leq x} \frac{\mu(a)}{a} = O(1).
\]
Well it turns out that this is equivalent to prime number theorem! This constant can be shown to be \(1/\zeta(1)\). As \(\zeta\) has a pole at \(z = 1\), this is indeed true.

\subsection{Selberg's identity and on elementary proof of prime number theorem}

Define Selberg's function
\[
  \Lambda_2(n) = \mu* (\log )^2(n) = \sum_{ab = n} \mu(a) (\log b)^2.
\]
The idea is to prove ``prime number theorem for \(\Lambda_2\)'' with elementary methods. The intuition is that \(\Lambda_2\) is like \(\Lambda\) multiplied by \(\log\) and if we do the same expansion as before, hopefully we can get
\[
  \sum_{n \leq x} \Lambda_2(n) = \text{main term} + O(x),
\]
but now this is now an acceptable error!

\begin{lemma}\leavevmode
  \begin{enumerate}
  \item \(\Lambda_2(n) = \Lambda(n) \log n + \Lambda * \Lambda (n)\).
  \item \(0 \leq \Lambda_2(n) \leq (\log n)^2\).
  \item If \(\Lambda_2(n) \neq 0\) then \(n\) has at most 2 distinct prime divisors.
  \end{enumerate}
\end{lemma}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item Use Möbius inversion suffices to show
    \[
      \sum_{d \divides n} (\Lambda(d) \log d + \Lambda * \Lambda(d)) = (\log n)^2.
    \]
    Start by expanding out,
    \begin{align*}
      \sum_{d \divides n} (\Lambda(d) \log d + \Lambda * \Lambda(d))
      &= \sum_{d \divides n} \Lambda(d) \log d + \sum_{ab \divides n} \Lambda(a) \Lambda(b) \\
      &= \sum_{d \divides n} \log d + \sum_{a \divides n} \Lambda(a) \underbrace{\sum_{b \divides \frac{n}{a}} \Lambda(b)}_{= \log (n/a)} \\
      &= \sum_{d \divides n} \log d + \sum_{d \divides n} \Lambda(d) \log \frac{n}{d} \\
      &= \log n \sum_{d \divides n} \Lambda(d) \\
      &= (\log n)^2
    \end{align*}
  \item \(\Lambda_2(n) \geq 0\) since both terms on RHS in 1 are nonnegative. Since
    \[
      \sum_{d \divides n} \Lambda_2(d) = (\log n)^2,
    \]
    \(\Lambda_2(n) \leq (\log n)^2\).
  \item Note that if \(n\) is divisible by 2 distinct primes then \(\Lambda(n) = 0\), and
    \[
      \Lambda * \Lambda(n) = \sum_{ab \divides n} \Lambda(a) \Lambda(b) = 0
    \]
    since at least one of \(a\) or \(b\) has \(\geq 2\) distinct prime divisors.
  \end{enumerate}
\end{proof}

As such while \(\Lambda\) can be thought as the indicator function for numbers with exactly 1 prime divisor, weighted by \(\log\), \(\Lambda_2\) can be thought as the indicator function for numbers with a pair of prime divisors, weighted by \((\log)^2\).

\begin{theorem}[Selberg]
  \[
    \sum_{n \leq x} \Lambda_2(n) = 2x \log x + O(x).
  \]
\end{theorem}

\begin{proof}
  \begin{align*}
    \sum_{n \leq x} \Lambda_2(n)
    &= \sum_{n \leq x} \mu * (\log)^2 (n) \\
    &= \sum_{ab \leq x} \mu(a) (\log b)^2 \\
    &= \sum_{a \leq x} \mu(a) \left( \sum_{b \leq x/a} (\log b)^2 \right)
  \end{align*}
  By partial summation,
  \[
    \sum_{m \leq x} (\log m)^2
    = x (\log x)^2 - 2x \log x + 2x + O((\log x)^2).
  \]
  We want to use the same trick and substitute sum of divisor function for the leading term. First we have to manufacture a \(x (\log x)^2\) term. By partial summation with
  \[
    A(t) = \sum_{n \leq t} \tau(n) = t \log t + Ct + O(t^{1/2}),
  \]
  have
  \begin{align*}
    \sum_{m \leq x} \frac{\tau(m)}{m}
    &= \frac{A(x)}{x} + \int_1^x \frac{A(t)}{t^2} dt \\
    &= \log x + C + O(x^{-1/2}) + \int_1^x \frac{\log t}{t} dt + C \int_1^x \frac{1}{t} dt + O(\int_1^x \frac{1}{t^{3/2}} dt) \\
    &= \frac{(\log x)^2}{2} + C_1 \log x + C_2 + O(x^{-1/2})
  \end{align*}
  Since we dislike \(\log\), we replace it by \(\sum_{m \leq x} \tau(m)\) to get
  \[
    \frac{x (\log x)^2}{2}
    = \sum_{m \leq x} \tau(m) \frac{x}{m} + C_1' \sum_{m \leq x} \tau(m) + C_2'x + O(x^{1/2}).
  \]
  Substituting back,
  \[
    \sum_{m \leq x} (\log m)^2
    = 2 \sum_{m \leq x} \tau(m) \frac{x}{m} + C_3 \sum_{m \leq x} \tau(m) + C_4 x + O(x^{1/2})
  \]
  so
  \begin{align*}
    \sum_{n \leq x} \Lambda_2(n)
    &= 2 \sum_{a \leq x} \mu(a) \sum_{b \leq x/a} \frac{\tau(b)x}{ab} + C_5 \sum_{a \leq x} \mu(a) \sum_{b \leq x/a} \tau(b) \\
    &+ C_6 \sum_{a \leq x} \mu(a) \frac{x}{a} + O(\sum_{a \leq x} \frac{x^{1/2}}{a^{1/2}}).
  \end{align*}
  We analyse the error terms one by one, starting from the back. First note that
  \[
    x^{1/2} \sum_{a \leq x} \frac{1}{a^{1/2}} = O(x).
  \]
  Secondly
  \begin{align*}
    x \sum_{a \leq x} \frac{\mu(a)}{a}
    &= \sum_{a \leq x} \mu(a) \floor*{\frac{x}{a}} + O(x) \\
    &= \sum_{a \leq x} \mu(a) \sum_{b \leq x/a} 1 + O(x) \\
    &= \sum_{d \leq x} \mu * 1(d) + O(x) \\
    &= 1 + O(x) \\
    &= O(x)
  \end{align*}
  Thirdly, (again essentially \(\mu * \tau = 1\))
  \begin{align*}
    \sum_{a \leq x} \mu(a) \sum_{b \leq x/a} \tau(b)
    &= \sum_{a \leq x} \mu(a) \sum_{b \leq x/a}\sum_{cd = b} 1 \\
    &= \sum_{a \leq x} \mu(a) \sum_{cd \leq x/a} 1 \\
    &= \sum_{acd \leq x} \mu(a)
    = \sum_{d \leq x} \sum_{ac \leq x/d} \mu(a) \\
    &= \sum_{d \leq x} \sum_{e \leq x/d} \mu * 1(e) \\
    &= \sum_{d \leq x} 1 \\
    &= O(x)
  \end{align*}
  Collecting what we've done,
  \begin{align*}
    \sum_{n \leq x} \Lambda_2(n)
    &= 2 \sum_{a \leq x} \mu(a) \sum_{b \leq x/a} \frac{\tau(b) x}{ab} + O(x) \\
    &= 2x \sum_{d \leq x} \frac{1}{d} \mu * \tau(d) + O(x) \\
    \intertext{Recall that \(\tau = 1 * 1\) so \(\mu * \tau = \mu * 1 * 1 = 1\),}
    &= 2x \sum_{d \leq x} \frac{1}{d} + O(x) \\
    &= 2x \log x + O(x)
  \end{align*}
\end{proof}

\subsubsection{*A 14-point plan to prove prime number theorem from Selberg's identity}

Let
\[
  r(x) = \frac{\psi(x)}{x} - 1.
\]
Then prime number theorem is the statement that
\[
  \lim_{x \to \infty} |r(x)| = 0.
\]
We will demonstrate how to count from 1 to 14 below. When you finished counting, you will get prime number theorem as a byproduct.

\begin{enumerate}
\item[1] Show that Selberg's identity implies
  \[
    r(x) \log x = - \sum_{n \leq x} \frac{\Lambda(n)}{n} r (\frac{x}{n}) + O(1).
  \]
\item[2] Consider 1 with \(x\) replaced by \(\frac{x}{m}\), summing over \(m\), show
  \[
    |r(x)| (\log x)^2 \leq \sum_{n \leq x} \frac{\Lambda_2(n)}{n} \left|r(\frac{x}{n})\right| + O(\log x).
  \]
\item[3]
  \[
    \sum_{n \leq x} \Lambda_2(n) = 2 \int_1^{\floor x} \log t dt + O(x).
  \]
\item[4 - 6]
  \[
    \sum_{n \leq x} \frac{\Lambda_2(n)}{n} |r(\frac{x}{n})|
    = 2 \int_1^x \frac{r(x/2)}{t \log t} dt + O(\log x).
  \]
\item[7] Let \(V(u) = r(e^u)\). Show that
  \[
    u^2 |V(u)| \leq 2 \int_0^u \int_0^v |V(t)| dt dv + O(u).
  \]
\item[8] Show
  \[
    \alpha = \limsup_{x \to \infty} |r(x)| \leq \limsup_{u \to \infty} \frac{1}{u} \int_0^u |V(t)| dt = \beta.
  \]
\item[9 - 14] If \(\alpha > 0\) then can show from 7 that \(\beta < \alpha\), contradiction. So \(\alpha = 0\). Prime number theorem.
\end{enumerate}

\section{Sieve methods}

Sieve of Eratosthenes: given natural numbers below \(20\), let's cross out all multiples of \(2\) to get
\[
  \begin{array}{cccccccccc}
    1 & \cancel 2 & 3 & \cancel 4 & 5 & \cancel 6 & 7 & \cancel 8 & 9 & \cancel{10} \\
    11 & \cancel{12} & 13 & \cancel{14} & 15 & \cancel{16} & 17 & \cancel{18} & 19 & \cancel{20}
  \end{array}
\]
Next we cross out all multiples of \(3\) to get
\[
  \begin{array}{cccccccccc}
    1 & \cancel 2 & \bcancel 3 & \cancel 4 & 5 & \xcancel 6 & 7 & \cancel 8 & \bcancel 9 & \cancel{10} \\
    11 & \xcancel{12} & 13 & \cancel{14} & \bcancel{15} & \cancel{16} & 17 & \xcancel{18} & 19 & \cancel{20}
  \end{array}
\]
As \(\sqrt{20} < 5\), we know that the numbers left on the list are prime (with the exception of \(1\)). Our interest is in using the sieve to \emph{count} things: we can find how many numbers are left, which by definition are those primes below \(20\) that are not used as sieves, by inclusion-exclusion principle:
\begin{align*}
  \pi(20) + 1 - \pi(\sqrt{20})
  &= 20 - \floor*{\frac{20}{2}} - \floor*{\frac{20}{3}} + \floor*{\frac{20}{6}} \\
  &= 20 - 10 - 6 + 3 \\
  &= 7 
\end{align*}
By the way if there are more sieves then we naturally include more terms in the inclusion-exclusion expansion. Note that the coefficient/sign in front of each term is precisely the Möbius function of the denominator.

\subsection{Setup}

Consider \(A \subseteq \N\) finite, which is the set to be sifted. Let \(P\) be a set of primes, which are those we sift out by. Usually \(P\) is the set of all primes. Let \(z\) be a sifting limit: we sift all primes in \(P\) that are smaller than \(z\). A \emph{sifting function}
\[
  S(A, P; z) = \sum_{n \in A} 1_{(n, P(z)) = 1}
\]
where \(P(z) = \prod_{p \in P, p < z} p\). The goal is to estimate \(S(A, P; z)\).

For \(d\), let
\[
  A_d = \{n \in A: d \divides n\}.
\]
Write
\[
  |A_d| = \frac{f(d)}{d} X + R_d
\]
where \(f\) is completely multiplicative (\(f(mn) = f(m)f(n)\) for all \(m, n\)) and \(f(d) \geq 0\) for all \(d\). Note that
\[
  |A| = \frac{f(1)}{1} X + R_1 = X + R_1
\]
Think of \(R_1\) as the remainder term, \(X\) is roughly the size of \(A\). Extending this analogy, for general \(d\), \(R_d\) is the ``error'' term and \(\frac{X}{d}\) measures the number of elements in the \(0\) residue class of \(d\), assuming they are distributed uniformly. Then \(f(d)\) is a factor that says how the residue class is actually distributed.

We choose \(f\) so that \(f(p) = 0\) if \(p \notin P\) (so \(R_p = |A_p|\)). Finally let
\[
  W_P(z) = \prod_{\substack{p \in P \\ p < z}} \left( 1 - \frac{f(p)}{p} \right),
\]
the probability that it is not divisible by any of the \(p\).

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Sieve of Eratosthenes: \(A = (x, x + y] \cap \N\) and \(P\) is the set of all primes. Then
    \[
      |A_d|
      = \floor*{\frac{x + y}{d}} - \floor*{\frac{x}{d}}
      = \frac{x + y}{d} - \frac{x}{d} + O(1)
      = \frac{y}{d} + O(1)
    \]
    so \(f(d) = 1\) and \(R_d = O(1)\). Have
    \[
      S(A, P; z) = \#\{x < n \leq x + y: p \divides n \implies p \geq z\}.
    \]
    For example if \(z \approx (x + y)^{1/2}\) then
    \[
      S(A, P; z) = \pi(x + y) - \pi(x) + O((x + y)^{1/2}).
    \]
  \item Let \(A = \{1 \leq n \leq y: n = a \mod q\}\). Then
    \[
      A_d = \{1 \leq m \leq \frac{y}{d}: dm = a \mod q\}.
    \]
    The congruence has solutions if and only if \((d, q) \divides a\). Thus
    \[
      |A_d| =
      \begin{cases}
        \frac{(d, q)}{dq} y + O((d, q)) & (d, q) \divides a \\
        O((d, q)) & \text{otherwise}
      \end{cases}
    \]
    So here \(X = \frac{y}{q}\) and
    \[
      f(d) =
      \begin{cases}
        (d, q) & (d, q) \divides a \\
        0 & \text{otherwise}
      \end{cases}
    \]
  \item Count twin primes: let \(A = \{n (n + 2): 1 \leq n \leq x\}\) and let \(P\) be all primes except \(2\). So \(p \divides n(n + 2)\) if and only if \(n = 0 \text{ or } 2 \mod p\). Thus
    \[
      |A_p| = \frac{2x}{p} + O(1).
    \]
    Thus \(f(p) = 2\). By complete multiplicity, \(f(d) = 2^{\omega(d)}\) if \(2 \ndivides d\). Have
    \begin{align*}
      S(A, P; x^{1/2})
      &= \#\{1 \leq p \leq x: p, p + 2 \text{ both primes}\} + O(x^{1/2}) \\
      &= \pi_2(x) + O(x^{1/2})
    \end{align*}
    We would expect \(\pi_2(x) \approx \frac{x}{(\log x)^2}\). We'll prove upper bound using sieves.
  \end{enumerate}
\end{eg}

\begin{theorem}[sieve of Eratosthenes-Legendre]
  \[
    S(A, P; z) = XW_P(z) + O(\sum_{d \divides P(z)} |R_d|).
  \]
\end{theorem}

\begin{proof}
  \begin{align*}
    S(A, P; z)
    &= \sum_{n \in A} 1_{(n, P(z)) = 1} \\
    &= \sum_{n \in A} \sum_{d \divides (n, P(z))} \mu(d) \\
    &= \sum_{n \in A} \sum_{\substack{d \divides n \\ d \divides P(z)}} \mu(d) \\
    &= \sum_{d \divides P(z)} \mu(d) \sum_{n \in A} 1_{d \divides n} \\
    &= \sum_{d \divides P(z)} \mu(d) |A_d| \\
    &= X \sum_{d \divides P(z)} \frac{\mu(d) f(d)}{d} + \sum_{d \divides P(z)} \mu(d) R_d \\
    &= X \prod_{p \in P, p < z} \left(1 - \frac{f(p)}{p} \right) + O(\sum_{d \divides P(z)} |R_d|)
  \end{align*}
\end{proof}

\begin{corollary}
  \[
    \pi(x + y) - \pi(x) \ll \frac{y}{\log \log y}.
  \]
\end{corollary}
By taking \(x = 0\) we see this is much worse bound in \(y\) than Chebyshev. On the other hand, however, we get a uniform bound independent of \(x\)!

\begin{proof}
  In example 1, \(X = y, f = 1\) and \(|R_d| \ll 1\). Thus
  \[
    W_P(z)
    = \prod_{p < z} \left( 1 - \frac{1}{p} \right)
    \ll (\log z)^{-1}
  \]
  and
  \[
    \sum_{d \divides P(z)} |R_d| \ll \sum_{d \divides P(z)} 1 \leq 2^z
  \]
  so
  \[
    \pi(x + y) - \pi(x) \ll \frac{y}{\log z} + 2^z \ll \frac{y}{\log \log y}
  \]
  by letting \(z = \log y\).
\end{proof}

\subsection{Selberg's sieve}

Using sieve of Eratosthenes-Legendre, we only get \(\frac{y}{\log\log y}\) instead of the expected \(\frac{y}{\log y}\). What prevents us from getting the result is that we can't take \(z = y\) --- otherwise the error term will be \(O(2^z) = O(2^y)\), which is much bigger than the main term.

The problem is that we have to consider \(2^z\) many divisors of \(P(z)\) so get \(2^z\) many error terms. However, we can design a different sieve, and only consider those divisors which are small, say \(\leq D\). The key part of Eratosthenes-Legendre sieve is
\[
  1_{(n, P(z)) = 1} = \sum_{d \divides (n, P(z))} \mu(d).
\]
However, for an upper bound, it is enough to use \emph{any} function \(F\) such that
\[
  F(n) \geq
  \begin{cases}
    1 & n = 1 \\
    0 & \text{otherwise}
  \end{cases}
\]
Selberg's observation was that if \((\lambda_i)\) is any sequence of reals with \(\lambda_1 = 1\) then
\[
  F(n) = \left( \sum_{d \divides n} \lambda_d \right)^2
\]
works.

We assume that \(0 < f(p) < p\) for \(p \in P\), which is a reasonable assumption for the sieve to be ``nontrivial'' (if \(f(p) = 0\) then the sieve does nothing and we may well just remove \(p\) from \(P\). If \(f(p) = p\) then it sifts out everything!) The let us define a new multiplicative function \(g\) such that
\[
  g(p) = \left( 1 - \frac{f(p)}{p} \right)^{-1} - 1 = \frac{f(p)}{p - f(p)}.
\]

\begin{theorem}[Selberg's sieve]\index{Selberg's sieve}
  \label{thm:Selberg's sieve}
  For all \(t\),
  \[
    S(A, P; z) \leq \frac{X}{G(t, z)} + \sum_{\substack{d \divides P(t) \\ d < t^2}} 3^{\omega(d)} |R_d|
  \]
  where
  \[
    G(t, z) = \sum_{\substack{d \divides P(z) \\ d < t}} g(d).
  \]
\end{theorem}

Recall that \(W_P = \prod_{\substack{p \in P \\ p < z}} (1 - \frac{f(p)}{p})\) so expected size of \(S(A, P; z)\) is \(XW_P\). Note that as \(t \to \infty\),
\[
  G(t, z)
  \to \sum_{d \divides P(z)} g(d) 
  = \prod_{p < z} (1 + g(p))
  = \prod_{p < z} \left( 1 - \frac{f(p)}{p} \right)^{-1}
  = \frac{1}{W_P}.
\]

Let's apply our new machinery:
\begin{corollary}
  For all \(x, y\),
  \[
    \pi(x + y) - \pi(x) \ll \frac{y}{\log y}.
  \]
\end{corollary}

\begin{proof}
  Let \(A = \{x < n \leq x + y\}, f(p) = 1, R_d = O(1)\) and \(X = y\). As \(g(p) = \frac{1}{p - 1} = \frac{1}{\varphi(p)}\) so \(g(d) = \frac{1}{\varphi(d)}\),
  \begin{align*}
    G(z, z)
    &= \sum_{\substack{d \divides P(z) \\ d < z}} \prod_{p \divides d} (p - 1)^{-1} \\
    &= \sum_{d = p_1 \cdots p_r < z} \prod_{i = 1}^r \sum_{k = 1}^\infty \frac{1}{p_i^k} \\
    &= \sum_{\substack{p_1 \cdots p_r < z \\ 1 \leq i \leq r}} \sum_{k_i = 1}^\infty \frac{1}{p_1^{k_1} \cdots p_r^{k_r}} \\
    &= \sum \frac{1}{n} \quad \text{square-free part of \(n\) \(< z\)} \\
    &\geq \sum_{d < z} \frac{1}{d} \\
    &\gg \log z
  \end{align*}
  so the main term \(\ll \frac{y}{\log z}\). Note that
  \[
    3^{\omega(d)} \leq \tau_3(d) \ll_\varepsilon d^\varepsilon
  \]
  from example sheet 1 so the error term is
  \[
    \sum_{\substack{d \divides P(t) \\ d < t^2}} 3^{\omega(d)} |R_d|
    \ll_\varepsilon t^\varepsilon \sum_{d < t^2} 1
    \ll t^{2 + \varepsilon} = z^{2 + \varepsilon}
  \]
  by setting \(t = z\). Thus
  \[
    S(A, P; z) \ll \frac{y}{\log z} + z^{2 + \varepsilon} \ll \frac{y}{\log y}
  \]
  by taking \(z = y^{1/3}\).
\end{proof}

\begin{proof}[Proof of \nameref{thm:Selberg's sieve}]
  Let \((\lambda_i)\) be a sequence of reals with \(\lambda_1 = 1\), to be chosen later. Then
  \begin{align*}
    S(A, P; z)
    &= \sum_{n \in A} 1_{(n, P(z)) = 1} \\
    &\leq \sum_{n \in A} \left( \sum_{d \divides (n, P(z))} \lambda_d \right)^2 \\
    &= \sum_{d, e \divides P(z)} \lambda_d \lambda_e \sum_{n \in A} 1_{d \divides n, e \divides n} \\
    &= \sum_{d, e \divides P(z)} \lambda_d \lambda_e |A_{[d, e]}| \\
    &= X \sum_{d, e \divides P(z)} \lambda_d \lambda_e \frac{f([d, e])}{[d, e]} + \sum_{d, e \divides P(z)} \lambda_d \lambda_e R_{[d, e]}
  \end{align*}
  We'll choose \(\lambda_d\) such that \(|\lambda_d| \leq 1\) and \(\lambda_d = 0\) if \(d \geq t\). Then
  \[
    \left| \sum_{d, e \divides P(z)} \lambda_d \lambda_e R_{[d, e]} \right|
    \leq \sum_{\substack{d, e < t \\ d, e \divides P(z)}} |R_{[d, e]}|
    \leq \sum_{\substack{n \divides P(z) \\ n < t^2}} |R_n| \sum_{d, e} 1_{[d, e] = n}
  \]
  and since \(n\) is square-free,
  \[
    \sum_{d, e} 1_{[d, e] = n} = 3^{\omega(n)}
  \]
  so the error term is settled.

  Now to the main term. Let
  \[
    V = \sum_{d, e \divides P(z)} \lambda_d \lambda_e \frac{f([d, e])}{[d, e]}.
  \]
  Write \([d, e] = acb\) where \(d = ac, e = bc\) and \((a, b) = (b, c) = (a, c) = 1\). We also require \(\lambda_d = 0\) if \(d\) is not square-free so the last two conditions are automatically satisfied, leaving the notations a bit clearer.
  \begin{align*}
    V
    &= \sum_{c \divides P(z)} \frac{f(c)}{c} \sum_{\substack{ab \divides P(z) \\ (a, b) = 1}} \frac{f(a)f(b)}{ab} \lambda_{ac} \lambda_{bc} \\
    &= \sum_{c \divides P(z)} \frac{f(c)}{c} \sum_{ab \divides P(z)} \frac{f(a)}{a} \frac{f(b)}{b} \sum_{d \divides a, d \divides b} \mu(d) \lambda_{ac} \lambda_{bc} \\
    &= \sum_{c \divides P(z)} \frac{f(c)}{c} \sum_{d \divides P(z)} \mu(d) \left( \sum_{d \divides a \divides P(z)} \frac{f(a)}{a} \lambda_{ac} \right)^2 \\
    &= \sum_{d \divides P(z)} \mu(d) \sum_{c \divides P(z)} \frac{c}{f(c)} \left( \sum_{cd \divides n \divides P(z)} \frac{f(n)}{n} \lambda_n \right)^2 \quad \text{write \(ac = n\)} \\
    &= \sum_{d \divides P(z)} \mu(d) \sum_{c \divides P(z)} \frac{c}{f(c)} y^2_{cd} \\
    &= \sum_{k \divides P(z)} \left( \sum_{cd = k} \mu(d) \frac{c}{f(c)} \right) y_k^2
  \end{align*}
  The term in the brackets is a convolution so we want to simply it. Note that both functions are multiplicative so suffice to work out the primes. For prime \(p\),
  \[
    \sum_{cd = p} \mu(d) \frac{c}{f(c)}
    = -1 + \frac{p}{f(p)} = \frac{1}{g(p)}
  \]
  and thus for all \(k \divides P(z)\),
  \[
    \sum_{cd = k} \mu(d) \frac{c}{f(c)} = \frac{1}{g(k)}.
  \]
  Note that if \(k \geq t\) then \(y_k = 0\). Thus
  \[
    V = \sum_{\substack{k \divides P(z) \\ k < t}} \frac{y_k^2}{g(k)}.
  \]
  We want to choose \(V\) as small as possible. The idea is to find a lower bound for \(V\) and use Cauchy-Schwarz to find the condition on the summands.

  Note that we have now expressed \(V\) in terms of \(y_k\), which is determined by \(\lambda_d\):
  \[
    y_k = \sum_{k \divides n \divides P(z)} \frac{f(n)}{n} \lambda_n.
  \]
  We would like to invert the relation, so that we can directly control \(y_k\). Use a Möbius inversion heuristics, for a fixed \(d\),
  \begin{align*}
    \sum_{d \divides k \divides P(z)} \mu(k) y_k
    &= \sum_{k \divides P(z)} \mu(k) \sum_{n \divides P(z)} \frac{f(n)}{n} \lambda_n 1_{d \divides k} 1_{k \divides n} \\
    &= \sum_{n \divides P(z)} \frac{f(n)}{n} \lambda_n 1_{d \divides n} \sum_{d \divides k \divides n} \mu(k)
  \end{align*}
  For the last summation, note that \(k = de\) is square-free so
  \[
    \sum_{d \divides k \divides n} \mu(k)
    = \mu(d) \sum_{e \divides \frac{n}{d}} \mu(e)
    =
    \begin{cases}
      \mu(d) & n = d \\
      0 & n > d
    \end{cases}
  \]
  by multiplicativity. Thus
  \[
    \sum_{d \divides k \divides P(z)} \mu(k) y_k
    = \mu(d) \frac{f(d)}{d} \lambda_d
  \]
  Thus instead of choosing \(\lambda_d\), we can choose \(y_k\) to make \(V\) small.

  Recall that \(\lambda_1 = 1\) so must have
  \[
    \sum_{k \divides P(z)} \mu(k) y_k = 1.
  \]
  Thus
  \begin{align*}
    1
    &= \left( \sum_{\substack{k \divides P(z) \\ k < t}} \mu(k) y_k g(k)^{1/2} \cdot \frac{1}{g(k)^{1/2}} \right)^2 \\
    &\leq \left( \sum_{\substack{k \divides P(z) \\ k < t}} g(k) \right) \left( \sum_{\substack{k \divides P(z) \\ k < t}} \frac{y_k^2}{g(k)} \right) \\
    &= GV
  \end{align*}
  where \(G = G(t, z)\) by Cauchy-Schwarz, with equaility if and only if there exists \(c\) such that for all \(k\),
  \[
    \frac{\mu(k) y_k}{g(k)^{1/2}} = c g(k)^{1/2}
  \]
  i.e.
  \[
    y_k = c \mu(k) g(k)
  \]
  for \(k < t\). To find \(c\), use the normalisation condition
  \[
    1 = c \sum_{\substack{k \divides P(z) \\ k < t}} \mu(k)^2 g(k) = cG
  \]
  so choose \(c = \frac{1}{G}\). Check that
  \begin{enumerate}
  \item \(\lambda_1 = 1\),
  \item \(\lambda_d = 0\) if \(d \geq t\),
  \item \(\lambda_d = 0\) if \(d\) is square-free (lecturer said this condition is actually not necessary).
  \item \(|\lambda_d| \leq 1\). Can be checked as follow:
    \begin{align*}
      \lambda_d
      &= \mu(d) \frac{d}{f(d)} \sum_{d \divides k \divides P(z)} \mu(k) y_k \\
      &= \frac{d}{f(d)} \frac{1}{G} \sum_{d \divides k \divides P(z)} g(k).
    \end{align*}
    Note that
    \begin{align*}
      G
      &= \sum_{\substack{e \divides P(z) \\ e < t}} g(e) \\
      &= \sum_{k \divides d} \sum_{\substack{e \divides P(z) \\ e < t \\ (d, e) = k}} g(e) \quad \text{for fixed \(d\)} \\
      &= \sum_{k \divides d} g(k) \sum_{\substack{m \divides P(z) \\ (m, d) = 1 \\ m < t/k}} g(m) \\
      &\geq \sum_{k \divides d} g(k) \sum_{\substack{m \divides P(z) \\ (m, d) = 1 \\ m < t/d}} g(m)
    \end{align*}
    Note that for prime \(p\),
    \[
      \sum_{k \divides p} g(k)
      = 1 + \frac{f(p)}{p - f(p)}
      = \frac{p}{p - f(p)}
      = \frac{p}{f(p)} g(p)
    \]
    so
    \[
      G
      \geq \frac{d}{f(d)} g(d) ( \sum_{\substack{m \divides P(z) \\ (m, d) = 1 \\ m < t/d}} g(m))
      = \frac{d}{f(d)} \sum_{d \divides k \divides P(z)} g(k)
      = |\lambda_d| G
    \]
    so \(|\lambda_d| \leq 1\).
  \end{enumerate}
\end{proof}

\begin{theorem}[Brun]
  Let \(\pi_2(x) = \#\{1 \leq n \leq x: n, n + 2 \text{ are prime}\}\). Then
  \[
    \pi_2(x) \ll \frac{x}{(\log x)^2}.
  \]
\end{theorem}

\begin{proof}
  Let \(A = \{n (n + 2): 1 \leq n \leq x\}\), \(P\) be the the set of all primes except \(2\). Have
  \[
    |A_d| = \#\{1 \leq n \leq x: d \divides n (n + 2)\}
  \]
  If \(d = p_1 \cdots p_r\) is odd and square-free then \(d \divides n(n + 2)\) if and only if \(p_i \divides n(n + 1)\) for all \(i\), if and only if \(n = 0 \text{ or } -2 \mod p_i\) for all \(i\), and Chinese remainder theorem, if and only if \(n\) lies in one of \(2^{\omega(d)}\) many residue classes mod \(d\). Thus
  \[
    |A_d| = \frac{2^{\omega(d)}}{d} X + O(2^{\omega(d)})
  \]
  so \(f(d) = 2^{\omega(d)}\) and \(R_d \ll 2^{\omega(d)}\) for \(d\) odd square-free. By \nameref{thm:Selberg's sieve}, with \(t = z = x^{1/4}\),
  \begin{align*}
    \pi_2(x)
    &\leq \#\{1 \leq n \leq x: p \divides n(n + 2) \implies p = 2 \text{ or } p \geq x^{1/4}\} + O(x^{1/4}) \\
    &= S(A, P; x^{1/4}) + O(x^{1/4}) \\
    &\leq \frac{x}{G(z, z)} + O(\sum_{\substack{d \divides P(z) \\ d < z^2}} 6^{\omega(d)})
  \end{align*}
  As before
  \[
    \sum_{d < z^2} 6^{\omega(d)} \leq z^{2 + O(1)} = x^{1/2 + O(1)}.
  \]
  To finish the proof need to show \(G(z, z) \gg (\log z)^2\). Note that \(g(2) = 2\) and 
  \[
    g(p) = \frac{f(p)}{p - f(p)} = \frac{2}{p - 2} \geq \frac{2}{p - 1},
  \]
  so if \(d\) is odd and square-free then
  \[
    g(d) \geq \frac{2^{\omega(d)}}{\varphi(d)}.
  \]
  so
  \begin{align*}
    G(z, z)
    &\geq \sum_{\substack{d < z \\ d \text{ odd, square-free}}} \frac{2^{\omega(d)}}{\varphi(d)} \\
    &= \sum_{d = p_1 \cdots p_r < z} 2^{\omega(d)} \prod_{i = 1}^r \left( \frac{1}{p_i} + \frac{1}{p_i^2} + \cdots \right) \\
    &\geq \sum_{d < z} \frac{2^{\omega(d)}}{d}
  \end{align*}
  By partial summation, it's enough to show \(\sum_{d < z} 2^{\omega(d)} \gg z \log z\). Recall that to show
  \[
    \sum_{d < z} \tau(d) \gg z \log z
  \]
  we used \(\tau = 1 * 1\). So we need to write \(2^{\omega(n)}\) as a convolution of multiplicative functions. Suppose
  \[
    2^{\omega(n)} = \sum_{d \divides n} f(d) g(\frac{n}{d})
  \]
  where \(f, g\) are multiplicative. We can actually write down values of \(f\) at prime powers:
  \begin{enumerate}
  \item[\(1\)]: \(f(1) = g(1) = 1\)
  \item[\(p\)]: \(2 = f(p) + g(p)\)
  \item[\(p^2\)]: \(2 = g(p^2) + f(p^2) + f(p)g(p)\).
  \end{enumerate}
  Let's say try \(f = \tau\), so \(g(p) = 0\), \(g(p^2) = -1\), \(g(p^k) = 0\) for \(k \geq 3\). Therefore
  \[
    g(n) =
    \begin{cases}
      0 & n \text{ not a square} \\
      \mu(d) & n = d^2
    \end{cases}
  \]
  and
  \[
    2^{\omega(n)} = \sum_{d \divides n} \tau(d) g(\frac{n}{d}).
  \]
  Therefore
  \begin{align*}
    \sum_{d < z} 2^{\omega(d)}
    &= \sum_{a < z} g(a) \sum_{b \leq z/a} \tau(b) \\
    &= \sum_{a < z} g(a) \left( \frac{z}{a} \log \frac{z}{a} + (2\gamma - 1) \frac{z}{a} + O(\sqrt{z/a}) \right) \\
    &= \sum_{a < z} g(a) \frac{z}{a} \log \frac{z}{a} + C \sum_{a < z} g(a) \frac{z}{a} + O(\underbrace{z^{1/2} \sum_{a < z} \frac{1}{a^{1/2}}}_{\ll z}) \\
    &= \sum_{d < z^{1/2}} \mu(d) \frac{z}{d^2} \log z - \underbrace{2 \sum_{d < z^{1/2}} \mu(d) \frac{z}{d^2} \log d}_{\ll z \sum_{d < z^{1/2}} \frac{\log d}{d^2} \ll z} + O(z)
  \end{align*}
  Note
  \begin{align*}
    \sum_{d < z^{1/2}} \frac{\mu(d)}{d^2}
    &= \sum_{d = 1}^\infty \frac{\mu(d)}{d^2} - \sum_{d \geq z^{1/2}} \frac{\mu(d)}{d^2} \\
    &\geq c + \sum_{d \geq z^{1/2}} \frac{1}{d^2} \\
    &= c + O(\frac{1}{z^{1/2}})
  \end{align*}
  so
  \[
    \sum_{d < z} 2^{\omega(d)} = c z \log z + O(z) \gg z \log z.
  \]
  Remains to show \(c > 0\). Either note LHS can't be \(O(z)\), or calculate the first couple of terms in the series, or note that \(c = \frac{6}{\pi^2} > 0\).
\end{proof}

\subsection{Combinatorial sieve}

Selberg's sieve is an upper bound sieve, while sieve of Eratosthenes uses the inclusion-exclusion principle
\[
  S(A, P; z) = |A| - \sum_p |A_p| + \sum_{p \neq q} |A_{pq}| - \dots
\]
to get a precise number. However this requires us to keep track of every term, thus resulting in an accummulation of error. The idea of a combinatorial sieve is to ``truncate'' the sieve process.

\begin{lemma}[Buchstab formula]\index{Buchstab formula}
  \[
    S(A, P; z) = |A| - \sum_{p \divides P(z)} S(A_p, P; p).
  \]
\end{lemma}

\begin{proof}
  Rearrange, required to show
  \[
    |A|
    = S(A, P; z) + \sum_{p \divides P(z)} S(A_p, P; p)
    = S_1 + \sum_{p \divides P(z)} S_p
  \]
  where
  \begin{align*}
    S_1 &= \#\{n \in A: p \divides n, p \in P \implies p \geq z\} \\
    S_p &= \#\{n \in A: n = mp, q \divides n, q \in P \implies q \geq p\}
  \end{align*}

  Every \(n \in A\) is either in the set counted by \(S_1\) or has some prime divisors from \(P(z)\). If \(p\) is the least such prime divisor then \(n \in S_p\). The \(S_p\)'s are disjoint.
\end{proof}

Similarly,

\begin{lemma}
 \[
   W(z) = 1 - \sum_{p \divides P(z)} \frac{f(p)}{p} W(p)
 \]
 where recall that
 \[
   W(z) = \prod_{p \divides P(z)} \left( 1 - \frac{f(p)}{p} \right).
 \]
\end{lemma}

\begin{proof}
  Exercise.
\end{proof}

\begin{corollary}
  For any \(r \geq 1\),
  \[
    S(A, P; z) = \sum_{\substack{d \divides P(z) \\ \omega(d) < r}} \mu(d) |A_d| + (-1)^r \sum_{\substack{d \divides P(z) \\ \omega(d) = r}} S(A_d, P; \ell(d))
  \]
  where \(\ell(d)\) is the least prime divisor of \(d\).
\end{corollary}

\begin{proof}
  Induction on \(r = 1\). When \(r = 1\) this is just Buchstab's formula. For inductive step, use
  \[
    S(A_d, P; \ell(d))
    = |A_d| - \sum_{\substack{p \in P \\ p < \ell(d)}} S(A_{dp}, P; p)
  \]
  so
  \begin{align*}
    &\phantom{= } (-1)^r \sum_{\substack{d \divides P(z) \\\omega(d) = r}} \left( |A_d| - \sum_{\substack{p \in P \\ p < \ell(d)}} S(A_{pd}, P; p) \right) \\
    &= \sum_{\substack{d \divides P(z) \\ \omega(d) = r}} \mu(d) |A_d| + (-1)^{r + 1} \sum_{\substack{e \divides P(z) \\ \omega(e) = r + 1}} S(A_e, P; \ell(e))
  \end{align*}
\end{proof}

In particular, if \(r\) is even then
\[
  S(A, P; z) \geq \sum_{\substack{d \divides P(z) \\ \omega(d) < r}} \mu(d) |A_d|
\]
and similarly if \(r\) is odd we get an upper bound.

\begin{theorem}[Brun's pure sieve]\index{Brun's pure sieve}
  If \(r \geq 6 \log \frac{1}{W(z)}\) then
  \[
    S(A, P; z) = XW(z) + O(2^{-r} X + \sum_{\substack{d \divides P(z) \\ d \leq z^r}} |R_d|).
  \]
\end{theorem}

Brun's pure sieve has the same main term as sieve of Erathosthenes, but the error term is split into the fixed bit \(2^{-r}X\) and an accummulation part truncated at \(2^r\).

\begin{proof}
  Recall that from iterating Buchstab's formula
  \begin{align*}
    S(A, P; z)
    &= \sum_{\substack{d \divides P(z) \\ \omega(d) < r}} \mu(d) |A_d| + (-1)^r \sum_{\substack{d \divides P(z) \\ \omega(d) = r}} S(A_d, P; \ell(d)) \\
    &= X \sum_{\substack{d \divides P(z) \\ \omega(d) < r}} \mu(d) \frac{f(d)}{d} + \sum_{\substack{d \divides P(z) \\ \omega(d) < r}} \mu(d) R_d + (-1)^r \sum_{\substack{d \divides P(z) \\ \omega(d) = r}} S(A_d, P; \ell(d))
  \end{align*}
  By the trivial bounds
  \[
    0 \leq S(A_d, P; \ell(d)) \leq |A_d|
  \]
  have
  \begin{align*}
    S(A, P; z)
    &= X \sum_{\substack{d \divides P(z) \\ \omega(d) < r}} \mu(d) \frac{f(d)}{d} + O(\sum_{\substack{d \divides P(z) \\ \omega(d) < r}} |R_d| + \sum_{\substack{d \divides P(z) \\ \omega(d) = r}} |A_d|)
  \end{align*}
  By Buchstab again, applied to \(W(z)\),
  \[
    W(z)
    = \sum_{\substack{d \divides P(z) \\ \omega(d) < r}} \mu(d) \frac{f(d)}{d} + (-1)^r \sum_{\substack{d \divides P(z) \\ \omega(d) = r}} \mu(d) \frac{f(d)}{d} W(\ell(d))
  \]
  so
  \[
    S(A, P; z) = XW(z) + O(\sum_{\substack{d \divides P(z) \\ \omega(d) < r}} |R_d| + \sum_{\substack{d \divides P(z) \\ \omega(d) = r}} |A_d| + X\sum_{\substack{d \divides P(z) \\ \omega(d) = r}} \frac{f(d)}{d})
  \]
  The error term is
  \begin{align*}
    &\phantom{=} \sum_{\substack{d \divides P(z) \\ \omega(d) < r}} |R_d| + \sum_{\substack{d \divides P(z) \\ \omega(d) = r}} |A_d| + X \sum_{\substack{d \divides P(z) \\ \omega(d) = r}} \frac{f(d)}{d} \\
    &\ll X \sum_{\substack{d \divides P(z) \\ \omega(d) = r}} \frac{f(d)}{d} + \sum_{\substack{d \divides P(z) \\ \omega(d) \leq r}} |R_d| \\
    &\leq X \sum_{\substack{d \divides P(z) \\ \omega(d) = r}} \frac{f(d)}{d} +\sum_{\substack{d \divides P(z) \\ d \leq z^r}} |R_d| \quad \text{as }
    d \divides P(z) = \prod_{\substack{p \in P \\ p < z}} P
  \end{align*}
  Remains to show
  \[
    \sum_{\substack{d \divides P(z) \\\omega(d) = r}} \frac{f(d)}{d} \ll 2^{-r}
  \]
  We need the condition on \(r\). Note that
  \begin{align*}
    \sum_{\substack{d \divides P(z) \\\omega(d) = r}} \frac{f(d)}{d}
    &= \sum_{\substack{p_1 \cdots p_r \\ p_i \in P \\ p_i < z}} \frac{f(p_1) \cdots f(p_r)}{p_1 \cdots p_r} \\
    &\leq \frac{1}{r!} \left( \sum_{p \divides P(z)} \frac{f(p)}{p} \right)^r \\
    &\leq \left( \frac{e}{r} \sum_{p \divides P(z)} \frac{f(p)}{p} \right)^r \\
  \end{align*}
  Furthermore
  \[
    \sum_{p \divides P(z)} \frac{f(p)}{p}
    \leq \sum_{p \divides P(z)} - \log(1 - \frac{f(p)}{p})
    = - \log W(z)
  \]
  so if \(r \geq 2 e |\log W(z)|\) then
  \[
    \sum_{\substack{d \divides P(z) \\\omega(d) = r}} \frac{f(d)}{d}
    \leq \left( \frac{e}{r} |\log W(z)| \right)^r
    \leq 2^{-r}
  \]
  Finally note that \(2e < 6\).
\end{proof}

Recall that Selberg's sieve shows that \(\pi_2(x) \ll \frac{x}{(\log x)^2}\). In the twin prime seive setting, \(W(z) \asymp \frac{1}{(\log z)^2}\). So in Brun's sieve, need to take \(r \gg 2 \log \log z\). If \(r = C \log \log z\) for \(C\) large enough then
\[
  \frac{X}{(\log z)^{100}}.
\]
The main term is \(\gg \frac{x}{\log z}^2\). As \(|R_d| \ll 2^{\omega(d)} = d^{o(1)}\). Thus
\[
  \sum_{\substack{d \divides P(z) \\ d \leq z^r}}
  \ll z^{r + o(1)}
  \ll z^{2 \log \log z + o(1)}
\]
so we need to choose a \(z\). For this to be \(o(\frac{x}{(\log z)^2}\), need to choose \(z \approx \exp((\log x)^{1/4})\). So far so good. Now we need to establish the relation between \(\pi_2(x)\) and
\[
  S(A, P; z) = \{1 \leq n \leq x: p \divides n(n + 2) \implies p > z\}
\]
so \(p \gg x^{1/2}\), so this counts only ``large'' primes

\begin{corollary}
  For any \(z \leq \exp (o(\frac{\log x}{\log \log x}))\),
  \[
    \#\{1 \leq n \leq x: p \divides n \implies p \geq z\} \sim e^{-\gamma} \frac{x}{\log z}.
  \]
\end{corollary}

\begin{remark}\leavevmode
  \begin{enumerate}
  \item In particular, \(z = (\log x)^A\) is allowed for any \(A\), but \(z = x^c\) for any \(c > 0\) is not allowed.
  \item In particular, we can't count primes like this as \(z = x^{1/2}\). Recall heuristic from before says if this asymptotic were correct for primes, then
    \[
      \pi(x) \sim 2e^{-\gamma} \frac{x}{\log x}
    \]
    which contradicts prime number theorem.

    This is telling us that for primes, the error term is genuinely large, not because of the estimates. Or in other words, \(W(z)\) is not a very good bound, intrinsic
  \end{enumerate}
\end{remark}

\begin{proof}
  Again use \(A = \{1 \leq n \leq x\}\) so \(f(d) = 1, |R_d| \ll 1\). Then
  \[
    W(z)
    = \prod_{p < z} (1 - \frac{1}{p})
    = \frac{e^{-\gamma}}{\log z} + o(\frac{1}{\log z})
  \]
  so
  \begin{align*}
    S(A, P; z)
    &= \#\{1 \leq n \leq x: p \divides n \implies p \geq z\} \\
    &= e^{-\gamma} \frac{x}{\log z} + o(\frac{x}{\log z} + O(2^{-r} x + \sum_{\substack{d \divides P(z) \\ d < z^r}} |R_d|)
  \end{align*}
  if \(r \geq 6 |\log W(z)|\), so \(r \geq 100 \log \log z\) is fine. Have
  \[
    2^{-r} x \leq (\log z)^{- (\log 2) 100} x = o(\frac{x}{\log z})
  \]
  and, choose \(r = \ceil{100 \log \log z}\),
  \[
    \sum_{\substack{d \divides P(z) \\ d < z^r}} |R_d|
    \ll \sum_{d \leq z^r} 1
    \ll 2^r
    \leq 2^{500 (\log \log z) \log z}
  \]
  Remains to note that if
  \[
    \log z = o(\frac{\log x}{\log \log x}) = \frac{\log x}{\log \log x} F(x)
  \]
  then this is
  \begin{align*}
    \log z \log \log z
    = o( \frac{\log x}{\log \log x} \log \log x)
    = o(\log x)
  \end{align*}
  so
  \[
    2^{500 (\log \log z) \log z} \leq x^{1/10} = o(\frac{x}{\log z})
  \]
  if \(x\) is large enough.
\end{proof}

\section{The Riemann zeta function}

As a tradition, in analytic number theory we write \(s = \sigma + it\) for a complex number \(s\) where \(\sigma\) and \(t\) are the real and imaginary part respectively. First, a trivial remark: if \(n \in \N\) then
\[
  n^s = e^{s \log n} = n^\sigma \cdot e^{it \log n}.
\]

The \emph{Riemann zeta function}\index{Riemann zeta function} is defined for \(\sigma > 1\) by
\[
  \zeta(s) = \sum_{n = 1}^\infty \frac{1}{n^s}.
\]

\subsection{Dirichlet series}

For any arithmetic \(f: \N \to \C\), we have a \emph{Dirichlet series}\index{Dirichlet series}
\[
  L_f(s) = \sum_{i = 1}^\infty \frac{f(n)}{n^s},
\]
at least formally.

\begin{lemma}
  For any \(f\) there is an \emph{abscissa of convergence} \(\sigma_c\) such that
  \begin{enumerate}
  \item if \(\sigma < \sigma_c\) then \(L_f(s)\) diverges,
  \item if \(\sigma > \sigma_c\) then \(L_f(s)\) converges uniformly in some neighbourhood of \(s\). In particular \(L_f(s)\) is holomorphic at \(s\).
  \end{enumerate}
\end{lemma}

\begin{proof}
  It is enough to show that if \(L_f(s)\) converges at \(s_0\) and \(\sigma = \sigma_0\) then there is a neighbourhood of \(s\) on which \(L_f\) converges uniformly, as then we can take
  \[
    \sigma_c = \inf \{\sigma: L_f(s) \text{ converges}\}.
  \]
  Let
  \[
    R(u) = \sum_{n > u} f(n) n^{-s_0}.
  \]
  By partial summation
  \[
    \sum_{M < n \leq N} f(n) n^{-s}
    = R(M) M^{s_0 - s} - R(N) N^{s_0 - s} - (s_0 - s) \int_M^N R(u) u^{s_0 - s - 1} du.
  \]
  If \(|R(u)| \leq \varepsilon\) for all \(u \geq M\) then 
  \[
    \left| \sum_{M < n \leq N} f(n) n^{-s} \right|
    \leq 2\varepsilon + \varepsilon |s_0 - s| \int_M^N u^{\sigma_0 - \sigma - 1} du
    \leq (2 + \frac{|s_0 - s|}{|\sigma_0 - \sigma|}) \varepsilon.
  \]
  Note that there is a neighbourhood of \(s\) in which \(\frac{|s_0 - s|}{|\sigma_0 - \sigma|} \ll_s 1\) so \(\sum \frac{f(n)}{n^s}\) converges uniformly here.
\end{proof}

\begin{lemma}
  If
  \[
    \sum \frac{f(n)}{n^s} = \sum \frac{g(n)}{n^s}
  \]
  for all \(s\) in some half plane \(\sigma > \sigma_0 \in \R\) then \(f(n) = g(n)\) for all \(n\).
\end{lemma}

\begin{proof}
  Enough to consider \(\sum \frac{f(n)}{n^s} = 0\) for all \(\sigma > \sigma_0\). Suppose exists \(n\) such that \(f(n) \neq 0\). Let \(N\) be the least such that \(f(N) \neq 0\). Since \(\sum_{n \geq N} \frac{f(n)}{n^\sigma} = 0\), have
  \[
    f(N) = - N^\sigma \sum_{n > N} \frac{f(n)}{n^\sigma}
  \]
  so \(|f(n)| \ll n^\sigma\) and so the series
  \[
    \sum_{n > N} \frac{f(n)}{n^{\sigma + 1 + \varepsilon}}
    \]
    is absolutely convergent. So since \(\frac{f(n)}{n^\sigma} \to 0\) as \(\sigma \to \infty\), RHS also converges to \(0\) so \(f(N) = 0\).
\end{proof}

\begin{lemma}
  If \(L_f(s)\) and \(L_g(s)\) are both absolutely convergent at \(s\) then
  \[
    L_{f * g} (s) = \sum_{n = 1}^\infty \frac{f * g (n)}{n^s}
    \]
    is also absolutely convergent at \(s\) and equals to \(L_f(s) L_g(s)\).
\end{lemma}

\begin{proof}
  Because of absolute convergence we can simply multiply them term-by-term:
  \[
    \left( \sum_{n = 1}^\infty \frac{f(n)}{n^s} \right) \left( \sum_{n = 1}^\infty \frac{g(n)}{n^s} \right)
    = \sum_{n, m = 1}^\infty \frac{f(n) g(m)}{(nm)^s} = \sum_{k = 1}^\infty \frac{1}{k^s} \left( \sum_{nm = k} f(n) g(m) \right).
  \]
\end{proof}

\begin{lemma}[Euler product]\index{Euler product}
  If \(f\) is multiplicative and \(L_f(s)\) is absolutely convergent at \(s\) then
  \[
    L_f(s) = \prod_p \left( 1 + \frac{f(p)}{p^s} + \frac{f(p^2)}{p^{2s}} + \dots \right).
  \]
\end{lemma}

\begin{proof}
  Informally we just multiply everything and apply fundamental theorem of arithmetics. However, we have to be more careful when dealing with this infinite product. Let \(y\) be arbitrary,
  \[
    \prod_{p < y} \left( 1 + \frac{f(p)}{p^s} + \dots \right)
    = \sum_{\substack{n \\ \forall p \divides n, p < y}} \frac{f(n)}{n^s}.
  \]
  Then
  \[
    \phantom{=} \left| \prod_{p < y} \left( 1 + \frac{f(p)}{p^s} + \dots \right) - \sum_{n = 1}^\infty \frac{f(n)}{n^s} \right|
    \leq \sum_{\substack{n \\ \exists p \divides n, p \geq y}} \frac{|f(n)|}{n^\sigma}
    \leq \sum_{n \geq y} \frac{|f(n)|}{n^\sigma}
    \to 0
  \]
  as \(n \to \infty\).
\end{proof}

For \(\sigma > 1\),
\[
  \zeta(s) = \sum_{n = 1}^\infty \frac{1}{n^s}
\]
defines a holomorphic function and converges \emph{absolutely} for \(\sigma > 1\). A word of caution: this series is only define for \(\sigma > 1\). We'll in later part of the course analytically extend \(\zeta\) beyond the line. Also note that for general Dirichlet series, uniform convergence and absolute convergence near a point do \emph{not} imply each other, although for this particular series they do. Because of uniform convergence this function has derivative
\[
  \zeta'(s) = - \sum \frac{\log n}{n^s}.
\]

Since \(1\) is completely multiplicative, we may apply Euler product
\[
  1 + \frac{1}{p^s} + \frac{1}{p^{2s}} + \dots = \frac{1}{1 - p^{-s}} = \left( 1 - \frac{1}{p^s} \right)^{-1}
\]
so
\[
  \zeta(s) = \prod_p \left( 1 - \frac{1}{p^s} \right)^{-1}.
\]
Thus
\begin{align*}
  \frac{1}{\zeta(s)} &= \prod_p \left( 1 - \frac{1}{p^s} \right) = \sum_n \frac{\mu(n)}{n^s} \\
  \log \zeta(s) &= - \sum_p \log (1 - \frac{1}{p^s}) = \sum_p\sum_k \frac{1}{k p^{ks}} = \sum \frac{\Lambda(n)}{\log n} \frac{1}{n^s} \\
  \frac{\zeta'(s)}{\zeta(s)} &= - \sum \frac{\Lambda(n)}{n^s}
\end{align*}
We can write many functions and identities in terms of \(\zeta(s)\). For example
\[
  \frac{\zeta'(s)}{\zeta(s)} \cdot \zeta(s) = \zeta'(s)
\]
corresponds to
\[
  \Lambda * 1 = \log,
\]
and the equivalence
\[
  L_f \cdot \zeta = L_g \iff L_f = \frac{1}{\zeta} \cdot L_g
\]
corresponds to Möbius inversion.












\printindex
\end{document}

% www.thomasbloom.org/ant.html