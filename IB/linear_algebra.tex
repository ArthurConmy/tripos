\documentclass[a4paper]{article}

\def\npart{IB}

\def\ntitle{Linear Algebra}
\def\nlecturer{A.\ M.\ Keating}

\def\nterm{Michaelmas}
\def\nyear{2017}

\input{header}

\newcommand*{\ann}{\circ}

\newcommand*{\basis}{\mathcal}

\theoremstyle{definition}
\newtheorem*{caution}{Caution}

\begin{document}

\input{titlepage}

\tableofcontents

\section{Vector Spaces}

\begin{convention}
  Throughout this course, $\mathbb{F}$ denotes a general field. If you wish, think of it as $\mathbb{R}$ or $\mathbb{C}$.
\end{convention}

\subsection{Definition}

\begin{definition}[Vector space]
  An $\mathbb{F}$-\emph{vector space} (or a vector space over $\mathbb{F}$) is an abelian group $(V, +)$ equipped with a function, called \emph{scalar multiplication}:
  \begin{align*}
    \mathbb{F}\times V &\to V \\
    (\lambda, v) &\mapsto \lambda\cdot v
  \end{align*}
  satisfying the axioms
  \begin{itemize}
  \item distributive over vectors: $\lambda(v_1+v_2) = \lambda(v_1+v_2)$,
  \item distributive over scalars: $(\lambda_1+\lambda_2)v= \lambda_1 v+\lambda_2 v$,
  \item $\lambda(\mu v) = \lambda \mu v$,
  \item $1\cdot v = v$.
  \end{itemize}
\end{definition}

The additive unit of $V$ is denoted by $\V 0$.

\begin{eg}\leavevmode
  \label{eg:matrix as V}
  \begin{enumerate}
  \item $\forall n \in \mathbb{N}, \mathbb{F}^n$ is the space of column vectors of length $n$ with entries in $\mathbb{F}$. It is an vector space by entry-wise addition and entry-wise scalar multiplication.
  \item $M_{m,n}(\mathbb{F})$, the set of $m\times n$ matrices with entries in $\mathbb{F}$, with the operation defined as entry-wise addition.
    \item For any set $X$, $\mathbb{R}^X = \{f: X \to \mathbb{R}\}$, the set of $\mathbb{R}$-valued functions on $X$, with addition and scalar multiplication defined pointwise. For instance, $(f_1+f_2)(x) = f_1(x)+f_2(x)$.
  \end{enumerate}
\end{eg}

\begin{ex}\leavevmode
  \begin{enumerate}
  \item Check the above examples satisfy the axioms.
    \item $0\cdot v = \V 0$ and $(-1)\cdot v = -v$ for all $v \in V$.
  \end{enumerate}
\end{ex}

\subsection{Vector Subspace}

\begin{definition}[Vector Subspace]
  Let $V$ be an $\mathbb{F}$-vector space. A subset $U \subseteq V$ is a \emph{subspace}, denoted $U \leq V$, if
  \begin{itemize}
  \item $\V 0 \in U$,
  \item $U$ is closed under addition: $\forall u_1, u_2 \in U, u_1+u_2 \in U$,
    \item $U$ is closed under scalar multiplication: $\forall u \in U, \forall \lambda \in \mathbb{F}, \lambda u \in U$.
  \end{itemize}
\end{definition}

\begin{ex}
  If $U$ is a subspace of $V$, then $U$ is also an $\mathbb{F}$-vector space.
\end{ex}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item $V = \mathbb{R}^{\mathbb{R}}$, the set all functions from $\mathbb{R}$ to itself, has a (proper) subspace $C(\mathbb{R})$, the space of continuous functions on $\mathbb{R}$ as continuous functions are closed under addition and scalar multiplication. $C(\mathbb{R})$ in turn has a proper subspace $P(\mathbb{R})$, the set of all polynomials in $\mathbb{R}$.
    \item $\{(x_1,x_2,x_3) \in \mathbb{R}^3: x_1+x_2+x_3 = t\}$ where $t$ is some fixed constant is a subspace of $\mathbb{R}^3$ if and only if $t = 0$.
  \end{enumerate}
\end{eg}

\begin{proposition}
  Let $V$ be an $\mathbb{F}$-vector space, $U, W \leq V$. Then $U \cap W \leq V$.
\end{proposition}

\begin{proof}\leavevmode
  \begin{itemize}
  \item $\V 0 \in U, \V 0 \in V$ so $\V 0 \in U \cap W$.
    \item Suppose $u, w \in U \cap W$. Fix $\lambda, \mu \in \mathbb{F}$. As $U \leq V$, $\lambda u + \mu w \in U$. As $W \leq V$, $\lambda u +\mu w \in W$ so $\lambda u + \mu w \in U \cap W$. Take $\lambda = \mu = 1$ for vector addition and $\mu = 0$ for scalar multiplication.
  \end{itemize}
\end{proof}

\begin{eg}
  $V = \mathbb{R}^3, U = \{(x,y,z): x=0\}, W=\{(x,y,z):y=0\}$, then $U\cap W=\{(x,y,z):x=y=0\}$.
\end{eg}

\begin{note}
The union of a family of subspaces is \emph{almost never} a subspace. For example, $V = \mathbb{R}^2$, $U, V$ be $x$- and $y$-axis.
\end{note}

\begin{definition}[Sum of vector spaces]
  Let $V$ be an $\mathbb{F}$-vector space, $U, W \leq V$, the \emph{sum} of $U$ and $W$ is the set
  \[
    U + W = \{u+w: u\in U, w\in W\}
  \]
\end{definition}

\begin{eg}
  Use the definition from the previous example, $U+W=V$.
\end{eg}

\begin{proposition}
  $U+W \leq V$.
\end{proposition}

\begin{proof}\leavevmode
  \begin{itemize}
  \item $\V 0 = \V 0 + \V 0 \in U+W$,
  \item $u_1,u_1\in U, w_1,w_2\in W$, $(u_1+w_2) + (u_2+w_2) = (u_1+u_2)+(w_1+w_2) \in U+W$,
    \item similar for scalar multiplication. Left as an exercise.
  \end{itemize}
\end{proof}

\begin{note}
  $U+W$ is the smallest subspace containing both $U$ and $W$. This is because all elements of the form $u+w$ are in such a space by closure under addition.
\end{note}

\begin{definition}[Quotient vector space]
  Let $V$ be an $\mathbb{F}$-vector space, $U \leq V$. The \emph{quotient space} $V/U$ is the abelian gropup $V/U$ equipped with scalar multiplication
  \begin{align*}
    \mathbb{F} \times V/U &\to V/U \\
    (\lambda, v+U) &\mapsto \lambda v+U
  \end{align*}
\end{definition}

\begin{proposition}
  This is well-defined and $V/U$ is an $\mathbb{F}$-vector space.
\end{proposition}

\begin{proof}
  First check it is well-defined. Suppose $v_1+U= v_2+U \in V/U$. Then $v_1-v_2\in U$. Now use closure under scalar multiplication and distributivity, $\lambda v_1 - \lambda v_2 = \lambda(v_1-v_2)\in U$ so $\lambda v_1 + U = \lambda v_2 +U\in V/U$.
  Now check vector space axioms of $V/U$, which will follow from the axioms for $V$:
  \begin{itemize}
  \item $\lambda(\mu(v+U)) = \lambda(\mu v+U) = \lambda(\mu v)+U = (\lambda\mu) v+U = \lambda\mu(v+U)$,
  \item other axioms are left as an exercise.
  \end{itemize}
\end{proof}

\subsection{Span, Linear Independence \& Basis}

\begin{definition}[Span]
  Let $V$ be a $\F$-vector space, $S \subseteq V$ be a subset. The \emph{span} of $S$
  \[
    \generation S = \Big\{\sum_{s\in S} \lambda_s s : \lambda_s \in F \Big\}
  \]
  is the set of all the finite linear combinations of elements (i.e.\ all but finitely many of the $\lambda$ are zero) of $S$.
\end{definition}

\begin{remark}
  $\generation S$ is the smallest subspace of $V$ containing all elements of $S$.
\end{remark}

\begin{convention}
  $\generation \emptyset = \{\V 0\}$
\end{convention}

\begin{eg}\leavevmode
  \begin{enumerate}
 \item $V=\R^3$, $S = \{(1,0,0),(0,1,2),(3,-2,-4)\}$, $\generation S = \{(a,b,2b): a,b\in \R \}$
 \item For any set $X$, $\R^X$ is a vector space. For $x \in X$, define $\delta_x: X \to \R, \delta_x(x) = 1, \delta_x(y) = 0 \: \forall y \neq x$, then
   \[
     \generation{\delta_x: x\in X} = \{f\in \R^X: f \text{ has finite support} \}
   \]
  \end{enumerate} 
\end{eg}

\begin{definition}[Span]
  $S$ spans $V$ if $\generation S = V$.
\end{definition}

\begin{definition}[Finite-dimensional]
  $V$ is \emph{finite-dimensional} over $\F$ if it is spanned by a finite set.
\end{definition}

\begin{definition}[Linear independence]
  The vectors $v_1,\ldots, v_n$ are \emph{linearly independent} over $\F$ if
  \[
    \sum_{i=1}^n \lambda_i = 0 \Rightarrow \lambda_i = 0 \: \forall i
  \]
  A subset $S \subset V$ is \emph{linearly independent} if every finite subset of $S$ is linearly independent.

  A subset if \emph{linearly dependent} if it is not linearly independent.
\end{definition}

\begin{eg}
  In the first example above, the three vectors are not linearly independent.
\end{eg}

\begin{ex}
  The set $\{\delta_x: x \in X\}$ is linearly independent.
\end{ex}

\begin{definition}[Basis]
  $S$ is a \emph{basis} of $V$ if it is linearly independent and spans $V$.
\end{definition}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item $\F^n$ has standard basis $\{e_1,e_2,\ldots,e_n\}$ where $e_i$ is the column vector with $1$ in the $i$th entry and $0$ elsewhere.
  \item $V=\C$ over $\C$ has natural basis $\{1\}$, but over $\R$ it has natural basis $\{1, i\}$.
  \item $V=P(\R)$, the space of real polynomials, has natural basis
    \[
      \{1, x, x^2, \dots \}.
    \]
    It is an exercise to check this carefully.
    \end{enumerate}
\end{eg}

\begin{lemma}
  Let $V$ be a $\F$-vector space. The vectors $v_1,\ldots,v_n$ form a basis of $V$ if and only if each vector $v\in V$ has a unique expression
  \[
    v = \sum_{i=1}^n \lambda_i v_i, \lambda_i \in \F.
  \]
  
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item $\Rightarrow:$ Fix $v\in V$. The $v_i$ span $V$, so exists $\lambda_i \in \F$ such that $v = \sum \lambda_i v_i$. Suppose also $v = \sum \mu_i v_i$ for some $\mu_i \in \F$. Then the difference
  \[
    \sum (\mu_i - \lambda_i) v_i = \V 0.
  \]
  Since the $v_i$ are linearly independent, $\mu_i-\lambda_i = 0$ for all $i$.
\item $\Leftarrow:$ The $v_i$ span $V$ by assumption. Suppose $\sum_{i=1}^n \lambda_i v_i = \V 0$. Note that $\V 0 = \sum_{i=0}^n 0 \cdot v_i$. By appying uniqueness to $\V 0$, $\lambda_i = 0$ for all $i$.
  \end{itemize}
\end{proof}

\begin{lemma}
  If $v_1,\ldots, v_n$ spans $V$ over $\F$, then some subset of $v_1,\ldots,v_n$ is a basis of $V$ over $\F$.
\end{lemma}

\begin{proof}
  If $v_1,\ldots, v_n$ is linearly independent then done. Otherwise for some $\ell$, there exist $\alpha_1, \ldots, \alpha_{\ell-1} \in \F$ such that
  \[
    v_\ell = \sum_{i=1}^{\ell-1} \alpha_i v_i.
  \]
  (If $\sum \lambda_i v_i = 0$, not all $\lambda_i$ is zero. Take $\ell$ maximal with $\lambda_\ell \neq 0$, then $\alpha_i = -\frac{\lambda_i}{\lambda_\ell}$.)

  Now $v_1,\ldots,v_{\ell-1},v_{\ell+1},\ldots,v_n$ still span $V$. Continue iteratively until we have linear independence.
\end{proof}

\begin{theorem}[Steinitz Exchange Lemma]
  Let $V$ be a finite-dimensional vector space over $\F$. Take $v_1,\ldots,v_m$ to be linearly independent, $w_1,\ldots,w_n$ to span $V$. Then
  \begin{itemize}
  \item $m \leq n$, and
    \item reordering the $w_i$ if needed, $v_1,\ldots, v_m, w_{m+1},\ldots,w_n$ spans $V$.
  \end{itemize}
\end{theorem}

\begin{proof}
  Proceed by induction. Suppose that we have replaced $\ell \geq 0$ of the $w_i$. Reordering $w_i$ if needed, $v_1,\ldots,v_\ell,w_{\ell+1},\ldots,w_n$ spans $V$.
  \begin{itemize}
  \item If $\ell = m$, done.
  \item If $\ell < m$, then $v_{\ell+1} = \sum_{i=1}^\ell \alpha_i v_i + \sum_{i> \ell} \beta_i w_i$. As the $v_i$ are linearly independent, $\beta_i \neq 0$ for some $i$. After reordering, $\beta_{\ell+1} \neq 0$,
    \[
      w_{\ell+1} = \frac{1}{\beta_{\ell+1}} (v_{\ell+1}-\sum_{i\leq \ell} \alpha_i v_i - \sum_{i>\ell+1} \beta_i w_i).
    \]
    Thus $v_1,\ldots, v_\ell, v_{\ell+1},w_{\ell+2},\ldots, w_n$ also spans $V$. After $m$ steps, we will replace $m$ of the $w_i$ by $v_i$. Thus $m \leq n$.
  \end{itemize}
\end{proof}

\subsection{Dimension}

\begin{theorem}
  If $V$ is a finite-dimensional vector space over $\F$, then any two bases for $V$ have the same cardinality, which is called the \emph{dimension} of $V$, donoted $\dim_\F V$.
\end{theorem}

\begin{proof}
  If $v_1,\ldots, v_n$ and $w_1,\ldots,w_m$ are both bases, then $\{v_i\}$ is linearly independent and $\{w_i\}$ spans $V$ so $n \leq m$. Similarly $m \leq n$.
\end{proof}

\begin{eg}
  $\dim_\C \C = 1$, but $\dim_\R \C = 2$.
\end{eg}

\begin{lemma}
  Let \(V\) be a finite-dimensional \(\F\)-vector space. If \(w_1,\ldots,w_\ell\) is a linearly independent set of vectors, we can extend it to a basis \(w_1,\ldots,w_\ell,w_{\ell+1},\ldots,w_n\).
\end{lemma}

\begin{proof}
  Apply Steinitz exchange lemma to \(w_1,\ldots, w_\ell\) and any basis \(v_1,\ldots, v_n\).

  Or more direcly, if \(V=\langle w_1,\ldots, w_\ell \rangle\), done. Otherwise take \(v_{\ell+1} \in V\setminus\langle w_1,\ldots, w_\ell\rangle\). Now \(w_1,\ldots, w_\ell,w_{\ell+1}\) is linearly independent. Iterate.
\end{proof}

\begin{corollary}
  Let \(V\) be a finite-dimensional vector space of dimension \(n\). Then
  \begin{enumerate}
  \item Any linearly independent set of vectors has at most \(n\) elements, with equality if and only if the set is a basis.
  \item Any spanning set of vectors has at least \(n\) elements, with equaility if and only if the set is a basis.
  \end{enumerate}
\end{corollary}

\begin{slogan}
  Choose the best basis for the job.
\end{slogan}

\begin{theorem}
  Let \(U, W\) be subspaces of \(V\). If \(V\) and \(W\) are finite-dimensional, so is \(U+W\) and
  \[
\dim(U+W) = \dim U + \dim W - \dim(U\cap W).
  \]
\end{theorem}

\begin{proof}
  Pick basis \(v_1,\ldots, v_\ell\) of \(U\cap W\). Extend it to basis \(v_1,\ldots,v_\ell,u_1,\ldots,u_m\) of \(U\) and \(v_1,\ldots,v_\ell,w_1,\ldots,w_n\) of \(W\). Claim \(v_1,\ldots, v_\ell,u_1,\ldots,u_m,w_1,\ldots,w_n\) is a basis for \(U+W\):
  \begin{itemize}
  \item spanning: if \(u\in U\), then \(u= \sum \alpha_iv_i + \sum \beta_iu_i\) and if \(w\in W\), \(w = \sum_{}^{}\gamma_iv_i + \sum_{}^{}\delta_iw_i\), so \(u+w = \sum_{}^{}(\alpha_i + \gamma_i)v_i + \sum_{}^{}\beta_iu_i + \sum_{}^{}\delta_iu_i\).
  \item linear independence: assume \(\sum \alpha_iv_i + \sum \beta_iu_i+ \sum \gamma_iw_i=0\). Rearrange, \(\sum\alpha_iv_i + \sum\beta_iu_i = -\sum\gamma_iw_i \in U\cap W\) so it equals to \(\sum\delta_iv_i\) for some \(\delta_i\in \F\) because \(v_i\) is a basis for \(U\cap W\). As \(v_i\) and \(w_i\) are linearly independent, \(\gamma_i=\delta_i=0\) for all \(i\). Thus \(\sum\alpha_iv_i + \sum\beta_iv_i=0\), so \(\alpha_i=\beta_i=0\) since \(v_i\) and \(u_i\) form a basis for \(U\).
  \end{itemize}
\end{proof}

\begin{theorem}
  Let \(V\) be a finite-dimensional vector space over \(\F\) and \(U \leq V\), then \(U\) and \(V/U\) are also finite dimensional and
  \[
\dim V = \dim U + \dim V/U.
  \]
\end{theorem}

\begin{proof}
  Left as an exercise. Outline: first show \(U\) is finite-dimensional, then let \(u_1,\ldots,u_\ell\) be a basis for \(U\). Extend it to a basis for \(V\), say \(u_1,\ldots,u_\ell,w_{\ell+1},\ldots,w_n\) of \(V\). Check \(w_{\ell+1}+U,\ldots,w_n+U\) form a basis for \(V/U\).
\end{proof}

\begin{corollary}
  If \(U\) is a proper subspace of \(V\), which is finite-dimensional, then \(\dim U < \dim V\).
\end{corollary}

\begin{proof}
  \(V/U \neq 0\) so \(\dim V/U > 0\).
\end{proof}

\subsection{Direct Sum}

\begin{definition}[Direct sum]
  Let \(V\) be a vector space over \(\F\), \(U, W\leq V\). Then
  \[
V = U \oplus W
  \]
  if every element of \(V\) can be written as \(v=u+w\) for some unique \(u\in U, w\in W\). This is called the \emph{internal direct sum}. \(W\) is a \emph{direct complement} of \(U\) in \(V\).
\end{definition}

\begin{lemma}
  Suppose \(U,W\leq V\), TFAE:
  \begin{enumerate}
  \item \(V = U \oplus W\),
  \item \(V=U+W\) and \(U\cap W = 0\),
  \item Given \(\basis B_1\) any basis of \(U\), \(\basis B_2\) any basis of \(V\), \(\basis B = \basis B_1\cup \basis B_2\) is a basis of \(V\).
  \end{enumerate}
\end{lemma}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item \(2 \Rightarrow 1\): any \(v\in V\) is \(u+w\) for some \(u\in U, w\in W\). Suppose \(u_1+w_1=u_2+w_2\), then \(u_1-u_2 = w_2-w_1 \in U\cap W = 0\). Thus \(u_1=u_2,w_1=w_2\).
  \item \(2 \Rightarrow 1\): \(\basis B\) spans as any \(v\in V\) is \(u+w\). Write \(u\) in terms of \(\basis B_1\) and \(w\) in terms of \(\basis B_2\). Then \(u+w\) is a linear combination of elements of \(\basis B\). To show \(\basis B\) is linearly independent, suppose \(\sum_{v\in \basis B} \lambda_v v = \V 0 = \V 0_V + \V 0_W\). Write LHS as \(\sum_{v\in \basis B_1} \lambda_vv + \sum_{v\in \basis B_2}\lambda_vv\). By uniqueness of expression, \(\sum_{v\in \basis B_1}\lambda_vv=\V 0_V\) and \(\sum_{w\in \basis B_2}\lambda_ww=\V 0_w\). As \(\basis B_1, \basis B_2\) are bases, all of the \(\lambda_v, \lambda_w\) are zero.
  \item \(2 \Rightarrow 1\): if \(v\in V, v=\sum_{x\in V}\lambda_xx = \sum_{u\in B_1}\lambda_uu + \sum_{w\in \basis B_1}\lambda_ww\) so \(v\in U+W\). Conversely, if \(v\in U\cap W, v = \sum_{u\in \basis B_1}\lambda_uu=\sum_{w\in \basis B_2}\lambda_ww\) so all \(\lambda_u, \lambda_v\) are zero since \(\basis B_1\cup \basis B_2\) is linearly independent.
  \end{enumerate}
\end{proof}

\begin{lemma}
  Let \(V\) be a finite-dimensional vector space over \(\F\) and \(U\leq V\). Then there exists a direct complement to \(U\) in \(V\).
\end{lemma}

\begin{proof}
  Let \(u_1,\ldots, u_\ell\) be a basis for \(U\). Extend this to a basis \(u_1,\ldots, u_\ell,w_{\ell+1},\ldots,w_n\) for \(V\). Then \(\generation{w_{\ell+1},\ldots,w_n}\) is a direct complement of \(U\).
\end{proof}

\begin{caution}
  Direct complements are \emph{not} unique.
\end{caution}

\begin{definition}
  Suppose \(V_1,\ldots, V_\ell \leq V\), then
  \[
\sum_i V_i = V_1+\cdots+V_\ell = \{v_1+\cdots+v_\ell: v_i\in V_i\}.
  \]
  The direct sum is direct if
  \[
    v_1+\cdots+v_\ell = v_1'+\cdots+ v_\ell' \Rightarrow v_i = v_i' \text{ for all } i.
  \]
  It is denoted
  \[
V = \bigoplus_{i=1}^\ell V_i.
  \]
\end{definition}

\begin{ex}
  \(V_1,\ldots, V_\ell \leq V\), TFAE:
  \begin{enumerate}
  \item The sum \(\sum_i V_i\) is direct,
  \item \(V_i \cap \sum_{j\neq i}V_j = 0\) for all \(i\),
  \item For any basis \(B_i\) of \(V_i\), the union \(B=\bigcup_{i=1}^\ell B_i\) is a basis for \(\sum_i V_i\).
  \end{enumerate}
\end{ex}

\begin{definition}[Direct Sum]
  Let \(U, W\) be vector spaces over \(\F\). The \emph{external direct sum} is
  \[
U\oplus W = \{(u,w): u\in U, w\in W\}
  \]
  with pointwise addition and scalar multiplication.
\end{definition}

\section{Linear Maps}

\subsection{Definition}

\begin{definition}[Linear map]
  \(V, W\) two \(\F\)-vector space, a map \(\alpha: V\to W\) is \emph{linear} if
  \begin{itemize}
  \item \(\alpha(v_1 + v_2) = \alpha(v_1) + \alpha(v_2)\),
  \item \(\alpha(\lambda v) = \lambda \alpha(v)\).
  \end{itemize}
  This is equivalent to
  \[
\alpha(\lambda_1v_1+ \lambda_2v_2) = \lambda_1\alpha(v_1) + \lambda_2\alpha(v_2).
  \]
\end{definition}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Given an \(n\times m\) matrix \(A\) with coefficients in \(\F\), the map \(\alpha: \F^m\to \F^n, v\to Av\).
  \item Differentiation \(D: P(\R) \to P(\R), f\mapsto \frac{df}{dx}\).
  \item Integration \(I: C[0,1] \to C[0,1], f\to I(f)\) where \(I(f)(x) = \int_0^x f(t)dt\).
  \item Fix \(x\in [0,1]\), the map \(C[0,1]\to \R, f\mapsto f(x)\).
  \end{enumerate}
\end{eg}

\begin{note}[Category of \(\mathbf{Vect}_\F\)]
  Suppose \(U, V, W\) are \(\F\)-vector spaces, then
  \begin{enumerate}
  \item \(\id: V\to V\) is linear.
  \item \(U \stackrel{\alpha}{\to} V \stackrel{\beta}{\to} W\), if \(\alpha, \beta\) are linear then so is \(\beta \compose \alpha\).
  \end{enumerate}
\end{note}

\begin{lemma}[Free functor \(\mathbf{Set} \to \mathbf{Vect}_\F\)]
  Suppose \(V, W\) are \(\F\)-vector spaces and \(\basis B\) is a basis for \(V\). If \(\alpha_0: \basis B\to W\) is \emph{any} map, then there is a \emph{unique} linear map \(\alpha: V\to W\) extending \(\alpha_o\).
\end{lemma}

\begin{proof}
  Let \(v\in V\). Write \(v = \sum \lambda_iv_i\) in a unique way. By linearity \(\alpha(v) = \alpha(\sum \lambda_iv_i) = \sum \lambda_i \alpha(v_i) = \sum \lambda_i \alpha_0(v_i)\). Uniqueness follows.
\end{proof}

\begin{note}\leavevmode
  \begin{itemize}
  \item This is true for infinite-dimensional vector spaces as well.
  \item Very often, to define a linear map, define it on a basis and extend it linearly to the vector space.
  \item Two linear maps \(\alpha_1,\alpha_2: V\to W\) are equal if and only if they agree on a basis.
  \end{itemize}
\end{note}

\subsection{Isomorphism of Vector Spaces}

\begin{definition}[Isomorphism]
  Given \(V, W\) two \(\F\)-vector spaces, the map \(\alpha:V\to W\) is an \emph{isomorphism} if it is linear and bijective, denoted \(V \cong W\).
\end{definition}

\begin{lemma}
  \(\cong\) is an equivalence relation on the class of all \(\F\)-vector spaces.
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item symmetric: obvious.
  \item reflexive: blah blah in lecture. Left as an exercise to reader.
  \item transitive: obvious.
  \end{itemize}
\end{proof}

\begin{theorem}
  If \(V\) is an \(\F\)-vector space, then \(V \cong \F^n\) for some \(n\).
\end{theorem}

\begin{proof}
  Choose a basis for \(V\), say \(v_1,\ldots, v_n\). Define a map
  \begin{align*}
    V &\to \F^n \\
    \sum_{i}^{ }\lambda_iv_i &\mapsto (\lambda_1,\ldots,\lambda_n)
  \end{align*}
which is an isomorphism.
\end{proof}

\begin{remark}
  Choosing an isomorphism \(V \cong \F^n\) is equivalent to choosing a basis for \(V\). i.e.\ there is a bijection \(\{\alpha\in\Hom(V,\F^n), \alpha\text{ bijective}\} \leftrightarrow \{\text{bases of } V\} \).
\end{remark}

\begin{theorem}
  Given two finite-dimensional \(\F\)-vector spaces \(V, W\), they are isomorphic if and only if they have the same dimension.
\end{theorem}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(\Leftarrow\): \(V \cong \F^{\dim V} = \F^{\dim W} \cong W\).
  \item \(\Rightarrow\): let \(a:V\to W\) be an isomorphism and \(\basis B\) be a basis for \(V\). Claim \(\alpha(\basis B)\) is a basis for \(W\): \(\alpha(\basis B)\) spans \(W\) due to surjectivity and \(\alpha(\basis B)\) is linearly independent due to injectivity.
  \end{itemize}
\end{proof}

\begin{definition}[Kernel \& Image]
  Given \(\alpha: V\to W\),
  \begin{itemize}
  \item \(N(\alpha) = \ker \alpha = \{v\in V: \alpha(v) = 0\} \leq V\),
  \item \(\im \alpha = \{w\in W: \exists v\in V, \alpha(v) = w \} \leq W\).
  \end{itemize}
\end{definition}

\begin{proposition}\leavevmode
  \begin{itemize}
  \item \(\alpha\) is injective if and only if \(N(\alpha) = 0\),
  \item \(\alpha\) is surjective if and only if \(\im \alpha = W\).
  \end{itemize}
\end{proposition}

\begin{proof}
  Easy.
\end{proof}

\begin{eg}
  Let \(\alpha: C^\infty(\R) \to C^\infty(\R), \alpha(f)(t) = f''(t)+2f'(t)+5f(t)\). \(\ker \alpha = \{f:f''+2f'+5f=0\}\) and \(g\in \im \alpha\) if and only if there exists an \(f\) such that \(f''+2f'+5f=g\).
\end{eg}

\begin{theorem}[First Isomorphism Theorem]
  Let \(\alpha: V\to W\) be a linear map. It induces an isomprhism
  \begin{align*}
    \bar \alpha: V/\ker \alpha &\to \im \alpha \\
    v + \ker \alpha &\mapsto \alpha(v)
  \end{align*}
\end{theorem}

\begin{proof}
  Check the following:
  \begin{itemize}
  \item \(\bar \alpha\) is well-defined,
  \item \(\bar \alpha\) is linear: immediate from linearity of \(\alpha\),
  \item \(\bar \alpha\) is surjective.
  \end{itemize}
\end{proof}

\begin{definition}\leavevmode
  \begin{itemize}
  \item \(r(\alpha) = rk(\alpha) = \dim( \im \alpha)\) is the \emph{rank} of \(\alpha\),
  \item \(n(\alpha) = \dim N(\alpha)\) is the \emph{nullity} of \(\alpha\).
  \end{itemize}
\end{definition}

\begin{theorem}[Rank-nullity]
  Let \(U, V\) be \(\F\)-vector spaces, \(\dim U < \infty\). Let \(\alpha:U\to V\) be a linear map. Then
  \[
\dim U = r(\alpha) + n(\alpha).
  \]
\end{theorem}

\begin{proof}
  \(U/\ker \alpha \cong \im \alpha\) so \(\dim U - \dim (\ker \alpha) = \dim (\im \alpha)\). Rearrange.
\end{proof}

\begin{lemma}
  Let \(V, W\) be \(\F\)-vector spaces with equal, finite dimension. Let \(\alpha:V\to W\) be linear, then TFAE:
  \begin{enumerate}
  \item \(\alpha\) is injective,
  \item \(\alpha\) is surjective,
  \item \(\alpha\) is an isomorphism.
  \end{enumerate}
\end{lemma}

\begin{proof}
  Rank-nullity theorem.
\end{proof}

%\subsection{\texorpdfstring{\(\Hom_\F(V,W)\)}{Hom\textsubscript{F}(V,W)} as \texorpdfstring{\(\F\)}{F}-vector space}
\subsection{Linear Maps as Vector Space}

Suppose \(V\) and \(W\) are \(\F\)-vector spaces. Let \(L(V,W) = \{\alpha:V\to W, \alpha \text{ linear}\}\).

\begin{proposition}
  \(L(V,W)\) is an \(\F\)-vector space, under operations
  \begin{align*}
    (\alpha_1+\alpha_2)(v) &= \alpha_1(v) + \alpha_2(v) \\
    (\lambda\alpha)(v) &= \lambda(\alpha(v))
  \end{align*}
\end{proposition}

\begin{proof}
  \(\alpha_1+\alpha_2, \lambda\alpha\) as above are well-defined linear maps. The vector space axioms can be easily checked.
\end{proof}

\begin{proposition}
  \label{prop:dimension of linear map space}
  If both \(V\) and \(W\) are finite-dimensional over \(\F\) then so is \(L(V,W)\) and \(L(V,W) = \dim V \cdot \dim W\).
\end{proposition}

\begin{proof}
  See Lemma~\ref{cor:dim of hom}.
\end{proof}

\subsubsection{Matrices, an Interlude}

\begin{definition}[Matrix]
  An \emph{\(m\times n\) matrix} over \(\F\) is an array with \(m\) rows and \(n\) columns with entries in \(\F\). We write
  \[
A = (a_{ij}), a_{ij}\in\F, 1\leq i \leq m, 1\leq j \leq n.
  \]
\end{definition}

\begin{definition}
  \(M_{m,n}(\F)\) is the set of all such \(m\times n\) matrices.
\end{definition}

\begin{proposition}
  \(M_{m,n}(\F)\) is an \(\F\)-vector space and \(\dim M_{m,n}(\F) = m\cdot n\).
\end{proposition}

\begin{proof}
  See Example~\ref{eg:matrix as V} for the proof of vector space axioms. For the dimension claim, a standard basis for \(M_{m,n}(F)\) is
  \[
    E_{ij}=
    \begin{pmatrix}
      0 & \dots & 0 \\
      \vdots & \ddots & \vdots \\
      0 & 1 & 0 \\
      \vdots & \ddots & \vdots \\
      0 & \dots & 0 
    \end{pmatrix}
  \]
  with \(1\) in the \((i,j)\)th entry so \(a_{ij} = \sum_{i,j}^{} a_{ij}E_{ij}\), from which span and linear independence follow. The basis has cardinality \(m\cdot n\).
\end{proof}

\subsubsection{Representation of Linear Maps by Matrices}

Let \(V\) and \(W\) be finite-dimensional \(\F\)-vector space, \(\alpha: V\to W\) linear. Let \(\basis B = \{v_1,\ldots,v_n\}\) be a basis for \(V\), \(\basis C = \{w_1,\ldots,w_m\}\) be a basis for \(W\). If \(v=\sum_{i}\lambda_iv_i \in V\), write
\[
[v]_{\basis B} =
\begin{pmatrix}
  \lambda_1 \\
  \vdots \\
  \lambda_n
\end{pmatrix}
\in \F^n
\]
which is called the \emph{coordinate vector of \(v\) with respect to \(\basis B\)}. Similarly \([w]_{\basis C}\in \F^m\).

\begin{definition}[Matrix representation]
  \([\alpha]_{\basis B, \basis C}\) is the matrix representation of \(\alpha\) with respect to \(\basis B\) and \(\basis C\) with
  \begin{align*}
    [\alpha]_{\basis B, \basis C} &= \Big( [\alpha(v_1)]_{\basis C} \: \Big| \: [\alpha(v_2)]_{\basis C} \: \Big | \: \cdots \: \Big| \: [\alpha(v_n)]_{\basis C} \Big) \\
    &= (a_{ij})
  \end{align*}
  The matrix says
  \[
\alpha(v_j) = \sum_{i}^{ }a_{ij}w_i.
  \]
\end{definition}

\begin{lemma}
  For any \(v\in V\),
  \[
[\alpha(v)]_{\basis C} = [\alpha]_{\basis B, \basis C}\cdot [v]_{\basis B}
  \]
  where \(\cdot\) is matrix multiplication applied to vectors.
\end{lemma}

\begin{proof}
  Fix \(v =\sum_{j=1}^{n}\lambda_jv_j \in V\), so
  \[
[v]_{\basis B} =
\begin{pmatrix}
  \lambda_1 \\
  \vdots \\
  \lambda_n
\end{pmatrix}
\]
\begin{align*}
  \alpha(v) &= \alpha\Big( \sum_{j}^{ }\lambda_jv_j \Big) \\
            &= \sum_{j}^{ }\lambda_j\alpha(v_j) \\
            &= \sum_{j}^{ }\lambda_j\Big( \sum_{i}^{ }\alpha_{ij}w_i \Big) \\
            &= \sum_{i}^{ }\Big( \sum_{j}^{} \alpha_{i,j}\lambda_j \Big) w_i
\end{align*}
so the \(i\)th entry of \([\alpha]_{\basis B, \basis C}\) is \([v]_{\basis B}\).
\end{proof}

\begin{lemma}
  Suppose \(U \stackrel{\beta}{\to} V \stackrel{\alpha}{\to} W\) with \(\alpha, \beta\) linear, with \(\alpha\compose\beta: U\to W\). Let \(\basis A, \basis B, \basis C\) be bases for \(U,V,W\) respectively. Then
  \[
[\alpha\compose\beta]_{\basis A, \basis C} = [\alpha]_{\basis B,\basis C}\cdot[\beta]_{\basis A, \basis B}.
  \]
\end{lemma}

\begin{proof}
  \begin{align*}
    (\alpha\compose\beta)(u_\ell) &= \alpha(\beta(u_\ell)), \: u_\ell\in A\\
                                  &= \alpha\Big( \sum_{j}^{ }b_{jl}v_j \Big), \: v_j\in B \\
                                  &= \sum_{j}^{ }b_{jl}\alpha(v_j) \\
                                  &= \sum_{j}^{ }b_{jl}\sum_{i}^{ }a_{ij}w_i, \: w_i\in W \\
                                  &= \sum_{i}^{ }\Big( \sum_{j}^{ }a_{ij}b_{jl})w_i
  \end{align*}
\end{proof}

\begin{proposition}
  Let \(V\) and \(W\) be \(\F\)-vector spaces with \(\dim V = n, \dim W = m\), then
  \[
L(V,W) \cong M_{m,n}(\F).
  \]
\end{proposition}

\begin{proof}
  Fix bases \(B=\{v_1\ldots,v_n\}, C=\{w_1,\ldots,w_m\}\) for \(V\) and \(W\) respectively. Claim
  \begin{align*}
    \theta: L(V,W) &\to M_{m,n}(\F) \\
    \alpha &\mapsto [\alpha]_{\basis B, \basis C}
  \end{align*}
  is an isomorphism:
  \begin{itemize}
  \item linearity: \([\lambda_1\alpha_1+\alpha_2\alpha_2]_{\basis B, \basis C} = \lambda_1[\alpha_1]_{\basis B, \basis C} + \lambda_2[\alpha_2]_{\basis B, \basis C}\).
  \item surjectivity: given \(A = (a_{ij})\), let \(\alpha:v_j\mapsto \sum_{i=1}^{m}a_{ij}w_i \) and extend linearly. It follows that \(\alpha\in L(V,W)\) and \(\theta(\alpha) = A\).
  \item injectivity: \([\alpha]_{\basis B, \basis C} = \V 0\) implies that \(\alpha\) is the zero map.
  \end{itemize}
\end{proof}

\begin{corollary}
  \label{cor:dim of hom}
  \[
\dim L(V,W) = \dim V \cdot \dim W.
  \]
\end{corollary}

\begin{eg}
  Suppose \(\alpha:V\to W\), \(Y\leq V, Z\leq W\) with \(\alpha(Y)\leq Z\). Let \(\basis B'=\{v_1,\ldots,v_k\}\) be a basis of \(Y\) and extend to \(\basis B=\{v_1,\ldots,v_k,v_{k+1},v_n\}\) a basis for \(V\). Similarly \(\basis C' = \{w_1,\dots,w_l\}\) and \(\basis C\) for \(Z\) and \(W\).
  \begin{itemize}
  \item \([\alpha]_{\basis B, \basis C} =
\begin{pmatrix}
  A & B \\
  0 & C
\end{pmatrix}
\) for some \(A, B, C\) because for \(1\leq j \leq k\), \(\alpha(v_j)\) is a linear combination of \(w_i\) where \(1\leq i \leq l\).
  \item \([\alpha|_Y]_{B',C'} = A. \)
  \item \(\alpha\) induces a map
    \begin{align*}
      \bar\alpha: V/Y &\to W/Z \\
      v+ Y &\mapsto \alpha(v) + Z
    \end{align*}
    This is well-defined. Linearity follows from that of \(\alpha\). A basis for \(V/Y\) is \(\basis B''=\{v_{k+1}+Y,\ldots,v_n+Y\}\) and similarly for \(W/Z\). It is an exercise to show \([\bar\alpha]_{\basis B'', \basis C''} = C \).
  \end{itemize}
\end{eg}

\subsubsection{Change of Bases}

Throughout this section, let \(V\) and \(W\) be \(\F\)-vector spaces and suppose they have the following bases:
\begin{table}[htbp]
  \centering
  \begin{tabular}{|c||c|c|}
    \hline
    Vector space & \(V\) & \(W\) \\ \hline
    Basis 1 & \(\basis B = \{v_1,\dots,v_n\}\) & \(\basis C = \{w_1,\dots,w_m\}\) \\ \hline
    Basis 2 & \(\basis B' = \{v_1',\dots,v_n'\}\) & \(\basis C' = \{w_1',\dots,w_m'\}\) \\ \hline
  \end{tabular}
\end{table}

\begin{definition}[Change-of-basis matrix]
  The \emph{change-of-basis matrix} from \(\basis B'\) to \(\basis B\) is \(P = (p_{ij})\) given by
  \begin{align*}
    v_j' &= \sum_{i}^{ }p_{ij}v_i \\
    P &= \Big( [v_1']_{\basis B} \: \Big| \: [v_2']_{\basis B} \: \Big| \: \dots \Big| \: [v_n']_{\basis B} \Big) = [\id]_{\basis B', \basis B}
  \end{align*}
\end{definition}

\begin{lemma}
  \[
    [v]_{\basis B} = P[v]_{\basis B'}.
  \]
\end{lemma}

\begin{proof}
  \[
    P[v]_{\basis B'} = [\id]_{\basis B', \basis B}[v]_{\basis B'} = [v]_{\basis B}.
  \]
\end{proof}

\begin{lemma}
  \(P\) is an invertible \(n\times n\) matrix and \(P^{-1}\) is the change-of-basis matrix from \(\basis B\) to \(\basis B'\).
\end{lemma}

\begin{proof}
  \begin{align*}
    [\id]_{\basis B, \basis B'}[\id]_{\basis B', \basis B} &= [\id]_{\basis B', \basis B'} = I_n \\
    [\id]_{\basis B', \basis B}[\id]_{\basis B, \basis B'} &= [\id]_{\basis B, \basis B} = I_n
  \end{align*}
\end{proof}

Let \(Q\) be the change-of-basis matrix from \(\basis C'\) to \(\basis C\). Then \(Q\) is an invertible \(m\times m\) matrix.

\begin{proposition}
  Let \(\alpha: V\to W\) be a linear map, \(A = [\alpha]_{\basis B,\basis C}\), \(A' = [\alpha]_{\basis B',\basis C'}\), then
  \[
    A' = Q^{-1}AP.
  \]
\end{proposition}

\begin{proof}
  \[
    \underbrace{[\id]_{\basis C,\basis C'}}_{Q^{-1}} [\alpha]_{\basis B,\basis C} \underbrace{[\id]_{\basis B',\basis B}}_P = \underbrace{[\id\compose\alpha\compose\id]_{\basis B',\basis C'}}_{A'}
  \]
\end{proof}

\begin{definition}[Equivalence of matrices]
  \(A, A' \in M_{m,n}(\F)\) are \emph{equivalent} if
  \[
    A' = Q^{-1}AP
  \]
  for some invertible \(P\in M_{n,n}(\F)\) and \(Q\in M_{m,m}(\F)\).
\end{definition}

\begin{note}
  This defines an equivalence relation on \(M_{m,n}(\F)\).
\end{note}

\begin{proposition}
  Let \(V, W\) be \(\F\)-vector spaces of dimension \(n\) and \(m\) respectively. Let \(\alpha:V\to W\) be a linear map. Then there exist bases \(\basis B\) of \(V\), \(\basis C\) of \(W\), and some \(r\leq m,n\) such that
  \[
    [\alpha]_{\basis B,\basis C} =
    \begin{pmatrix}
      I_r & 0 \\
      0 & 0 
    \end{pmatrix}
  \]
  where \(I_r\) the is \(r\times r\) the identity matrix.
\end{proposition}

\begin{note}
  \(r = rk(\alpha) = r(\alpha)\).
\end{note}

\begin{proof}
  Fix \(r\) such that \(\dim N(\alpha) = n-r\). Fix a basis for \(N(\alpha)\), say \(v_{r+1},\dots,v_n\). Extend this to a basis \(\basis B\) for \(V\), say \(v_1,\dots,v_r,v_{r+1},\dots,v_n\). Now \(\alpha(v_1),\dots,\alpha(v_r)\) is a basis for \(\im(\alpha)\):
  \begin{itemize}
  \item span: \(\alpha(v_1),\dots, \alpha(v_n)\) certainly span \(\im(\alpha)\). Since \(v_{r+1},\dot,v_n \in \ker \alpha\), \(\alpha(v_{r+1}),\dots,\alpha(v_n) = 0\) so we can remove them from the spanning set.
  \item linear independence: assume \(\sum_{i=1}^{n}\lambda_i \alpha(v_i) =\V 0 \). Then \(\alpha \big(\sum_{i=1}^n\lambda_iv_i\big) =\V0\). This implies that
    \[
      \sum_{i=1}^{n}\lambda_iv_i = \sum_{j=r+1}^{n}\mu_jv_j.
    \]
    As \(v_1,\dots v_n\) are linearly independent, \(\lambda_i=\mu_j=0\) for all \(i,j\).
  \end{itemize}
  Extend \(\alpha(v_1),\dots,\alpha(v_r)\) to a basis for \(W\), say \(\basis C\). By construction,
  \[
    [\alpha]_{\basis B,\basis C} =
    \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}
  \]
\end{proof}

\begin{remark}
  In the proof above we didn't need to assume that \(r = r(\alpha)\). This gives us another way prove Rank-nullity Theorem.
\end{remark}

\begin{corollary}
  Any \(m\times n\) matrix is equivalent to
  \[
  \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}
  \]
  for some \(r\).
\end{corollary}

\begin{definition}[Row and column rank]
  Let \(A\in M_{m,n}(\F)\).
  \begin{itemize}
  \item The \emph{column rank} of \(A\), \(r(A)\) is the dimension of the subspace of \(\F^m\) spanned by the columns of \(A\).
  \item The \emph{row rank} of \(A\) is the column rank of \(A^T\).
  \end{itemize}
\end{definition}

\begin{note}
  If \(\alpha\) is a linear map represented by \(A\) with respect to any choice of bases, then \(r(\alpha) = r(A)\).
\end{note}

\begin{proposition}
  Two \(m\times n\) matrices \(A, A'\) are equivalent if and only if
  \[
    r(A) = r(A').
  \]
\end{proposition}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(\Leftarrow\): Both \(A\) and \(A'\) are equivalent to \(
    \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}
    \) and matrix equivalence is transitive.
  \item \(\Rightarrow\): Let \(\alpha:\F^n\to \F^m\) be the linear map represented by \(A\) with repect to, say, the standard basis. Since \(A'=Q^{-1}AP\) for some invertible \(P\) and \(Q\), \(A'\) represents the same \(\alpha\) with respect to another bases. \(r(\alpha)\) is defined in a basis-invariant way so \(r(A) = r(\alpha) = r(A')\).
  \end{itemize}
\end{proof}

\begin{theorem}
  \label{thm:upper corner matrix}
  \[
    r(A) = r(A^T).
  \]
\end{theorem}

\begin{proof} \(
    Q^{-1}AP =
    \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}_{n,m}
  \) where \(P\) and \(Q\) are invertible. Take transpose of the whole equation:
  \[
    \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}_{n,m}
    =(Q^{-1}AP)^T = P^TA^T(Q^T)^{-1}
  \]
  so \(A^T\) is equivalent to
  \[
      \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}
  \]
\end{proof}

Note a special case for change of basis: \(V=W\), \(\basis C=\basis B\), the other basis is \(\basis B'\). \(P\), the change-of-basis matrix from \(\basis B'\) to \(\basis B\), given a mpar \(\alpha \in L(V,V)\)
\[
  [\alpha]_{\basis B',\basis B'} = P^{-1}[\alpha]_{\basis B,\basis B}P
\]

\begin{definition}[Similar matrices]
  Given \(A, A' \in M_{n,n}(\F)\), \(A\) and \(A'\) are \emph{similar}, or \emph{conjugate} if
  \[
    A' = P^{-1}AP
  \]
  for some invertible \(P\).
\end{definition}

\subsubsection{Elementary Matrices and Operations}

\begin{definition}[Elementary column operation]
  \emph{Elementary column operation} on a \(m\times n\) matrix \(A\) is one of the following operations:
  \begin{enumerate}
  \item swap column \(i\) and \(j\) (wlog \(i\neq j\)),
  \item scale column \(i\) by \(\lambda\)  (\(\lambda\neq0\)),
  \item add \(\lambda\) times column \(i\) to column \(j\) (\(i\neq j,\lambda\neq 0\)).
  \end{enumerate}
\end{definition}

\begin{definition}[Elementary row operation]
  Defined analoguously, replacing ``column'' by ``row''.
\end{definition}

\begin{note}
  All of these operations are invertible.
\end{note}

\begin{definition}[Elementary matrix]
The elementary row (column, respectively) have corresponding elementary matrices: th effect of performing these column (row, respectively) operations on \(I_n\) (\((I_m\), respectively):
\begin{enumerate}
\item
  \[
    \begin{pmatrix}
      1 & 0 & \cdots & 0 & 0 & 0\\
      \vdots & \ddots & & & & \vdots\\
      & & 1 & & \cdots & 0\\
      0 & \cdots & 0 & \ddots & 0 & 0 \\
      & & 0 & 0 & 1 & &\\
      \vdots & && \ddots& &\\
      0 & & \cdots & & \cdots & 0
    \end{pmatrix}
  \]
\item
  \[
    \begin{pmatrix}
      1 & 0 & 0 & 0 \\
      \vdots & \ddots & & \vdots \\
      0 & \cdots & \lambda & 0 \\
      \vdots & & & \vdots \\
      0 & \cdots & 0 & 1
    \end{pmatrix}
  \]
\item \(I_n+\lambda E_{ij}\) where \(E_{ij}\) is the matrix with \(1\) on \(ij\)th entry and \(0\) elsewhere.
\end{enumerate}
\end{definition}

An elementary column (row, respectively) operation on \(A\in M_{m,n}(\F)\) can be performed by multiplying \(A\) by these corresponding elementary matrices on the right (left respectively).

\begin{eg}
  \[
    \begin{pmatrix}
      1 & 2 \\ 3 & 4
    \end{pmatrix}
    \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} = \begin{pmatrix} 2 & 1 \\ 4 & 3 \end{pmatrix}
  \]
\end{eg}

Given the elementary matrices, we can give a constructive proof that any \(m\times n\) matrix is equivalent to \(\begin{pmatrix} I_r & 0 \\ 0 & 0 \end{pmatrix}\) for some \(r\):

\begin{proof}[Constructive proof of Theorem~\ref{thm:upper corner matrix}]
  Start with \(A\). If all entries of \(A\) are zero then done. If not the some \(a_{ij} = \lambda \neq 0\). Perform the following:
  \begin{enumerate}
  \item swap row \(1\) and \(i\), swap column \(1\) and \(j\) so \(\lambda\) is in position \((1,1)\),
  \item multiply column \(1\) by \(1/\lambda\) to get \(1\) in position \((1,1)\),
  \item add \((-a_{1,2})\) times column \(1\) to column \(2\). Do so for the other entries in row \(1\). Also use row operations to clear out all other entries in column \(1\). Now the matrix is in the form
    \[
      \begin{pmatrix}
        1 & 0 \\
        0 & A'
      \end{pmatrix}
    \]
  \item iterate for \(A'\). Stop when the new \(A' = 0\).
  \end{enumerate}
  The result of these operations is
  \[
    \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}
    =\underbrace{E'_{\ell}E'_{\ell-1}\dots E'_1}_{Q^{-1}} A \underbrace{E_1E_2\dots E_{\ell-1}E_\ell}_{P}.
  \]
  As elementary operations are invertible, the elementary matrices are invertible so
  \[
    Q^{-1}AP = 
     \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}
  \]
\end{proof}

Some variations of the algorithm:
\begin{enumerate}
\item If you only use elementary row operations, we can get the \emph{row echelon form} of a matrix:
  \[
    \begin{pmatrix}
      a & b & \dots & c \\
      0 & d & \dots & e \\
      \vdots & & \ddots & \vdots \\
      0 & 0 & \dots & f 
    \end{pmatrix}
  \]
\end{enumerate}

\begin{lemma}
  If \(A\) is an \(n\times n\) invertible matrix then we can obtain \(I_n\) by using only elementary row/column operations.
\end{lemma}

\begin{proof}
  We prove the column operation case. Use induction on \(n\), the number of rows. Suppose we have got \(\begin{pmatrix} I_k & 0 \\ \star & \ast \end{pmatrix}\) for some \(k\geq 0\). There exists \(j>k\) such that \(a_{k+1,j}\neq 0\), i.e.\ in the \(\ast\) block) as otherwise \((0,\dots,1,\dots, 0)\) with \(1\) is \((k+1)\)th position would not be in the span of the column vectors, contradicting the invertiblity. Next we carry out the following operations:
  \begin{enumerate}
  \item swap column \(k+1\) and \(j\),
  \item divide volumn \(k+1\) by \(\lambda\) so have \(1\) in \((k+1,k+1)\) position,
  \item use column operation to clear other entries of \((k+1)\)th row.
  \end{enumerate}
  Proceed inductively.
\end{proof}

Note that the equality
\[
  AE_1E_2\dots E_c = I_n
\]
gives
\[
  A^{-1} = E_1E_2\dots E_c,
\]
one way to compute inverses.

\begin{proposition}
  Any invertible matrix can be written as a product of elementary ones.
\end{proposition}

\section{Dual Spaces \& Dual Maps}

\subsection{Definition}

Let \(V\) be an \(\F\)-vector space.

\begin{definition}
  The \emph{dual space} of \(V\) is defined to be
  \[
    V^* = L(V,\F) = \{\alpha:V\to \F, \alpha \text{ linear}\}.
  \]
\end{definition}

\(V^*\) is itself an \(\F\)-vector space. Its elements are sometimes called \emph{linear functionals}.

\begin{eg}\leavevmode
  \begin{enumerate}
  \item \(\R^3 \to \R, (a,b,c)\mapsto a-c\) is an element of \(V^*\).
  \item \(\tr: M_{n,n}(\F)\to \F, A\mapsto \sum_i A_{ii}\) is an element of \(M_{n,n}(\F)^*\).
  \end{enumerate}
\end{eg}

\begin{lemma}[Dual basis]
  Let \(V\) be a finite-dimensional \(\F\)-vector space with basis \(\basis B = \{e_1,\dots,e_n\}\). Then there is a basis for \(V^*\), given by
  \[
    \basis B* = \{\varepsilon_1,\dots, \varepsilon_n\}
  \]
  where
  \[
    \varepsilon_j \Big( \sum_{i=1}^{n} a_i e_i \Big) = a_j
  \]
  for \(1\leq j\leq m\).

  \(\basis B^*\) is called the \emph{dual basis} to \(\basis B\).
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item linear independence: suppose
    \[
      \sum_{j=1}^{n}\lambda_j\varepsilon_j = 0.
    \]
    Apply the relation to basis vectors,
    \[
      0 = \Big( \sum_{j=1}^n \lambda_j\varepsilon_j \Big) e_i = \sum_{j=1}^n \lambda_j\varepsilon_j(e_i)
      \]
      The last expression is 
      \[
        \varepsilon_j(e_i) = 
      \begin{cases}
        0 & \text{ if } i \neq j \\
        1 & \text{ if } i = j
      \end{cases}
    \]
    so \(\lambda_i=0\) for all \(1 \leq i \leq n\).
  \item span: if \(\alpha \in V^*\), then
    \[
      \alpha = \sum_{i=1}^{n}\alpha(e_i)\varepsilon_i
    \]
    since linear maps are uniquely determined by the action on basis.
  \end{itemize}
\end{proof}

\begin{corollary}
  If \(V\) is a finite-dimensional \(\F\)-vector space then
  \[
    \dim V = \dim V^*.
  \]
\end{corollary}

\begin{remark}
  Sometimes it is useful to think about \((\F^n)^*\) as the space of row vectors of length \(n\) over \(\F\).
\end{remark}

\subsection{Dual Map}

It turns out dual spaces have maps between them. Before studying them in detail later, we introduce this concept add richness to the theory of dual map:

\begin{definition}[Annihilator]
  If \(U \subseteq V\), the \emph{annihilator} of \(U\) is
  \[
    U^\ann = \{\alpha\in V^*: \forall u \in U,\,\alpha(u) = 0 \}.
  \]
\end{definition}

\begin{lemma}\leavevmode
  \begin{enumerate}
  \item \(U^\ann \leq V^*\),
  \item If \(U \leq V\) and \(\dim V = n < \infty\) then
    \[
      \dim V = \dim U + \dim U^\ann.
    \]
  \end{enumerate}
\end{lemma}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item \(0 \in U^\ann\). If \(\alpha\) and \(\alpha'\) are in \(U^\ann\) then
    \[
      (\alpha+ \alpha')(u) = \alpha(u) + \alpha'(u) = 0+0 = 0
    \]
    for all \(u\in U\). Similarly \(\lambda\alpha\in U^\ann\) for any \(\lambda \in \F\).
  \item Let \(\basis B = \{e_1,\dots, e_k\}\) be a basis for \(U\) and extend it to a basis for \(V\), say \(e_1,\dots,e_k,e_{k+1},\dots,e_n\). Let \(\basis B^*=\{\varepsilon_1,\dots,\varepsilon_n\}\) be its dual basis. Claim \(\varepsilon_{k+1},\dots,\varepsilon_n \) is a basis for \(U^\ann\):
    \begin{itemize}
    \item If \(i>k,j\leq k\) then \(\varepsilon_i(e_j) = 0 \) so \(\varepsilon_i\in U^\ann\)
    \item Linear independence comes from the fact that \(B^*\) is a basis.
    \item If \(\alpha\in U^\ann\), \(\alpha = \sum_{i=1}^na_i\varepsilon_i\) for some \(\alpha_i\in \F\). Then for any \(j\leq k\),
      \[
        \Big( \sum_{i=1}^{n}a_i\varepsilon_i \Big) (e_j) = 0
      \]
      so \(a_j=0\). It follows that \(\alpha \in \langle \varepsilon_{k+1},\dots,\varepsilon_n \rangle\).
    \end{itemize}
  \end{enumerate}
\end{proof}

\begin{lemma}[Dual space as a contravariant functor]
  Let \(V\) and \(W\) be \(\F\)-vector spaces. Let \(\alpha \in L(V,W)\). Then the map
  \begin{align*}
    \alpha^*: W^* &\to V^* \\
    \varepsilon &\mapsto \varepsilon \compose \alpha
  \end{align*}
  is linear. \(\alpha^*\) is called the \emph{dual} of \(\alpha\).
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(\varepsilon \compose \alpha \in V^*\) since composition preserves linearity.
  \item Fix \(\theta_1,\theta_2\in W^*\),
    \begin{align*}
      \alpha^*(\theta_1+\theta_2) &= (\theta_1+\theta_2)\compose \alpha \\
                                  &= \theta_1\compose \alpha + \theta_2 \compose \alpha \\
      &= \alpha^*\theta_1 + \alpha^*\theta_2
    \end{align*}
  \item Similarly \(\alpha^*(\lambda\theta) = \lambda\alpha^*(\theta)\).
  \end{itemize}
\end{proof}

\begin{proposition}
  Let \(V\) and \(W\) be \(\F\)-vector spaces with bases \(\basis B\) and \(\basis C\) repectively. Let \(\basis B^*\) and \(\basis C^*\) be the dual bases. Consider \(\alpha\in L(V,W)\) with dual \(\alpha^*\), then
  \[
    [\alpha^*]_{\basis C^*,\basis B^*} = [\alpha]^T_{\basis B,\basis C}.
  \]
\end{proposition}

\begin{proof}
  Say \(\basis B = \{b_1,\dots,b_n\}\), \(\basis B^* = \{\beta_1,\dots,\beta_n\}\), \(\basis C = \{c_1,\dots,c_m\}\) and \(\basis C^* = \{\gamma_1,\dots,\gamma_n\}\). Further let \([\alpha]_{\basis B,\basis C} = (a_{ij})\), an \(m\times n\) matrix.
  \begin{align*}
    \alpha^*(\gamma_r)(b_s) &= \gamma_r \compose \alpha(b_s) \\
                            &= \gamma_r(\alpha(b_s)) \\
                            &= \gamma(r) \Big( \sum_{t}^{ }a_{ts}c_t \Big) \\
                            &= \sum_{t}^{ } a_{ts} \gamma_r(c_t) \\
                            &= a_{rs} \\
                            &= \Big( \sum_{i}^{ } a_{ri}\beta_i \Big) (b_s)
  \end{align*}
  Thus
  \[
    \alpha^*(\gamma_r) = \sum_{i}^{ } a_{ri}\beta_i
  \]
  so
  \[
    [\alpha^*]_{\basis C^*,\basis B^*} = [\alpha]^T_{\basis B,\basis C}.
  \]
\end{proof}


It follows that
\begin{lemma}
  Let \(V\) be a finite-dimensioanl \(\F\)-vector space with bases \(\basis E = \{e_1,\dots,e_n\}\) and \(\basis F = \{f_1,\dots,f_n\}\). They have correponding dual bases \(\basis E^* = \{\varepsilon_1,\dots, \varepsilon_n\}\) and \(\basis F^*\). Then the change of basis matrix from \(\basis F^*\) to \(\basis E^*\) is
  \[
    (P^{-1})^T.
  \]
\end{lemma}

\begin{proof}
  \[
    [\id]_{\basis F^*,\basis E^*} = [\id]_{\basis E, \basis F}^T = ([\id]_{\basis F, \basis E}^{-1})^T.
  \]
\end{proof}

\begin{caution}
  \(V \cong V^*\) only if \(V\) is finite dimensional. Let \(V = P\), the space of all real polynomials. It has basis \(\{p_j\}_{j\in \N}\) where \(p(j) = t^j\). In example sheet 2 we will see
\begin{align*}
  P^* &\cong \R^\N \\
  \varepsilon &\to (\varepsilon(p_0), \varepsilon(p_1), \dots)
\end{align*}
and on example sheet 1 we prove
\[
  P \ncong \R^\N
\]
as the latter does not have a countable basis.
\end{caution}

Now we move on to more discussion about annhilator.

\begin{lemma}
  Let \(V\) and \(W\) be \(\F\)-vector spaces. Fix \(\alpha \in L(V,W)\) and let \(\alpha^* \in L(W^*, V^*)\) be its dual. Then
  \begin{itemize}
  \item \(N(\alpha^*) = (\im \alpha)^\ann\), so if \(\alpha^*\) is injectve if and only if \(\alpha\) is surjective.
  \item \(\im \alpha^* \leq N(\alpha)^\ann\), with equality if \(V\) and \(W\) are both finite-dimensional, in which case \(\alpha^*\) is surjective if and only if \(\alpha\) is injective.
  \end{itemize}
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item Let \(\varepsilon \in W^*\), then
  \begin{align*}
    & \varepsilon \in N(\alpha^*) \\
    \Leftrightarrow & \alpha^* (\varepsilon) = 0 \\
    \Leftrightarrow & \varepsilon \compose \alpha = 0 \\
    \Leftrightarrow & \varepsilon(u) = 0 \, \forall u \in \im \alpha \\
    \Leftrightarrow & \varepsilon \in (\im \alpha)^\ann
  \end{align*}

\item Let \(\varepsilon \in \im \alpha^*\), Then \(\varepsilon = \alpha^* (\phi)\) for some \(\phi \in W^*\). For any \(u \in N(a)\),
  \[
    \varepsilon(u) = (\alpha^* \phi)(u) = (\phi\compose \alpha)(u) = \phi(\alpha(u)) = \phi(0) = 0
  \]
  so \(\varepsilon \in N(\alpha)^\ann\).

  Now use the fact that \(V\) and \(W\) are finite-dimensional:
  \[
    \dim \im(\alpha^*) = r(\alpha^*) = r(\alpha)
  \]
  as \(r(A) = r(A^T)\). On the other hand,
  \[
    r(\alpha) = \dim V - \dim N(\alpha) = \dim (N(\alpha))^\ann
  \]
  Thus they are equal.
\end{itemize}
\end{proof}

\subsection{Double Dual}

Let \(V\) be an \(\F\)-vector space. Then \(V^* = L(V,\F)\) is its dual space. The natrual next step is

\begin{definition}[Double dual]
  The \emph{double dual} of \(V\) is
  \[
    V^{**} = V^* = L(V^*, \F).
  \]
\end{definition}

\begin{theorem}[Double dual as natural transformation]
  If \(V\) is an \(\F\)-vector space, then the map
  \begin{align*}
    \hat \cdot: V &\to V^{**} \\
    v &\mapsto \hat v
  \end{align*}
  where \(\hat v(\varepsilon) = \varepsilon(v)\), is an natural homomorphism. In particular when \(V\) is finite-dimensional this is a natural isomorphism.
\end{theorem}

\begin{proof}\leavevmode
  \begin{itemize}
  \item For \(v\in V\), the map \(\hat v: V^* \to \F\) is linear so \(\hat \cdot\) does give a map from \(V\) to \(V^{**}\).
  \item Linear: for \(v_1, v_2 \in V\), \(\lambda_1, \lambda_2 \in \F\), \(\varepsilon \in V*\), then
    \[
      \reallywidehat{\lambda_1v_1 + \lambda_2v_2}(\varepsilon) = \varepsilon(\lambda_1v_1 + \lambda_2v_2) = \lambda_1 \varepsilon(v_1) + \lambda_2 \varepsilon(v_2) = \lambda_1 \hat v_1(\varepsilon) + \lambda_2 \hat v_2(\epsilon)
    \]
  \item Injectivity: let \(e \in V\setminus\{0\}\). Extend it to a basis of \(V\), say \(e, e_2,\dots, e_n\). Let \(\varepsilon, \varepsilon_2,\dots, \varepsilon_n\) be its corresponding dual basis. Now
    \[
      \hat e(\varepsilon) = \varepsilon(e) = 1 
    \]
    so \(\hat e\) is non-zero. \(\hat \cdot\) is injective.
  \item Finally, if \(V\) is finite-dimensional, \(\dim V = \dim V^* = \dim V^{**}\) so \(\hat \cdot\) is an isomorphism.
  \end{itemize}
\end{proof}

\begin{lemma}
  Let \(V\) be a finite-dimensional \(\F\)-vector space and \(U \leq V\). Then
  \[
    \hat U = U^{\ann \ann}.
  \]
  So \(U = U^{\ann \ann}\).
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item First show \(\hat U \leq U^{\ann \ann}\): given \(u \in U\), for all \(\varepsilon \in U^\ann\), \(\varepsilon(u) = 0\) so \(\hat u(\varepsilon) = 0\). Thus \(\hat u \in (U^\ann)^\ann = U^{\ann \ann}\).
  \item
    \[
      \dim U^{\ann \ann} = \dim V^* - \dim U^\ann = \dim V - \dim U^\ann = \dim U
    \] so \(\hat U = U^{\ann \ann}\).
  \end{itemize}
\end{proof}

\begin{lemma}[Galois Connection]
  Let \(V\) be a finite-dimensional \(\F\)-vector space and \(U_1, U_2 \leq V\). Then
  \begin{itemize}
  \item \((U_1 + U_2)^\ann = U_1^\ann \cap U_2^\ann\),
  \item \((U_1 \cap U_2)^\ann = U_1^\ann + U_2^\ann\).
  \end{itemize}
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item Let \(\theta \in V^*\), \(\theta \in (U_1+U_2)^\ann\) if and only if \(\theta(u_1+u_2) = 0\) for all \(u_1\in U_1, u_2 \in U_2\), if and only if \(\theta(u) = 0\) for all \(u \in U_1 \cup U_2\), so \(\theta\in U_2^\ann \cap U_2^\ann\).
  \item Apply \(^\ann\) to the first result.
  \end{itemize}
\end{proof}
\end{document}