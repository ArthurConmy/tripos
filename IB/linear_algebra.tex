\documentclass[a4paper]{article}

\def\npart{IB}

\def\ntitle{Linear Algebra}
\def\nlecturer{A.\ M.\ Keating}

\def\nterm{Michaelmas}
\def\nyear{2017}

\input{header}

\newcommand*{\M}{\matrixring}
\newcommand*{\spans}{\generation}

\newcommand*{\ann}{\circ}

\newcommand*{\basis}{\mathcal}

\theoremstyle{definition}
\newtheorem*{caution}{Caution}


\makeindex

\begin{document}

\input{titlepage}

\tableofcontents

\section{Vector Space}

\begin{convention}
  Throughout this course, $\mathbb{F}$ denotes a general field. If you wish, think of it as $\mathbb{R}$ or $\mathbb{C}$.
\end{convention}

\subsection{Definition}

\begin{definition}[Vector space]\index{vector space}
  An $\mathbb{F}$-\emph{vector space} (or a vector space over $\mathbb{F}$) is an abelian group $(V, +)$ equipped with a function, called \emph{scalar multiplication}:
  \begin{align*}
    \mathbb{F}\times V &\to V \\
    (\lambda, v) &\mapsto \lambda\cdot v
  \end{align*}
  satisfying the axioms
  \begin{itemize}
  \item distributive over vectors: $\lambda(v_1+v_2) = \lambda(v_1+v_2)$,
  \item distributive over scalars: $(\lambda_1+\lambda_2)v= \lambda_1 v+\lambda_2 v$,
  \item $\lambda(\mu v) = \lambda \mu v$,
  \item $1\cdot v = v$.
  \end{itemize}
\end{definition}

The additive unit of $V$ is denoted by $\V 0$.

\begin{eg}\leavevmode
  \label{eg:matrix as V}
  \begin{enumerate}
  \item $\forall n \in \mathbb{N}, \mathbb{F}^n$ is the space of column vectors of length $n$ with entries in $\mathbb{F}$. It is an vector space by entry-wise addition and entry-wise scalar multiplication.
  \item $\\M_{m,n}(\mathbb{F})$, the set of $m\times n$ matrices with entries in $\mathbb{F}$, with the operation defined as entry-wise addition.
    \item For any set $X$, $\mathbb{R}^X = \{f: X \to \mathbb{R}\}$, the set of $\mathbb{R}$-valued functions on $X$, with addition and scalar multiplication defined pointwise. For instance, $(f_1+f_2)(x) = f_1(x)+f_2(x)$.
  \end{enumerate}
\end{eg}

\begin{ex}\leavevmode
  \begin{enumerate}
  \item Check the above examples satisfy the axioms.
    \item $0\cdot v = \V 0$ and $(-1)\cdot v = -v$ for all $v \in V$.
  \end{enumerate}
\end{ex}

\subsection{Vector Subspace}

\begin{definition}[Vector subspace]\index{vector space!subspace}
  Let $V$ be an $\mathbb{F}$-vector space. A subset $U \subseteq V$ is a \emph{subspace}, denoted $U \leq V$, if
  \begin{itemize}
  \item $\V 0 \in U$,
  \item $U$ is closed under addition: $\forall u_1, u_2 \in U, u_1+u_2 \in U$,
    \item $U$ is closed under scalar multiplication: $\forall u \in U, \forall \lambda \in \mathbb{F}, \lambda u \in U$.
  \end{itemize}
\end{definition}

\begin{ex}
  If $U$ is a subspace of $V$, then $U$ is also an $\mathbb{F}$-vector space.
\end{ex}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item $V = \mathbb{R}^{\mathbb{R}}$, the set all functions from $\mathbb{R}$ to itself, has a (proper) subspace $C(\mathbb{R})$, the space of continuous functions on $\mathbb{R}$ as continuous functions are closed under addition and scalar multiplication. $C(\mathbb{R})$ in turn has a proper subspace $P(\mathbb{R})$, the set of all polynomials in $\mathbb{R}$.
    \item $\{(x_1,x_2,x_3) \in \mathbb{R}^3: x_1+x_2+x_3 = t\}$ where $t$ is some fixed constant is a subspace of $\mathbb{R}^3$ if and only if $t = 0$.
  \end{enumerate}
\end{eg}

\begin{proposition}
  Let $V$ be an $\mathbb{F}$-vector space, $U, W \leq V$. Then $U \cap W \leq V$.
\end{proposition}

\begin{proof}\leavevmode
  \begin{itemize}
  \item $\V 0 \in U, \V 0 \in V$ so $\V 0 \in U \cap W$.
    \item Suppose $u, w \in U \cap W$. Fix $\lambda, \mu \in \mathbb{F}$. As $U \leq V$, $\lambda u + \mu w \in U$. As $W \leq V$, $\lambda u +\mu w \in W$ so $\lambda u + \mu w \in U \cap W$. Take $\lambda = \mu = 1$ for vector addition and $\mu = 0$ for scalar multiplication.
  \end{itemize}
\end{proof}

\begin{eg}
  $V = \mathbb{R}^3, U = \{(x,y,z): x=0\}, W=\{(x,y,z):y=0\}$, then $U\cap W=\{(x,y,z):x=y=0\}$.
\end{eg}

\begin{note}
The union of a family of subspaces is \emph{almost never} a subspace. For example, $V = \mathbb{R}^2$, $U, V$ be $x$- and $y$-axis.
\end{note}

\begin{definition}[Sum of vector spaces]\index{vector space!sum}
  Let $V$ be an $\mathbb{F}$-vector space, $U, W \leq V$, the \emph{sum} of $U$ and $W$ is the set
  \[
    U + W = \{u+w: u\in U, w\in W\}
  \]
\end{definition}

\begin{eg}
  Use the definition from the previous example, $U+W=V$.
\end{eg}

\begin{proposition}
  $U+W \leq V$.
\end{proposition}

\begin{proof}\leavevmode
  \begin{itemize}
  \item $\V 0 = \V 0 + \V 0 \in U+W$,
  \item $u_1,u_1\in U, w_1,w_2\in W$, $(u_1+w_2) + (u_2+w_2) = (u_1+u_2)+(w_1+w_2) \in U+W$,
    \item similar for scalar multiplication. Left as an exercise.
  \end{itemize}
\end{proof}

\begin{note}
  $U+W$ is the smallest subspace containing both $U$ and $W$. This is because all elements of the form $u+w$ are in such a space by closure under addition.
\end{note}

\begin{definition}[Quotient vector space]\index{vector space!quotient}
  Let $V$ be an $\mathbb{F}$-vector space, $U \leq V$. The \emph{quotient space} $V/U$ is the abelian gropup $V/U$ equipped with scalar multiplication
  \begin{align*}
    \mathbb{F} \times V/U &\to V/U \\
    (\lambda, v+U) &\mapsto \lambda v+U
  \end{align*}
\end{definition}

\begin{proposition}
  This is well-defined and $V/U$ is an $\mathbb{F}$-vector space.
\end{proposition}

\begin{proof}
  First check it is well-defined. Suppose $v_1+U= v_2+U \in V/U$. Then $v_1-v_2\in U$. Now use closure under scalar multiplication and distributivity, $\lambda v_1 - \lambda v_2 = \lambda(v_1-v_2)\in U$ so $\lambda v_1 + U = \lambda v_2 +U\in V/U$.
  Now check vector space axioms of $V/U$, which will follow from the axioms for $V$:
  \begin{itemize}
  \item $\lambda(\mu(v+U)) = \lambda(\mu v+U) = \lambda(\mu v)+U = (\lambda\mu) v+U = \lambda\mu(v+U)$,
  \item other axioms are left as an exercise.
  \end{itemize}
\end{proof}

\subsection{Span, Linear Independence \& Basis}

\begin{definition}[Span]\index{span}
  Let $V$ be a $\F$-vector space, $S \subseteq V$ be a subset. The \emph{span} of $S$
  \[
    \spans S = \Big\{\sum_{s\in S} \lambda_s s : \lambda_s \in F \Big\}
  \]
  is the set of all the finite linear combinations of elements (i.e.\ all but finitely many of the $\lambda$ are zero) of $S$.
\end{definition}

\begin{remark}
  $\spans S$ is the smallest subspace of $V$ containing all elements of $S$.
\end{remark}

\begin{convention}
  $\spans \emptyset = \{\V 0\}$
\end{convention}

\begin{eg}\leavevmode
  \begin{enumerate}
 \item $V=\R^3$, $S = \{(1,0,0),(0,1,2),(3,-2,-4)\}$, $\spans S = \{(a,b,2b): a,b\in \R \}$
 \item For any set $X$, $\R^X$ is a vector space. For $x \in X$, define $\delta_x: X \to \R, \delta_x(x) = 1, \delta_x(y) = 0 \: \forall y \neq x$, then
   \[
     \spans{\delta_x: x\in X} = \{f\in \R^X: f \text{ has finite support} \}
   \]
  \end{enumerate} 
\end{eg}

\begin{definition}[Span]
  $S$ spans $V$ if $\spans S = V$.
\end{definition}

\begin{definition}[Finite-dimensional]\index{finite-dimensional}
  $V$ is \emph{finite-dimensional} over $\F$ if it is spanned by a finite set.
\end{definition}

\begin{definition}[Linear independence]\index{linear independence}
  The vectors $v_1,\ldots, v_n$ are \emph{linearly independent} over $\F$ if
  \[
    \sum_{i=1}^n \lambda_i = 0 \Rightarrow \lambda_i = 0 \: \forall i
  \]
  A subset $S \subset V$ is \emph{linearly independent} if every finite subset of $S$ is linearly independent.

  A subset if \emph{linearly dependent} if it is not linearly independent.
\end{definition}

\begin{eg}
  In the first example above, the three vectors are not linearly independent.
\end{eg}

\begin{ex}
  The set $\{\delta_x: x \in X\}$ is linearly independent.
\end{ex}

\begin{definition}[Basis]\index{basis}
  $S$ is a \emph{basis} of $V$ if it is linearly independent and spans $V$.
\end{definition}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item $\F^n$ has standard basis $\{e_1,e_2,\ldots,e_n\}$ where $e_i$ is the column vector with $1$ in the $i$th entry and $0$ elsewhere.
  \item $V=\C$ over $\C$ has natural basis $\{1\}$, but over $\R$ it has natural basis $\{1, i\}$.
  \item $V=P(\R)$, the space of real polynomials, has natural basis
    \[
      \{1, x, x^2, \dots \}.
    \]
    It is an exercise to check this carefully.
    \end{enumerate}
\end{eg}

\begin{lemma}
  Let $V$ be a $\F$-vector space. The vectors $v_1,\ldots,v_n$ form a basis of $V$ if and only if each vector $v\in V$ has a unique expression
  \[
    v = \sum_{i=1}^n \lambda_i v_i, \lambda_i \in \F.
  \]
  
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item $\Rightarrow:$ Fix $v\in V$. The $v_i$ span $V$, so exists $\lambda_i \in \F$ such that $v = \sum \lambda_i v_i$. Suppose also $v = \sum \mu_i v_i$ for some $\mu_i \in \F$. Then the difference
  \[
    \sum (\mu_i - \lambda_i) v_i = \V 0.
  \]
  Since the $v_i$ are linearly independent, $\mu_i-\lambda_i = 0$ for all $i$.
\item $\Leftarrow:$ The $v_i$ span $V$ by assumption. Suppose $\sum_{i=1}^n \lambda_i v_i = \V 0$. Note that $\V 0 = \sum_{i=0}^n 0 \cdot v_i$. By appying uniqueness to $\V 0$, $\lambda_i = 0$ for all $i$.
  \end{itemize}
\end{proof}

\begin{lemma}
  If $v_1,\ldots, v_n$ spans $V$ over $\F$, then some subset of $v_1,\ldots,v_n$ is a basis of $V$ over $\F$.
\end{lemma}

\begin{proof}
  If $v_1,\ldots, v_n$ is linearly independent then done. Otherwise for some $\ell$, there exist $\alpha_1, \ldots, \alpha_{\ell-1} \in \F$ such that
  \[
    v_\ell = \sum_{i=1}^{\ell-1} \alpha_i v_i.
  \]
  (If $\sum \lambda_i v_i = 0$, not all $\lambda_i$ is zero. Take $\ell$ maximal with $\lambda_\ell \neq 0$, then $\alpha_i = -\frac{\lambda_i}{\lambda_\ell}$.)

  Now $v_1,\ldots,v_{\ell-1},v_{\ell+1},\ldots,v_n$ still span $V$. Continue iteratively until we have linear independence.
\end{proof}

\begin{theorem}[Steinitz Exchange Lemma]
  Let $V$ be a finite-dimensional vector space over $\F$. Take $v_1,\ldots,v_m$ to be linearly independent, $w_1,\ldots,w_n$ to span $V$. Then
  \begin{itemize}
  \item $m \leq n$, and
    \item reordering the $w_i$ if needed, $v_1,\ldots, v_m, w_{m+1},\ldots,w_n$ spans $V$.
  \end{itemize}
\end{theorem}

\begin{proof}
  Proceed by induction. Suppose that we have replaced $\ell \geq 0$ of the $w_i$. Reordering $w_i$ if needed, $v_1,\ldots,v_\ell,w_{\ell+1},\ldots,w_n$ spans $V$.
  \begin{itemize}
  \item If $\ell = m$, done.
  \item If $\ell < m$, then $v_{\ell+1} = \sum_{i=1}^\ell \alpha_i v_i + \sum_{i> \ell} \beta_i w_i$. As the $v_i$ are linearly independent, $\beta_i \neq 0$ for some $i$. After reordering, $\beta_{\ell+1} \neq 0$,
    \[
      w_{\ell+1} = \frac{1}{\beta_{\ell+1}} (v_{\ell+1}-\sum_{i\leq \ell} \alpha_i v_i - \sum_{i>\ell+1} \beta_i w_i).
    \]
    Thus $v_1,\ldots, v_\ell, v_{\ell+1},w_{\ell+2},\ldots, w_n$ also spans $V$. After $m$ steps, we will replace $m$ of the $w_i$ by $v_i$. Thus $m \leq n$.
  \end{itemize}
\end{proof}

\subsection{Dimension}

\begin{theorem}
  If $V$ is a finite-dimensional vector space over $\F$, then any two bases for $V$ have the same cardinality, which is called the \emph{dimension} of $V$, donoted $\dim_\F V$.
\end{theorem}

\begin{proof}
  If $v_1,\ldots, v_n$ and $w_1,\ldots,w_m$ are both bases, then $\{v_i\}$ is linearly independent and $\{w_i\}$ spans $V$ so $n \leq m$. Similarly $m \leq n$.
\end{proof}

\begin{eg}
  $\dim_\C \C = 1$, but $\dim_\R \C = 2$.
\end{eg}

\begin{lemma}
  Let \(V\) be a finite-dimensional \(\F\)-vector space. If \(w_1,\ldots,w_\ell\) is a linearly independent set of vectors, we can extend it to a basis \(w_1,\ldots,w_\ell,w_{\ell+1},\ldots,w_n\).
\end{lemma}

\begin{proof}
  Apply Steinitz exchange lemma to \(w_1,\ldots, w_\ell\) and any basis \(v_1,\ldots, v_n\).

  Or more direcly, if \(V=\langle w_1,\ldots, w_\ell \rangle\), done. Otherwise take \(v_{\ell+1} \in V\setminus\langle w_1,\ldots, w_\ell\rangle\). Now \(w_1,\ldots, w_\ell,w_{\ell+1}\) is linearly independent. Iterate.
\end{proof}

\begin{corollary}
  Let \(V\) be a finite-dimensional vector space of dimension \(n\). Then
  \begin{enumerate}
  \item Any linearly independent set of vectors has at most \(n\) elements, with equality if and only if the set is a basis.
  \item Any spanning set of vectors has at least \(n\) elements, with equaility if and only if the set is a basis.
  \end{enumerate}
\end{corollary}

\begin{slogan}
  Choose the best basis for the job.
\end{slogan}

\begin{theorem}
  Let \(U, W\) be subspaces of \(V\). If \(V\) and \(W\) are finite-dimensional, so is \(U+W\) and
  \[
\dim(U+W) = \dim U + \dim W - \dim(U\cap W).
  \]
\end{theorem}

\begin{proof}
  Pick basis \(v_1,\ldots, v_\ell\) of \(U\cap W\). Extend it to basis \(v_1,\ldots,v_\ell,u_1,\ldots,u_m\) of \(U\) and \(v_1,\ldots,v_\ell,w_1,\ldots,w_n\) of \(W\). Claim \(v_1,\ldots, v_\ell,u_1,\ldots,u_m,w_1,\ldots,w_n\) is a basis for \(U+W\):
  \begin{itemize}
  \item spanning: if \(u\in U\), then \(u= \sum \alpha_iv_i + \sum \beta_iu_i\) and if \(w\in W\), \(w = \sum_{}^{}\gamma_iv_i + \sum_{}^{}\delta_iw_i\), so \(u+w = \sum_{}^{}(\alpha_i + \gamma_i)v_i + \sum_{}^{}\beta_iu_i + \sum_{}^{}\delta_iu_i\).
  \item linear independence: assume \(\sum \alpha_iv_i + \sum \beta_iu_i+ \sum \gamma_iw_i=0\). Rearrange, \(\sum\alpha_iv_i + \sum\beta_iu_i = -\sum\gamma_iw_i \in U\cap W\) so it equals to \(\sum\delta_iv_i\) for some \(\delta_i\in \F\) because \(v_i\) is a basis for \(U\cap W\). As \(v_i\) and \(w_i\) are linearly independent, \(\gamma_i=\delta_i=0\) for all \(i\). Thus \(\sum\alpha_iv_i + \sum\beta_iv_i=0\), so \(\alpha_i=\beta_i=0\) since \(v_i\) and \(u_i\) form a basis for \(U\).
  \end{itemize}
\end{proof}

\begin{theorem}
  Let \(V\) be a finite-dimensional vector space over \(\F\) and \(U \leq V\), then \(U\) and \(V/U\) are also finite-dimensional and
  \[
\dim V = \dim U + \dim V/U.
  \]
\end{theorem}

\begin{proof}
  Left as an exercise. Outline: first show \(U\) is finite-dimensional, then let \(u_1,\ldots,u_\ell\) be a basis for \(U\). Extend it to a basis for \(V\), say \(u_1,\ldots,u_\ell,w_{\ell+1},\ldots,w_n\) of \(V\). Check \(w_{\ell+1}+U,\ldots,w_n+U\) form a basis for \(V/U\).
\end{proof}

\begin{corollary}
  If \(U\) is a proper subspace of \(V\), which is finite-dimensional, then \(\dim U < \dim V\).
\end{corollary}

\begin{proof}
  \(V/U \neq 0\) so \(\dim V/U > 0\).
\end{proof}

\subsection{Direct Sum}

\begin{definition}[Direct sum]\index{vector space!sum!direct}
  Let \(V\) be a vector space over \(\F\), \(U, W\leq V\). Then
  \[
    V = U \oplus W
  \]
  if every element of \(V\) can be written as \(v=u+w\) for some unique \(u\in U, w\in W\). This is called the \emph{internal direct sum}. \(W\) is a \emph{direct complement} of \(U\) in \(V\).
\end{definition}

\begin{lemma}
  Suppose \(U,W\leq V\), TFAE:
  \begin{enumerate}
  \item \(V = U \oplus W\),
  \item \(V=U+W\) and \(U\cap W = 0\),
  \item Given \(\basis B_1\) any basis of \(U\), \(\basis B_2\) any basis of \(V\), \(\basis B = \basis B_1\cup \basis B_2\) is a basis of \(V\).
  \end{enumerate}
\end{lemma}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item \(2 \Rightarrow 1\): any \(v\in V\) is \(u+w\) for some \(u\in U, w\in W\). Suppose \(u_1+w_1=u_2+w_2\), then \(u_1-u_2 = w_2-w_1 \in U\cap W = 0\). Thus \(u_1=u_2,w_1=w_2\).
  \item \(2 \Rightarrow 1\): \(\basis B\) spans as any \(v\in V\) is \(u+w\). Write \(u\) in terms of \(\basis B_1\) and \(w\) in terms of \(\basis B_2\). Then \(u+w\) is a linear combination of elements of \(\basis B\). To show \(\basis B\) is linearly independent, suppose \(\sum_{v\in \basis B} \lambda_v v = \V 0 = \V 0_V + \V 0_W\). Write LHS as \(\sum_{v\in \basis B_1} \lambda_vv + \sum_{v\in \basis B_2}\lambda_vv\). By uniqueness of expression, \(\sum_{v\in \basis B_1}\lambda_vv=\V 0_V\) and \(\sum_{w\in \basis B_2}\lambda_ww=\V 0_w\). As \(\basis B_1, \basis B_2\) are bases, all of the \(\lambda_v, \lambda_w\) are zero.
  \item \(2 \Rightarrow 1\): if \(v\in V, v=\sum_{x\in V}\lambda_xx = \sum_{u\in B_1}\lambda_uu + \sum_{w\in \basis B_1}\lambda_ww\) so \(v\in U+W\). Conversely, if \(v\in U\cap W, v = \sum_{u\in \basis B_1}\lambda_uu=\sum_{w\in \basis B_2}\lambda_ww\) so all \(\lambda_u, \lambda_v\) are zero since \(\basis B_1\cup \basis B_2\) is linearly independent.
  \end{enumerate}
\end{proof}

\begin{lemma}
  Let \(V\) be a finite-dimensional vector space over \(\F\) and \(U\leq V\). Then there exists a direct complement to \(U\) in \(V\).
\end{lemma}

\begin{proof}
  Let \(u_1,\ldots, u_\ell\) be a basis for \(U\). Extend this to a basis \(u_1,\ldots, u_\ell,w_{\ell+1},\ldots,w_n\) for \(V\). Then \(\spans{w_{\ell+1},\ldots,w_n}\) is a direct complement of \(U\).
\end{proof}

\begin{caution}
  Direct complements are \emph{not} unique.
\end{caution}

\begin{definition}[Direct sum]\index{vector space!direct sum}
  Suppose \(V_1,\ldots, V_\ell \leq V\), then the sum
  \[
    \sum_i V_i = V_1+\cdots+V_\ell = \{v_1+\cdots+v_\ell: v_i\in V_i\}.
  \]
  is \emph{direct} if
  \[
    v_1+\cdots+v_\ell = v_1'+\cdots+ v_\ell' \Rightarrow v_i = v_i' \text{ for all } i.
  \]
  In which case it is denoted
  \[
    V = \bigoplus_{i=1}^\ell V_i.
  \]
\end{definition}

\begin{ex}
  \(V_1,\ldots, V_\ell \leq V\), TFAE:
  \begin{enumerate}
  \item The sum \(\sum_i V_i\) is direct,
  \item \(V_i \cap \sum_{j\neq i}V_j = 0\) for all \(i\),
  \item For any basis \(B_i\) of \(V_i\), the union \(B=\bigcup_{i=1}^\ell B_i\) is a basis for \(\sum_i V_i\).
  \end{enumerate}
\end{ex}

\begin{definition}[Direct sum]\index{vector space!direct sum}
  Let \(U, W\) be vector spaces over \(\F\). The \emph{external direct sum} is
  \[
    U\oplus W = \{(u,w): u\in U, w\in W\}
  \]
  with pointwise addition and scalar multiplication.
\end{definition}

\section{Linear Map}

\subsection{Definition}

\begin{definition}[Linear map]\index{linear map}
  \(V, W\) two \(\F\)-vector space, a map \(\alpha: V\to W\) is \emph{linear} if
  \begin{itemize}
  \item \(\alpha(v_1 + v_2) = \alpha(v_1) + \alpha(v_2)\),
  \item \(\alpha(\lambda v) = \lambda \alpha(v)\).
  \end{itemize}
  This is equivalent to
  \[
\alpha(\lambda_1v_1+ \lambda_2v_2) = \lambda_1\alpha(v_1) + \lambda_2\alpha(v_2).
  \]
\end{definition}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Given an \(n\times m\) matrix \(A\) with coefficients in \(\F\), the map \(\alpha: \F^m\to \F^n, v\to Av\).
  \item Differentiation \(D: P(\R) \to P(\R), f\mapsto \frac{df}{dx}\).
  \item Integration \(I: C[0,1] \to C[0,1], f\to I(f)\) where \(I(f)(x) = \int_0^x f(t)dt\).
  \item Fix \(x\in [0,1]\), the map \(C[0,1]\to \R, f\mapsto f(x)\).
  \end{enumerate}
\end{eg}

\begin{note}[Category of \(\mathbf{Vect}_\F\)]
  Suppose \(U, V, W\) are \(\F\)-vector spaces, then
  \begin{enumerate}
  \item \(\id: V\to V\) is linear.
  \item \(U \stackrel{\alpha}{\to} V \stackrel{\beta}{\to} W\), if \(\alpha, \beta\) are linear then so is \(\beta \compose \alpha\).
  \end{enumerate}
\end{note}

\begin{lemma}[Free functor \(\mathbf{Set} \to \mathbf{Vect}_\F\)]
  Suppose \(V, W\) are \(\F\)-vector spaces and \(\basis B\) is a basis for \(V\). If \(\alpha_0: \basis B\to W\) is \emph{any} map, then there is a \emph{unique} linear map \(\alpha: V\to W\) extending \(\alpha_o\).
\end{lemma}

\begin{proof}
  Let \(v\in V\). Write \(v = \sum \lambda_iv_i\) in a unique way. By linearity \(\alpha(v) = \alpha(\sum \lambda_iv_i) = \sum \lambda_i \alpha(v_i) = \sum \lambda_i \alpha_0(v_i)\). Uniqueness follows.
\end{proof}

\begin{note}\leavevmode
  \begin{itemize}
  \item This is true for infinite-dimensional vector spaces as well.
  \item Very often, to define a linear map, define it on a basis and extend it linearly to the vector space.
  \item Two linear maps \(\alpha_1,\alpha_2: V\to W\) are equal if and only if they agree on a basis.
  \end{itemize}
\end{note}

\subsection{Isomorphism of Vector Spaces}

\begin{definition}[Isomorphism]\index{isomorphism}
  Given \(V, W\) two \(\F\)-vector spaces, the map \(\alpha:V\to W\) is an \emph{isomorphism} if it is linear and bijective, denoted \(V \cong W\).
\end{definition}

\begin{lemma}
  \(\cong\) is an equivalence relation on the class of all \(\F\)-vector spaces.
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item symmetric: obvious.
  \item reflexive: blah blah in lecture. Left as an exercise to reader.
  \item transitive: obvious.
  \end{itemize}
\end{proof}

\begin{theorem}
  If \(V\) is an \(\F\)-vector space, then \(V \cong \F^n\) for some \(n\).
\end{theorem}

\begin{proof}
  Choose a basis for \(V\), say \(v_1,\ldots, v_n\). Define a map
  \begin{align*}
    V &\to \F^n \\
    \sum_{i}^{ }\lambda_iv_i &\mapsto (\lambda_1,\ldots,\lambda_n)
  \end{align*}
which is an isomorphism.
\end{proof}

\begin{remark}
  Choosing an isomorphism \(V \cong \F^n\) is equivalent to choosing a basis for \(V\). i.e.\ there is a bijection \(\{\alpha\in\Hom(V,\F^n), \alpha\text{ bijective}\} \leftrightarrow \{\text{bases of } V\} \).
\end{remark}

\begin{theorem}
  Given two finite-dimensional \(\F\)-vector spaces \(V, W\), they are isomorphic if and only if they have the same dimension.
\end{theorem}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(\Leftarrow\): \(V \cong \F^{\dim V} = \F^{\dim W} \cong W\).
  \item \(\Rightarrow\): let \(a:V\to W\) be an isomorphism and \(\basis B\) be a basis for \(V\). Claim \(\alpha(\basis B)\) is a basis for \(W\): \(\alpha(\basis B)\) spans \(W\) due to surjectivity and \(\alpha(\basis B)\) is linearly independent due to injectivity.
  \end{itemize}
\end{proof}

\begin{definition}[Kernel \& Image]\index{linear map!kernel}\index{linear map!image}
  Given \(\alpha: V\to W\),
  \begin{itemize}
  \item \(N(\alpha) = \ker \alpha = \{v\in V: \alpha(v) = 0\} \leq V\),
  \item \(\im \alpha = \{w\in W: \exists v\in V, \alpha(v) = w \} \leq W\).
  \end{itemize}
\end{definition}

\begin{proposition}\leavevmode
  \begin{itemize}
  \item \(\alpha\) is injective if and only if \(N(\alpha) = 0\),
  \item \(\alpha\) is surjective if and only if \(\im \alpha = W\).
  \end{itemize}
\end{proposition}

\begin{proof}
  Easy.
\end{proof}

\begin{eg}
  Let \(\alpha: C^\infty(\R) \to C^\infty(\R), \alpha(f)(t) = f''(t)+2f'(t)+5f(t)\). \(\ker \alpha = \{f:f''+2f'+5f=0\}\) and \(g\in \im \alpha\) if and only if there exists an \(f\) such that \(f''+2f'+5f=g\).
\end{eg}

\begin{theorem}[First Isomorphism Theorem]\index{isomorphism}
  Let \(\alpha: V\to W\) be a linear map. It induces an isomprhism
  \begin{align*}
    \bar \alpha: V/\ker \alpha &\to \im \alpha \\
    v + \ker \alpha &\mapsto \alpha(v)
  \end{align*}
\end{theorem}

\begin{proof}
  Check the following:
  \begin{itemize}
  \item \(\bar \alpha\) is well-defined,
  \item \(\bar \alpha\) is linear: immediate from linearity of \(\alpha\),
  \item \(\bar \alpha\) is surjective.
  \end{itemize}
\end{proof}

\begin{definition}[Rank \& Nullity]\index{linear map!rank}\index{linear map!nullity}\leavevmode
  \begin{itemize}
  \item \(r(\alpha) = rk(\alpha) = \dim( \im \alpha)\) is the \emph{rank} of \(\alpha\),
  \item \(n(\alpha) = \dim N(\alpha)\) is the \emph{nullity} of \(\alpha\).
  \end{itemize}
\end{definition}

\begin{theorem}[Rank-nullity]
  Let \(U, V\) be \(\F\)-vector spaces, \(\dim U < \infty\). Let \(\alpha:U\to V\) be a linear map. Then
  \[
\dim U = r(\alpha) + n(\alpha).
  \]
\end{theorem}

\begin{proof}
  \(U/\ker \alpha \cong \im \alpha\) so \(\dim U - \dim (\ker \alpha) = \dim (\im \alpha)\). Rearrange.
\end{proof}

\begin{lemma}
  Let \(V, W\) be \(\F\)-vector spaces with equal, finite dimension. Let \(\alpha:V\to W\) be linear, then TFAE:
  \begin{enumerate}
  \item \(\alpha\) is injective,
  \item \(\alpha\) is surjective,
  \item \(\alpha\) is an isomorphism.
  \end{enumerate}
\end{lemma}

\begin{proof}
  Rank-nullity theorem.
\end{proof}

\subsection{Linear Maps as Vector Space}

Suppose \(V\) and \(W\) are \(\F\)-vector spaces. Let \(L(V,W) = \{\alpha:V\to W, \alpha \text{ linear}\}\).

\begin{proposition}
  \(L(V,W)\) is an \(\F\)-vector space, under operations
  \begin{align*}
    (\alpha_1+\alpha_2)(v) &= \alpha_1(v) + \alpha_2(v) \\
    (\lambda\alpha)(v) &= \lambda(\alpha(v))
  \end{align*}
\end{proposition}

\begin{proof}
  \(\alpha_1+\alpha_2, \lambda\alpha\) as above are well-defined linear maps. The vector space axioms can be easily checked.
\end{proof}

\begin{proposition}
  \label{prop:dimension of linear map space}
  If both \(V\) and \(W\) are finite-dimensional over \(\F\) then so is \(L(V,W)\) and \(L(V,W) = \dim V \cdot \dim W\).
\end{proposition}

\begin{proof}
  See Lemma~\ref{cor:dim of hom}.
\end{proof}

\subsubsection{Matrices, an Interlude}

\begin{definition}[Matrix]\index{matrix}
  An \emph{\(m\times n\) matrix} over \(\F\) is an array with \(m\) rows and \(n\) columns with entries in \(\F\). We write
  \[
A = (a_{ij}), a_{ij}\in\F, 1\leq i \leq m, 1\leq j \leq n.
  \]
\end{definition}

\begin{definition}
  \(\M_{m,n}(\F)\) is the set of all such \(m\times n\) matrices.
\end{definition}

\begin{proposition}
  \(\M_{m,n}(\F)\) is an \(\F\)-vector space and \(\dim \M_{m,n}(\F) = m\cdot n\).
\end{proposition}

\begin{proof}
  See this \hyperref[eg:matrix as V]{example} for the proof of vector space axioms. For the dimension claim, a standard basis for \(\M_{m,n}(F)\) is
  \[
    E_{ij}=
    \begin{pmatrix}
      0 & \dots & 0 \\
      \vdots & \ddots & \vdots \\
      0 & 1 & 0 \\
      \vdots & \ddots & \vdots \\
      0 & \dots & 0 
    \end{pmatrix}
  \]
  with \(1\) in the \((i,j)\)th entry so \(a_{ij} = \sum_{i,j}^{} a_{ij}E_{ij}\), from which span and linear independence follow. The basis has cardinality \(m\cdot n\).
\end{proof}

\subsubsection{Representation of Linear Maps by Matrices}

Let \(V\) and \(W\) be finite-dimensional \(\F\)-vector space, \(\alpha: V\to W\) linear. Let \(\basis B = \{v_1,\ldots,v_n\}\) be a basis for \(V\), \(\basis C = \{w_1,\ldots,w_m\}\) be a basis for \(W\). If \(v=\sum_{i}\lambda_iv_i \in V\), write
\[
[v]_{\basis B} =
\begin{pmatrix}
  \lambda_1 \\
  \vdots \\
  \lambda_n
\end{pmatrix}
\in \F^n
\]
which is called the \emph{coordinate vector of \(v\) with respect to \(\basis B\)}. Similarly \([w]_{\basis C}\in \F^m\).

\begin{definition}[Matrix representation]\index{linear map!matrix representation}
  \([\alpha]_{\basis B, \basis C}\) is the matrix representation of \(\alpha\) with respect to \(\basis B\) and \(\basis C\) with
  \begin{align*}
    [\alpha]_{\basis B, \basis C} &= \Big( [\alpha(v_1)]_{\basis C} \: \Big| \: [\alpha(v_2)]_{\basis C} \: \Big | \: \cdots \: \Big| \: [\alpha(v_n)]_{\basis C} \Big) \\
                                  &= (a_{ij})
  \end{align*}
\end{definition}

The matrix says
\[
  \alpha(v_j) = \sum_{i}^{ }a_{ij}w_i.
\]

\begin{lemma}
  For any \(v\in V\),
  \[
[\alpha(v)]_{\basis C} = [\alpha]_{\basis B, \basis C}\cdot [v]_{\basis B}
  \]
  where \(\cdot\) is matrix multiplication.
\end{lemma}

\begin{proof}
  Fix \(v =\sum_{j=1}^{n}\lambda_jv_j \in V\), so
  \[
[v]_{\basis B} =
\begin{pmatrix}
  \lambda_1 \\
  \vdots \\
  \lambda_n
\end{pmatrix}
\]
\begin{align*}
  \alpha(v) &= \alpha\left( \sum_{j}^{ }\lambda_jv_j \right) \\
            &= \sum_{j}^{ }\lambda_j\alpha(v_j) \\
            &= \sum_{j}^{ }\lambda_j\left( \sum_{i}^{ }\alpha_{ij}w_i \right) \\
            &= \sum_{i}^{ }\left( \sum_{j}^{} a_{ij}\lambda_j \right) w_i
\end{align*}
so the \(i\)th entry of \(\alpha(v)\) is the \(i\)th entry of \([\alpha]_{\basis B, \basis C} \cdot [v]_{\basis B}\).
\end{proof}

\begin{lemma}
  Suppose \(U \stackrel{\beta}{\to} V \stackrel{\alpha}{\to} W\) with \(\alpha, \beta\) linear, with \(\alpha \compose \beta: U\to W\). Let \(\basis A, \basis B, \basis C\) be bases for \(U,V,W\) respectively. Then
  \[
    [\alpha\compose\beta]_{\basis A, \basis C} = [\alpha]_{\basis B,\basis C}\cdot[\beta]_{\basis A, \basis B}.
  \]
\end{lemma}

\begin{proof}
  \begin{align*}
    (\alpha\compose\beta)(u_\ell) &= \alpha(\beta(u_\ell)), \: u_\ell\in A\\
                                  &= \alpha\Big( \sum_{j}^{ }b_{jl}v_j \Big), \: v_j\in B \\
                                  &= \sum_{j}^{ }b_{jl}\alpha(v_j) \\
                                  &= \sum_{j}^{ }b_{jl}\sum_{i}^{ }a_{ij}w_i, \: w_i\in W \\
                                  &= \sum_{i}^{ }\left( \sum_{j}^{ }a_{ij}b_{jl} \right)w_i
  \end{align*}
\end{proof}

\begin{proposition}
  Let \(V\) and \(W\) be \(\F\)-vector spaces with \(\dim V = n, \dim W = m\), then
  \[
L(V,W) \cong \M_{m,n}(\F).
  \]
\end{proposition}

\begin{proof}
  Fix bases \(B=\{v_1\ldots,v_n\}, C=\{w_1,\ldots,w_m\}\) for \(V\) and \(W\) respectively. Claim
  \begin{align*}
    \theta: L(V,W) &\to \M_{m,n}(\F) \\
    \alpha &\mapsto [\alpha]_{\basis B, \basis C}
  \end{align*}
  is an isomorphism:
  \begin{itemize}
  \item linearity: \([\lambda_1\alpha_1+\alpha_2\alpha_2]_{\basis B, \basis C} = \lambda_1[\alpha_1]_{\basis B, \basis C} + \lambda_2[\alpha_2]_{\basis B, \basis C}\).
  \item surjectivity: given \(A = (a_{ij})\), let \(\alpha:v_j\mapsto \sum_{i=1}^{m}a_{ij}w_i \) and extend linearly. It follows that \(\alpha\in L(V,W)\) and \(\theta(\alpha) = A\).
  \item injectivity: \([\alpha]_{\basis B, \basis C} = \V 0\) implies that \(\alpha\) is the zero map.
  \end{itemize}
\end{proof}

\begin{corollary}
  \label{cor:dim of hom}
  \[
\dim L(V,W) = \dim V \cdot \dim W.
  \]
\end{corollary}

\begin{eg}
  Suppose \(\alpha:V\to W\), \(Y\leq V, Z\leq W\) with \(\alpha(Y)\leq Z\). Let \(\basis B'=\{v_1,\ldots,v_k\}\) be a basis of \(Y\) and extend to \(\basis B=\{v_1,\dots,v_k,v_{k+1}, \dots, v_n\}\) a basis for \(V\). Similarly \(\basis C' = \{w_1,\dots,w_l\}\) and \(\basis C\) for \(Z\) and \(W\).
  \begin{itemize}
  \item \([\alpha]_{\basis B, \basis C} =
\begin{pmatrix}
  A & B \\
  0 & C
\end{pmatrix}
\) for some \(A, B, C\) because for \(1\leq j \leq k\), \(\alpha(v_j)\) is a linear combination of \(w_i\) where \(1\leq i \leq l\).
  \item \([\alpha|_Y]_{B',C'} = A. \)
  \item \(\alpha\) induces a map
    \begin{align*}
      \bar\alpha: V/Y &\to W/Z \\
      v+ Y &\mapsto \alpha(v) + Z
    \end{align*}
    This is well-defined. Linearity follows from that of \(\alpha\). A basis for \(V/Y\) is \(\basis B''=\{v_{k+1}+Y,\ldots,v_n+Y\}\) and similarly for \(W/Z\). It is an exercise to show \([\bar\alpha]_{\basis B'', \basis C''} = C \).
  \end{itemize}
\end{eg}

\subsubsection{Change of Bases}

Throughout this section, let \(V\) and \(W\) be \(\F\)-vector spaces and suppose they have the following bases:
\begin{table}[htbp]
  \centering
  \begin{tabular}{|c||c|c|}
    \hline
    Vector space & \(V\) & \(W\) \\ \hline
    Basis 1 & \(\basis B = \{v_1,\dots,v_n\}\) & \(\basis C = \{w_1,\dots,w_m\}\) \\ \hline
    Basis 2 & \(\basis B' = \{v_1',\dots,v_n'\}\) & \(\basis C' = \{w_1',\dots,w_m'\}\) \\ \hline
  \end{tabular}
\end{table}

\begin{definition}[Change-of-basis matrix]\index{matrix!change-of-basis}
  The \emph{change-of-basis matrix} from \(\basis B'\) to \(\basis B\) is \(P = (p_{ij})\) given by
  \begin{align*}
    v_j' &= \sum_{i}^{ }p_{ij}v_i \\
    P &= \Big( [v_1']_{\basis B} \: \Big| \: [v_2']_{\basis B} \: \Big| \: \dots \Big| \: [v_n']_{\basis B} \Big) = [\id]_{\basis B', \basis B}
  \end{align*}
\end{definition}

\begin{lemma}
  \[
    [v]_{\basis B} = P[v]_{\basis B'}.
  \]
\end{lemma}

\begin{proof}
  \[
    P[v]_{\basis B'} = [\id]_{\basis B', \basis B}[v]_{\basis B'} = [v]_{\basis B}.
  \]
\end{proof}

\begin{lemma}
  \(P\) is an invertible \(n\times n\) matrix and \(P^{-1}\) is the change-of-basis matrix from \(\basis B\) to \(\basis B'\).
\end{lemma}

\begin{proof}
  \begin{align*}
    [\id]_{\basis B, \basis B'}[\id]_{\basis B', \basis B} &= [\id]_{\basis B', \basis B'} = I_n \\
    [\id]_{\basis B', \basis B}[\id]_{\basis B, \basis B'} &= [\id]_{\basis B, \basis B} = I_n
  \end{align*}
\end{proof}

Let \(Q\) be the change-of-basis matrix from \(\basis C'\) to \(\basis C\). Then \(Q\) is an invertible \(m\times m\) matrix.

\begin{proposition}
  Let \(\alpha: V\to W\) be a linear map, \(A = [\alpha]_{\basis B,\basis C}\), \(A' = [\alpha]_{\basis B',\basis C'}\), then
  \[
    A' = Q^{-1}AP.
  \]
\end{proposition}

\begin{proof}
  \[
    \underbrace{[\id]_{\basis C,\basis C'}}_{Q^{-1}} [\alpha]_{\basis B,\basis C} \underbrace{[\id]_{\basis B',\basis B}}_P = \underbrace{[\id\compose\alpha\compose\id]_{\basis B',\basis C'}}_{A'}
  \]
\end{proof}

\begin{definition}[Equivalence of matrices]\index{matrix!equivalence}
  \(A, A' \in \M_{m,n}(\F)\) are \emph{equivalent} if
  \[
    A' = Q^{-1}AP
  \]
  for some invertible \(P\in \M_{n,n}(\F)\) and \(Q\in \M_{m,m}(\F)\).
\end{definition}

\begin{note}
  This defines an equivalence relation on \(\M_{m,n}(\F)\).
\end{note}

\begin{proposition}
  Let \(V, W\) be \(\F\)-vector spaces of dimension \(n\) and \(m\) respectively. Let \(\alpha:V\to W\) be a linear map. Then there exist bases \(\basis B\) of \(V\), \(\basis C\) of \(W\), and some \(r\leq m,n\) such that
  \[
    [\alpha]_{\basis B,\basis C} =
    \begin{pmatrix}
      I_r & 0 \\
      0 & 0 
    \end{pmatrix}
  \]
  where \(I_r\) the is \(r\times r\) the identity matrix.
\end{proposition}

\begin{note}
  \(r = rk(\alpha) = r(\alpha)\).
\end{note}

\begin{proof}
  Fix \(r\) such that \(\dim N(\alpha) = n-r\). Fix a basis for \(N(\alpha)\), say \(v_{r+1},\dots,v_n\). Extend this to a basis \(\basis B\) for \(V\), say \(v_1,\dots,v_r,v_{r+1},\dots,v_n\). Now \(\alpha(v_1),\dots,\alpha(v_r)\) is a basis for \(\im(\alpha)\):
  \begin{itemize}
  \item span: \(\alpha(v_1),\dots, \alpha(v_n)\) certainly span \(\im(\alpha)\). Since \(v_{r+1},\dot,v_n \in \ker \alpha\), \(\alpha(v_{r+1}),\dots,\alpha(v_n) = 0\) so we can remove them from the spanning set.
  \item linear independence: assume \(\sum_{i=1}^{n}\lambda_i \alpha(v_i) =\V 0 \). Then \(\alpha \big(\sum_{i=1}^n\lambda_iv_i\big) =\V0\). This implies that
    \[
      \sum_{i=1}^{n}\lambda_iv_i = \sum_{j=r+1}^{n}\mu_jv_j.
    \]
    As \(v_1,\dots v_n\) are linearly independent, \(\lambda_i=\mu_j=0\) for all \(i,j\).
  \end{itemize}
  Extend \(\alpha(v_1),\dots,\alpha(v_r)\) to a basis for \(W\), say \(\basis C\). By construction,
  \[
    [\alpha]_{\basis B,\basis C} =
    \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}
  \]
\end{proof}

\begin{remark}
  In the proof above we didn't need to assume that \(r = r(\alpha)\). This gives us another way prove Rank-nullity Theorem.
\end{remark}

\begin{corollary}
  Any \(m\times n\) matrix is equivalent to
  \[
  \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}
  \]
  for some \(r\).
\end{corollary}

\begin{definition}[Row and column rank]\index{matrix!rank}
  Let \(A\in \M_{m,n}(\F)\).
  \begin{itemize}
  \item The \emph{column rank} of \(A\), \(r(A)\) is the dimension of the subspace of \(\F^m\) spanned by the columns of \(A\).
  \item The \emph{row rank} of \(A\) is the column rank of \(A^T\).
  \end{itemize}
\end{definition}

\begin{note}
  If \(\alpha\) is a linear map represented by \(A\) with respect to any choice of bases, then \(r(\alpha) = r(A)\).
\end{note}

\begin{proposition}
  Two \(m\times n\) matrices \(A, A'\) are equivalent if and only if
  \[
    r(A) = r(A').
  \]
\end{proposition}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(\Leftarrow\): Both \(A\) and \(A'\) are equivalent to \(
    \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}
    \) and matrix equivalence is transitive.
  \item \(\Rightarrow\): Let \(\alpha:\F^n\to \F^m\) be the linear map represented by \(A\) with repect to, say, the standard basis. Since \(A'=Q^{-1}AP\) for some invertible \(P\) and \(Q\), \(A'\) represents the same \(\alpha\) with respect to another bases. \(r(\alpha)\) is defined in a basis-invariant way so \(r(A) = r(\alpha) = r(A')\).
  \end{itemize}
\end{proof}

\begin{theorem}
  \label{thm:upper corner matrix}
  \[
    r(A) = r(A^T).
  \]
\end{theorem}

\begin{proof} \(
    Q^{-1}AP =
    \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}_{n,m}
  \) where \(P\) and \(Q\) are invertible. Take transpose of the whole equation:
  \[
    \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}_{n,m}
    =(Q^{-1}AP)^T = P^TA^T(Q^T)^{-1}
  \]
  so \(A^T\) is equivalent to
  \[
      \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}
  \]
\end{proof}

Note a special case for change of basis: \(V = W\), \(\basis C = \basis B\) and \(\basis C' = \basis B'\). \(P\), the change-of-basis matrix from \(\basis B'\) to \(\basis B\), is given the map \(\alpha \in L(V,V)\)
\[
  [\alpha]_{\basis B',\basis B'} = P^{-1}[\alpha]_{\basis B,\basis B}P.
\]

\begin{definition}[Similar matrices]\index{matrix!similar}\index{matrix!conjugacy}
  Given \(A, A' \in \M_{n,n}(\F)\), \(A\) and \(A'\) are \emph{similar}, or \emph{conjugate} if
  \[
    A' = P^{-1}AP
  \]
  for some invertible \(P\).
\end{definition}

\subsubsection{Elementary Matrices and Operations}

\begin{definition}[Elementary column operation]\index{elementary operation}
  \emph{Elementary column operation} on a \(m\times n\) matrix \(A\) is one of the following operations:
  \begin{enumerate}
  \item swap column \(i\) and \(j\) (wlog \(i\neq j\)),
  \item scale column \(i\) by \(\lambda\)  (\(\lambda\neq0\)),
  \item add \(\lambda\) times column \(i\) to column \(j\) (\(i\neq j,\lambda\neq 0\)).
  \end{enumerate}
\end{definition}

\begin{definition}[Elementary row operation]
  Defined analoguously, replacing ``column'' by ``row''.
\end{definition}

\begin{note}
  All of these operations are invertible.
\end{note}

\begin{definition}[Elementary matrix]\index{matrix!elementary}
The elementary row (column, respectively) operations have corresponding elementary matrices, which are the results of performing these column (row, respectively) operations on \(I_n\) (\(I_m\), respectively):
\begin{enumerate}
\item
  \[
    \begin{pmatrix}
      1 & 0 & \cdots & & & 0 \\
      \vdots & \ddots & & & & \vdots \\
      & & 0 & & 1 & 0 \\
      0 & \cdots & 0 & \ddots & 0 & 0 \\
      & & 1 & 0 & 0 & & \\
      \vdots & &&  \ddots & \\
      0 & & \cdots & & \cdots & 0
    \end{pmatrix}
  \]
\item
  \[
    \begin{pmatrix}
      1 & 0 & \cdots & & 0 \\
       & \ddots & & & \\
      \vdots & & \lambda & & \vdots \\
       & & & \ddots & \\
      0 & \cdots & & 0 & 1
    \end{pmatrix}
  \]
\item \(I_n+\lambda E_{ij}\) where \(E_{ij}\) is the matrix with \(1\) on \(ij\)th entry and \(0\) elsewhere.
\end{enumerate}
\end{definition}

An elementary column (row, respectively) operation on \(A\in \M_{m,n}(\F)\) can be performed by multiplying \(A\) by these corresponding elementary matrices on the right (left respectively).

\begin{eg}
  \[
    \begin{pmatrix}
      1 & 2 \\ 3 & 4
    \end{pmatrix}
    \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} = \begin{pmatrix} 2 & 1 \\ 4 & 3 \end{pmatrix}
  \]
\end{eg}

Given the elementary matrices, we can give a constructive proof that any \(m\times n\) matrix is equivalent to \(\begin{pmatrix} I_r & 0 \\ 0 & 0 \end{pmatrix}\) for some \(r\):

\begin{proof}[Constructive proof of Theorem~\ref{thm:upper corner matrix}]
  Start with \(A\). If all entries of \(A\) are zero then done. If not the some \(a_{ij} = \lambda \neq 0\). Perform the following:
  \begin{enumerate}
  \item swap row \(1\) and \(i\), swap column \(1\) and \(j\) so \(\lambda\) is in position \((1,1)\),
  \item multiply column \(1\) by \(1/\lambda\) to get \(1\) in position \((1,1)\),
  \item add \((-a_{12})\) times column \(1\) to column \(2\). Do so for the other entries in row \(1\). Also use row operations to clear out all other entries in column \(1\). Now the matrix is in the form
    \[
      \begin{pmatrix}
        1 & 0 \\
        0 & A'
      \end{pmatrix}
    \]
  \item iterate for \(A'\). Stop when the new \(A' = 0\).
  \end{enumerate}
  The result of these operations is
  \[
    \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}
    =\underbrace{E'_{\ell}E'_{\ell-1}\dots E'_1}_{Q^{-1}} A \underbrace{E_1E_2\dots E_{\ell-1}E_\ell}_{P}.
  \]
  As elementary operations are invertible, the elementary matrices are invertible so
  \[
    Q^{-1}AP = 
     \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}
  \]
\end{proof}

If you only use elementary row operations, we can get the \emph{row echelon form} of a matrix:
\[
  \begin{pmatrix}
    a & b & \dots & c \\
    0 & d & \dots & e \\
    \vdots & & \ddots & \vdots \\
    0 & 0 & \dots & f
  \end{pmatrix}
\]

\begin{lemma}
  If \(A\) is an \(n\times n\) invertible matrix then we can obtain \(I_n\) by using only elementary row/column operations.
\end{lemma}

\begin{proof}
  We prove the column operation case. Use induction on \(n\), the number of rows. Suppose we have got \(\begin{pmatrix} I_k & 0 \\ \star & \ast \end{pmatrix}\) for some \(k\geq 0\). There exists \(j>k\) such that \(a_{k+1,j}\neq 0\), (i.e.\ in the \(\ast\) block) as otherwise \((0,\dots,1,\dots, 0)\) with \(1\) in \((k+1)\)th position would not be in the span of the column vectors, contradicting the invertiblity. Next we carry out the following operations:
  \begin{enumerate}
  \item swap column \(k+1\) and \(j\),
  \item divide column \(k+1\) by \(a_{k + 1, k + 1}\) so have \(1\) in \((k+1,k+1)\) position,
  \item use column operation to clear other entries of \((k+1)\)th row.
  \end{enumerate}
  Proceed inductively.
\end{proof}

Note that the equality
\[
  AE_1E_2\dots E_c = I_n
\]
gives
\[
  A^{-1} = E_1E_2\dots E_c,
\]
one way to compute inverses.

\begin{proposition}
  Any invertible matrix can be written as a product of elementary ones.
\end{proposition}

\section{Dual Space \& Dual Map}

\subsection{Definition}

Let \(V\) be an \(\F\)-vector space.

\begin{definition}[Dual space]\index{vector space!dual}
  The \emph{dual space} of \(V\) is defined to be
  \[
    V^* = L(V,\F) = \{\alpha:V\to \F, \alpha \text{ linear}\}.
  \]
\end{definition}

\(V^*\) is itself an \(\F\)-vector space. Its elements are sometimes called \emph{linear functionals}.

\begin{eg}\leavevmode
  \begin{enumerate}
  \item \(\R^3 \to \R, (a,b,c)\mapsto a-c\) is an element of \(V^*\).
  \item \(\tr: \M_{n,n}(\F)\to \F, A\mapsto \sum_i A_{ii}\) is an element of \(\M_{n,n}(\F)^*\).
  \end{enumerate}
\end{eg}

\begin{lemma}[Dual basis]
  Let \(V\) be a finite-dimensional \(\F\)-vector space with basis \(\basis B = \{e_1,\dots,e_n\}\). Then there is a basis for \(V^*\), given by
  \[
    \basis B^* = \{\varepsilon_1,\dots, \varepsilon_n\}
  \]
  where
  \[
    \varepsilon_j \Big( \sum_{i=1}^{n} a_i e_i \Big) = a_j
  \]
  for \(1\leq j\leq m\).

  \(\basis B^*\) is called the \emph{dual basis} to \(\basis B\).
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item linear independence: suppose
    \[
      \sum_{j=1}^{n}\lambda_j\varepsilon_j = 0.
    \]
    Apply the relation to basis vectors,
    \[
      0 = \Big( \sum_{j=1}^n \lambda_j\varepsilon_j \Big) e_i = \sum_{j=1}^n \lambda_j\varepsilon_j(e_i)
      \]
      The last expression is 
      \[
        \varepsilon_j(e_i) = 
      \begin{cases}
        0 & \text{ if } i \neq j \\
        1 & \text{ if } i = j
      \end{cases}
    \]
    so \(\lambda_i=0\) for all \(1 \leq i \leq n\).
  \item span: if \(\alpha \in V^*\), then
    \[
      \alpha = \sum_{i=1}^{n}\alpha(e_i)\varepsilon_i
    \]
    since linear maps are uniquely determined by the action on basis.
  \end{itemize}
\end{proof}

\begin{corollary}
  If \(V\) is a finite-dimensional \(\F\)-vector space then
  \[
    \dim V = \dim V^*.
  \]
\end{corollary}

\begin{remark}
  Sometimes it is useful to think about \((\F^n)^*\) as the space of row vectors of length \(n\) over \(\F\).
\end{remark}

\subsection{Dual Map}

It turns out dual spaces have maps between them. Before studying them in detail, we introduce this concept to add richness to the theory of dual map:

\begin{definition}[Annihilator]\index{annihilator}
  If \(U \subseteq V\), the \emph{annihilator} of \(U\) is
  \[
    U^\ann = \{\alpha\in V^*: \forall u \in U,\,\alpha(u) = 0 \}.
  \]
\end{definition}

\begin{lemma}\leavevmode
  \begin{enumerate}
  \item \(U^\ann \leq V^*\),
  \item If \(U \leq V\) and \(\dim V = n < \infty\) then
    \[
      \dim V = \dim U + \dim U^\ann.
    \]
  \end{enumerate}
\end{lemma}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item \(0 \in U^\ann\). If \(\alpha\) and \(\alpha'\) are in \(U^\ann\) then
    \[
      (\alpha+ \alpha')(u) = \alpha(u) + \alpha'(u) = 0+0 = 0
    \]
    for all \(u\in U\). Similarly \(\lambda\alpha\in U^\ann\) for any \(\lambda \in \F\).
  \item Let \(\basis B = \{e_1,\dots, e_k\}\) be a basis for \(U\) and extend it to a basis for \(V\), say \(e_1,\dots,e_k,e_{k+1},\dots,e_n\). Let \(\basis B^*=\{\varepsilon_1,\dots,\varepsilon_n\}\) be its dual basis. Claim \(\varepsilon_{k+1},\dots,\varepsilon_n \) is a basis for \(U^\ann\):
    \begin{itemize}
    \item If \(i>k,j\leq k\) then \(\varepsilon_i(e_j) = 0 \) so \(\varepsilon_i\in U^\ann\).
    \item Linear independence comes from the fact that \(B^*\) is a basis.
    \item If \(\alpha\in U^\ann\), \(\alpha = \sum_{i=1}^na_i\varepsilon_i\) for some \(\alpha_i\in \F\). Then for any \(j\leq k\),
      \[
        \Big( \sum_{i=1}^{n}a_i\varepsilon_i \Big) (e_j) = 0
      \]
      so \(a_j=0\). It follows that \(\alpha \in \langle \varepsilon_{k+1},\dots,\varepsilon_n \rangle\).
    \end{itemize}
  \end{enumerate}
\end{proof}

\begin{lemma}[Dual space as a contravariant functor]
  Let \(V\) and \(W\) be \(\F\)-vector spaces. Let \(\alpha \in L(V,W)\). Then the map
  \begin{align*}
    \alpha^*: W^* &\to V^* \\
    \varepsilon &\mapsto \varepsilon \compose \alpha
  \end{align*}
  is linear. \(\alpha^*\) is called the \emph{dual} of \(\alpha\).
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(\varepsilon \compose \alpha \in V^*\) since composition preserves linearity.
  \item Fix \(\theta_1,\theta_2\in W^*\),
    \begin{align*}
      \alpha^*(\theta_1+\theta_2) &= (\theta_1+\theta_2)\compose \alpha \\
                                  &= \theta_1\compose \alpha + \theta_2 \compose \alpha \\
      &= \alpha^*\theta_1 + \alpha^*\theta_2
    \end{align*}
  \item Similarly \(\alpha^*(\lambda\theta) = \lambda\alpha^*(\theta)\).
  \end{itemize}
\end{proof}

\begin{proposition}
  Let \(V\) and \(W\) be \(\F\)-vector spaces with bases \(\basis B\) and \(\basis C\) repectively. Let \(\basis B^*\) and \(\basis C^*\) be the dual bases. Consider \(\alpha\in L(V,W)\) with dual \(\alpha^*\), then
  \[
    [\alpha^*]_{\basis C^*,\basis B^*} = [\alpha]^T_{\basis B,\basis C}.
  \]
\end{proposition}

\begin{proof}
  Say \(\basis B = \{b_1,\dots,b_n\}\), \(\basis B^* = \{\beta_1,\dots,\beta_n\}\), \(\basis C = \{c_1,\dots,c_m\}\) and \(\basis C^* = \{\gamma_1,\dots,\gamma_n\}\). Further let \([\alpha]_{\basis B,\basis C} = (a_{ij})\), an \(m\times n\) matrix.
  \begin{align*}
    \alpha^*(\gamma_r)(b_s) &= \gamma_r \compose \alpha(b_s) \\
                            &= \gamma_r(\alpha(b_s)) \\
                            &= \gamma(r) \Big( \sum_{t}^{ }a_{ts}c_t \Big) \\
                            &= \sum_{t}^{ } a_{ts} \gamma_r(c_t) \\
                            &= a_{rs} \\
                            &= \Big( \sum_{i}^{ } a_{ri}\beta_i \Big) (b_s)
  \end{align*}
  Thus
  \[
    \alpha^*(\gamma_r) = \sum_{i}^{ } a_{ri}\beta_i
  \]
  so
  \[
    [\alpha^*]_{\basis C^*,\basis B^*} = [\alpha]^T_{\basis B,\basis C}.
  \]
\end{proof}

It follows that
\begin{lemma}
  Let \(V\) be a finite-dimensional \(\F\)-vector space with bases \(\basis E = \{e_1,\dots,e_n\}\) and \(\basis F = \{f_1,\dots,f_n\}\). They have correponding dual bases \(\basis E^* = \{\varepsilon_1,\dots, \varepsilon_n\}\) and \(\basis F^*\). If the change of basis matrix from \(\basis F\) to \(\basis E\) is \(P\) then the change of basis matrix from \(\basis F^*\) to \(\basis E^*\) is
  \[
    (P^{-1})^T.
  \]
\end{lemma}

\begin{proof}
  \[
    [\id]_{\basis F^*,\basis E^*} = [\id]_{\basis E, \basis F}^T = ([\id]_{\basis F, \basis E}^{-1})^T.
  \]
\end{proof}

\begin{caution}
  \(V \cong V^*\) only if \(V\) is finite-dimensional. Let \(V = P\), the space of all real polynomials. It has basis \(\{p_j\}_{j\in \N}\) where \(p(j) = t^j\). In example sheet 2 we will see
\begin{align*}
  P^* &\cong \R^\N \\
  \varepsilon &\mapsto (\varepsilon(p_0), \varepsilon(p_1), \dots)
\end{align*}
and on example sheet 1 we prove
\[
  P \ncong \R^\N
\]
as the latter does not have a countable basis.
\end{caution}

Now we move on to more discussion about annhilator.

\begin{lemma}
  Let \(V\) and \(W\) be \(\F\)-vector spaces. Fix \(\alpha \in L(V,W)\) and let \(\alpha^* \in L(W^*, V^*)\) be its dual. Then
  \begin{itemize}
  \item \(N(\alpha^*) = (\im \alpha)^\ann\), so \(\alpha^*\) is injectve if and only if \(\alpha\) is surjective.
  \item \(\im \alpha^* \leq N(\alpha)^\ann\), with equality if \(V\) and \(W\) are both finite-dimensional, in which case \(\alpha^*\) is surjective if and only if \(\alpha\) is injective.
  \end{itemize}
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item Let \(\varepsilon \in W^*\), then
  \begin{align*}
    & \varepsilon \in N(\alpha^*) \\
    \Leftrightarrow & \alpha^* (\varepsilon) = 0 \\
    \Leftrightarrow & \varepsilon \compose \alpha = 0 \\
    \Leftrightarrow & \varepsilon(u) = 0 \, \forall u \in \im \alpha \\
    \Leftrightarrow & \varepsilon \in (\im \alpha)^\ann
  \end{align*}

\item Let \(\varepsilon \in \im \alpha^*\), Then \(\varepsilon = \alpha^* (\phi)\) for some \(\phi \in W^*\). For any \(u \in N(a)\),
  \[
    \varepsilon(u) = (\alpha^* \phi)(u) = (\phi\compose \alpha)(u) = \phi(\alpha(u)) = \phi(0) = 0
  \]
  so \(\varepsilon \in N(\alpha)^\ann\).

  Now use the fact that \(V\) and \(W\) are finite-dimensional:
  \[
    \dim \im(\alpha^*) = r(\alpha^*) = r(\alpha)
  \]
  as \(r(A) = r(A^T)\). On the other hand,
  \[
    r(\alpha) = \dim V - \dim N(\alpha) = \dim (N(\alpha))^\ann
  \]
  Thus they are equal.
\end{itemize}
\end{proof}

\subsection{Double Dual}

Let \(V\) be an \(\F\)-vector space. Then \(V^* = L(V,\F)\) is its dual space. The natrual (oops) next step is

\begin{definition}[Double dual]\index{vector space!double dual}
  The \emph{double dual} of \(V\) is
  \[
    V^{**} = V^* = L(V^*, \F).
  \]
\end{definition}

\begin{theorem}[Double dual as natural transformation]
  If \(V\) is an \(\F\)-vector space, then the map
  \begin{align*}
    \hat \cdot: V &\to V^{**} \\
    v &\mapsto \hat v
  \end{align*}
  where \(\hat v(\varepsilon) = \varepsilon(v)\), is an natural homomorphism. In particular when \(V\) is finite-dimensional this is a natural isomorphism.
\end{theorem}

\begin{proof}\leavevmode
  \begin{itemize}
  \item For \(v\in V\), the map \(\hat v: V^* \to \F\) is linear so \(\hat \cdot\) does give a map from \(V\) to \(V^{**}\).
  \item Linear: for \(v_1, v_2 \in V\), \(\lambda_1, \lambda_2 \in \F\), \(\varepsilon \in V^*\), then
    \[
      \reallywidehat{\lambda_1v_1 + \lambda_2v_2}(\varepsilon) = \varepsilon(\lambda_1v_1 + \lambda_2v_2) = \lambda_1 \varepsilon(v_1) + \lambda_2 \varepsilon(v_2) = \lambda_1 \hat v_1(\varepsilon) + \lambda_2 \hat v_2(\epsilon)
    \]
  \item Injectivity: let \(e \in V\setminus\{0\}\). Extend it to a basis of \(V\), say \(e, e_2,\dots, e_n\). Let \(\varepsilon, \varepsilon_2,\dots, \varepsilon_n\) be its corresponding dual basis. Now
    \[
      \hat e(\varepsilon) = \varepsilon(e) = 1 
    \]
    so \(\hat e\) is non-zero. \(\hat \cdot\) is injective.
  \item Finally, if \(V\) is finite-dimensional, \(\dim V = \dim V^* = \dim V^{**}\) so \(\hat \cdot\) is an isomorphism.
  \end{itemize}
\end{proof}

\begin{lemma}
  Let \(V\) be an \(\F\)-vector space and \(U \leq V\). Then
  \[
    \hat U \leq U^{\ann \ann}.
  \]
  If \(V\) is finite-dimensional then \(\hat U = U^{\ann \ann}\) so \(U \cong U^{\ann \ann}\).
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item First show \(\hat U \leq U^{\ann \ann}\): given \(u \in U\), for all \(\varepsilon \in U^\ann\), \(\varepsilon(u) = 0\) so \(\hat u(\varepsilon) = 0\). Thus \(\hat u \in (U^\ann)^\ann = U^{\ann \ann}\).
  \item If \(V\) is finite-dimensional then
    \[
      \dim U^{\ann \ann} = \dim V^* - \dim U^\ann = \dim V - \dim U^\ann = \dim U
    \] so \(\hat U = U^{\ann \ann}\).
  \end{itemize}
\end{proof}

\begin{lemma}[Galois Connection]
  Let \(V\) be a finite-dimensional \(\F\)-vector space and \(U_1, U_2 \leq V\). Then
  \begin{itemize}
  \item \((U_1 + U_2)^\ann = U_1^\ann \cap U_2^\ann\),
  \item \((U_1 \cap U_2)^\ann = U_1^\ann + U_2^\ann\).
  \end{itemize}
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item Let \(\theta \in V^*\), \(\theta \in (U_1+U_2)^\ann\) if and only if \(\theta(u_1+u_2) = 0\) for all \(u_1\in U_1, u_2 \in U_2\), if and only if \(\theta(u) = 0\) for all \(u \in U_1 \cup U_2\), so \(\theta\in U_2^\ann \cap U_2^\ann\).
  \item Apply \(^\ann\) to the first result and use the previous lemma.
  \end{itemize}
\end{proof}

\section{Bilinear Form I}

\begin{definition}[Bilinear form]\index{bilinear}
  Let \(U\) and \(V\) be \(\F\)-vector spaces. A map \(\varphi: U \times V \to \F\) is \emph{bilinear} if it is linear in both arguments, i.e.
  \begin{align*}
    \forall u \in U, \, \varphi(u, -) &\in V^* \\
    \forall v \in V, \, \varphi(-, v) &\in U^*
  \end{align*}
\end{definition}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item \(V \times V^* \to \F, (v, \theta) \mapsto \theta(v)\).
  \item \(U = V = \R^n, \varphi(x, y) = \sum_{i=1}^{n}x_iy_i\).
  \item \(A \in \M_{m,n}(\F), \varphi: \F^m \times \F^n \to \F, (u, v) \mapsto u^TAv\).
  \item \(U = V = C([0, 1], \R), (f, g) \mapsto \int_{0}^{1} f(t)g(t) dt \)
  \end{enumerate}
\end{eg}

\begin{definition}[Matrix of bilinear form]\index{matrix!bilinear form, of}
  Let \(\basis B = \{e_1,\dots,e_m\}\) be a basis for \(U\) and \(\basis C = \{f_1,\dots,f_n\}\) be a basis for \(V\). Given a bilinear map \(\varphi: U \times V \to \F\), the \emph{matrix of \(\varphi\)} with respect to \(\basis B\) and \(\basis C\) is
  \[
    [\varphi]_{\basis B, \basis C} = \left( \varphi(e_i, f_j) \right)_{m \times n}.
  \]
\end{definition}

\begin{lemma}
  \[
    \varphi(u, v) = [u]_{\basis B}^T [\varphi]_{\basis B, \basis C} [v]_{\basis C}.
  \]
\end{lemma}

\begin{proof}
  Let \(u = \sum_{i}^{ }\lambda_ie_i, v = \sum_{j}^{} \mu_jf_j\), then
  \begin{align*}
    \varphi(u, v) &= \varphi\left( \sum_{i}^{ }\lambda_ie_i, \sum_{j}^{} \mu_jf_j \right) \\
                  &= \sum_{i}^{ }\lambda_i \varphi\left(e_i, \sum_{j}^{} \mu_jf_j \right)\\
                  &= \sum_{i, j}^{ }\lambda_i \varphi(e_i, f_j) \mu_j
  \end{align*}
\end{proof}

\begin{note}\leavevmode
  \begin{enumerate}
  \item \([\varphi]_{\basis B, \basis C}\) is the unique matrix with this property.
  \item A bilinear form \(\varphi: U \times V \to \F\) induces linear maps
    \begin{align*}
      \varphi_L: U &\to V^* \\
      u &\mapsto \varphi(u, -) \\
      \varphi_R: V &\to U^* \\
      v &\mapsto \varphi(-, v)
    \end{align*}
  \end{enumerate}
\end{note}

\begin{lemma}
  Let \(\basis B = \{e_1,\dots, e_m\}\) be a basis for \(U\), \(\basis B^* = \{\varphi_1,\dots, \varphi_m\}\) a basis for \(U^*\), \(\basis C = \{f_1,\dots, f_n\}, \basis C^* = \{\eta_1,\dots, \eta_n\}\) for \(V\) and \(V^*\). If \([\varphi]_{\basis B, \basis C} = A\) then
  \begin{align*}
    [\varphi_L]_{\basis B, \basis C^*} &= A^T, \\
    [\varphi_R]_{\basis C, \basis B^*} &= A. \\
  \end{align*}
\end{lemma}

\begin{proof}
  \begin{align*}
    \varphi_L(e_i)(f_j) = A_{ij} &\Longrightarrow \varphi_L(e_i) = \sum_{j}^{ }A_{ij}\eta_j \\
    \varphi_R(f_j)(e_i) = A_{ij} &\Longrightarrow \varphi_R(f_j) = \sum_{i}^{ }A_{ij}\varepsilon_j
  \end{align*}
\end{proof}

\begin{definition}[Left and right kernel]\index{bilinear!kernel}
  The \emph{left (right, respectively) kernel} of \(\varphi\) is \(\ker \varphi_L\) (\(\ker \varphi_R\), respectively).
\end{definition}

\begin{definition}[Degeneracy]\index{bilinear!dengeneracy}
  \(\varphi\) is \emph{non-degenerate} if \(\ker \varphi_L = 0\) and \(\ker \varphi_R = 0\). Otherwise, \(\varphi\) is \emph{degenerate}.
\end{definition}

\begin{lemma}
  Let \(U, V\) have bases as before, and \(\varphi, A\) as before. Then \(\varphi\) is non-degenerate if and only if \(A\) is invertible.
\end{lemma}

\begin{proof}
  \begin{align*}
    & \varphi \text{ is non-degenerate} \\
    \Leftrightarrow & \ker \varphi_L = 0 \text{ and } \ker \varphi_R = 0 \\
    \Leftrightarrow & n(A^T) = n(A) = 0 \\
    \Leftrightarrow & r(A^T) = \dim V, r(A) = \dim U \\
    \Leftrightarrow & A \text{ is invertible}
  \end{align*}
\end{proof}

\begin{corollary}
  If \(\varphi\) is non-degenerate then \(U\) and \(V\) are finite-dimensional and \(\dim U = \dim V\).
\end{corollary}

\begin{corollary}
  When \(U\) and \(V\) are finite-dimensional, choosing a non-degenerate bilinear form \(\varphi: U \times V \to \F\) is equivalent to picking an homomorphism \(\varphi_L: U \to V^*\).
\end{corollary}

\begin{definition}
  For \(T \subseteq U, S \in V\),
  \begin{align*}
    T^\perp &= \{ v\in V: \varphi(t, v) = 0 \, \forall t\in T \} \leq V \\
    ^\perp S &= \{ u\in U: \varphi(u, s) = 0 \, \forall s\in S \} \leq U
  \end{align*}
\end{definition}
They are generalisation of annihilators.

\begin{proposition}
  \label{prop:change of basis of bilinear form}
  Suppose \(U\) have basese \(\basis B, \basis B'\) and \(V \) have bases \(\basis C, \basis C'\), \(P = [\id]_{\basis B', \basis B}, Q = [\id]_{\basis C',\basis C}\). Let \(\varphi: U \times V \to \F\) be a bilinear form. Then
  \[
    [\varphi]_{\basis B',\basis C'} = P^T[\varphi]_{\basis B,\basis C}Q.
  \]
\end{proposition}

\begin{proof}
  \begin{align*}
    \varphi(u, v) &= [u]_{\basis B}^T [\varphi]_{\basis B, \basis C} [v]_{\basis C} \\
                  &= (P[u]_{\basis B'})^T [\varphi]_{\basis B, \basis C} (Q[v]_{\basis C'}) \\
                  &= [u]_{\basis B'}^T P^T[\varphi]_{\basis B, \basis C}Q[v]_{\basis C'} 
  \end{align*}
\end{proof}

\begin{definition}[Rank of bilinear form]\index{rank}
  The \emph{rank} of \(\varphi\), \(r(\varphi)\), is the rank of its matrix representation (which is well-defined by the previous proposition).
\end{definition}

\begin{note}
  \[
    r(\varphi) = r(\varphi_L) = r(\varphi_R).
  \]
\end{note}

\section{Determinant \& Trace}

% TODO: add exposition about eigenvalues and basis-independent values.

\subsection{Trace}

\begin{definition}[Trace]\index{trace}
  For \(A \in \M_n(\F) = \M_{n,n}(\F)\), the \emph{trace} of \(A\) is
  \[
    \tr(A) = \sum_{i=1}^{n}A_{ii}.
  \]
\end{definition}

\begin{lemma}
  For \(A, B\in \M_n(\F)\),
  \[
    \tr(AB) = \tr(BA).
  \]
\end{lemma}

\begin{proof}
  \[
    \tr(AB) = \sum_{i}^{ }\sum_{j}^{ }a_{ij}b_{ji} = \sum_{j}^{ }\sum_{i}^{ }b_{ji}a_{ij} = \tr(BA).
  \]
\end{proof}

\begin{lemma}
  Similar (or conjugate) matrices have the same trace.
\end{lemma}

\begin{proof}
  Suppose \(A\) and \(B\) are conjugates, they there exists \(P\) such that \(B = P^{-1}AP\) so
  \[
    \tr(B) = \tr(P^{-1}AP) = \tr(APP^{-1}) = \tr(A).
  \]
\end{proof}

\begin{definition}[Trace]\index{trace}
  Let \(\alpha: V \to V\) be a linear map. The \emph{trace} of \(\alpha\) is
  \[
    \tr \alpha = \tr[\alpha]_{\basis B} = \tr [\alpha]_{\basis B, \basis B}
  \]
  with repsect to a basis \(\basis B\). This is well-defined by the previous lemma.
\end{definition}

\begin{lemma}
  Let \(\alpha: V \to V\) be linear and \(\alpha^*: V^* \to V^*\) be its dual. Then
  \[
    \tr \alpha = \tr \alpha^*.
  \]
\end{lemma}

\begin{proof}
  \[
    \tr \alpha = \tr [\alpha]_{\basis B} = \tr [\alpha]_{\basis B}^T = \tr[\alpha^*]_{\basis B^*} = \tr \alpha^*.
  \]
\end{proof}

\subsection{Determinant}

Recall some results from IA Groups: let \(S_n\) be the permutation group of the set \(\{1, 2, \dots, n\}\) and \(\varepsilon: S_n \to \{1, -1\}\) be the signature of a permutation, i.e.
\[
  \varepsilon(\sigma) =
  \begin{cases}
    1 &\text{if \(\sigma\) is a product of even number of transpotitions}\\
    0 &\text{otherwise}
  \end{cases}
\]

\begin{definition}[Determinant]\index{determinant}
  Suppose \(A \in \M_n(\F)\), \(A = (a_{ij})\), the \emph{determinant} of \(A\) is
  \[
    \det A = \sum_{\sigma \in S_n}^{ } \varepsilon(\sigma) a_{\sigma(1), 1} a_{\sigma(2), 2} \cdots a_{\sigma(n), n}.
  \]
\end{definition}

There are \(n!\) terms in this summation and each is the signed product of \(n\) elements (one from each row and each column).

\begin{eg}
  For \(n = 2\),
  \[
    \det
    \begin{pmatrix}
      a_{11} & a_{12} \\
      a_{21} & a_{22}
    \end{pmatrix}
    =a_{11}a_{22} - a_{21}a_{12}
  \]
\end{eg}

\begin{lemma}
  If \(A = (a_{ij})\) is an upper-triangular matrix (i.e.\ \(a_{ij} = 0\) for all \(i > j\)) then
  \[
    \det A = a_{11}a_{22}\dots a_{nn}.
  \]
  Similar for lower-trianglular matrices.
\end{lemma}

\begin{proof}
  In the summation
  \[
    \det A = \sum_{\sigma \in S_n}^{ }\varepsilon(\sigma) a_{\sigma(1), 1}\dots a_{\sigma(n),n},
  \]
  for a summand to be non-zero, we need \(\sigma(j) \leq j\) for all \(j\). Thus \(\sigma = \id\).
\end{proof}

\begin{lemma}
  \[
    \det A = \det A^T
  \]
\end{lemma}

\begin{proof}
  \begin{align*}
    \det A &= \sum_{\sigma \in S_n}^{ } \varepsilon(\sigma) \prod_{i = 1}^n a_{\sigma(i), i} \\
           &= \sum_{\sigma \in S_n}^{ } \varepsilon(\sigma) \prod_{i = 1}^n a_{i, \sigma^{-1}(i)} \\
           &= \sum_{\sigma \in S_n}^{ } \varepsilon(\sigma^{-1}) \prod_{i = 1}^n a_{i, \sigma^{-1}(i)} \\
           &= \sum_{\tau \in S_n}^{ } \varepsilon(\tau) \prod_{i = 1}^n a_{i, \tau(i)} \text{ where \(\tau = \sigma^{-1}\)} \\
           &= \det A^T
  \end{align*}
\end{proof}

\begin{definition}[Volume form]\index{volume form}
  A \emph{volume form} on \(\F^n\) is a function
  \[
    d: \underbrace{\F^n \times \F^n \times \dots \times \F^n}_{\text{\(n\) copies}} \to \F
  \]
  which is:
  \begin{itemize}
  \item multilinear: for any \(i\) and \(v_1, \dots, v_{i-1}, v_i, v_{i + 1}, \dots, v_n \in \F^n\),
    \[
      d(v_1, \dots, v_{i-1}, -, v_{i+1}, \dots, v_n) \in (\F^n)^*.
    \]
  \item alternating: if \(v_i = v_j\) for \(i \neq j\), \(d(v_1, \dots, v_n) = 0\).
  \end{itemize}
\end{definition}

\begin{notation}
  Given \(A = (a_{ij})\), write \(A\) in column form
  \[
    \left( A^{(1)} | \cdots | A^{(n)} \right).
  \]
\end{notation}
For example, if \(\{e_i\}\) is a standard basis for \(\F^n\) then
\[
  I = \left(e_1 | \cdots | e_n \right).
\]

\begin{lemma}
  \begin{align*}
    \det: \F^n \times \dots \times \F^n &\to \F \\
    (A^{(1)}, \dots, A^{(n)}) &\mapsto \det A
  \end{align*}
  is a volume form.
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item Multilinear: for any fixed \(\sigma \in S_n\), \(\prod_{i = 1}^n a_{\sigma(i), i}\) contains exactly one term from each column so it is multilinear. Multilinearity is preserved under addition.
  \item Alternating: suppose \(A^{(k)} = A^{(l)}\) for some \(l \neq k\). Let \(\tau = (kl)\). Then \(a_{ij} = a_{i \tau(j)}\) for all \(i, j\). Also \(S_n\) can be expressed as a union of two disjoint cosets \(A_n\) and \(\tau A_n\) so
    \begin{align*}
      \det A &= \sum_{\sigma \in A_n}^{ } \varepsilon(\sigma) \prod_{i = 1}^n a_{i, \sigma(i)} - \sum_{\sigma \in A_n}^{ } \varepsilon(\sigma) \prod_{i = 1}^n a_{i, \tau\sigma(i)} \\
      \intertext{since \(\varepsilon\) is a homomorphism}
             &= \sum_{\sigma \in A_n}^{ } \varepsilon(\sigma) \prod_{i = 1}^n a_{i, \sigma(i)} - \sum_{\sigma \in A_n}^{ } \varepsilon(\sigma) \prod_{i = 1}^n a_{i, \sigma(i)} \\
             &= 0
    \end{align*}
  \end{itemize}
\end{proof}

In the rest of the section we are going to prove that the converse is also true, i.e.\ all volume forms are determinants up to a scaling constant.

\begin{lemma}
  Let \(d\) be a volume form. Then swapping two entries changes the sign:
  \[
    d(v_1, \dots, v_i, \dots, v_j, \dots, v_n) = - d(v_1, \dots, v_j, \dots, v_i, \dots, v_n).
  \]
\end{lemma}

\begin{proof}
  \begin{align*}
    0 &= d(v_1, \dots, v_{i-1}, v_i + v_j, v_{i+1}, \dots, v_{j-1}, v_i + v_j, v_{j + 1}, \dots, v_n) \\
      &= \underbrace{(v_1, \dots, v_i, \dots, v_i, \dots, v_n)}_{ = 0} + d(v_1, \dots, v_j, \dots, v_i, \dots, v_n) \\
      &+ d(v_1, \dots, v_i, \dots, v_j, \dots, v_n) + \underbrace{d(v_1, \dots, v_j, \dots, v_j, \dots, v_n)}_{ = 0} \\
  \end{align*}
  Rearrange.
\end{proof}

\begin{corollary}
  If \(\sigma \in S_n\),
  \[
    d(v_{\sigma(1)}, \dots, v_{\sigma(n)}) = \varepsilon(\sigma) d(v_1, \dots, v_n).
  \]
\end{corollary}

\begin{theorem}
  Let \(d\) be a volume form on \(\F^n\), \(A = (A^{(1)}|\dots | A^{(n)})\), then
  \[
    d(A^{(1)}, \dots, A^{(n)}) = \det A \cdot d(e_1, \dots, e_n).
  \]
\end{theorem}

\begin{proof}
  \begin{align*}
    d(A^{(1)}, \dots, A^{(n)}) &= d \left( \sum_{i=1}^{n} a_{i1}e_i, A^{(2)}, \dots, A^{(n)} \right) \\
                               &= \sum_{i = 1}^{n}a_{i1} d(e_i, A^{(2)}, \dots, A^{(n)}) \\
                               &= \sum_{i}^{} \sum_{j}^{ } a_{i1} a_{j2} d(e_i, e_j, \dots, A^{(n)}) \\
                               &= \sum_{i_1, i_2, \dots, i_n}^{ } \prod_{k = 1}^{n} a_{{i_k},k} d(e_{i_1}, \dots e_{i_n})
\end{align*}
The last term is \(0\) unless all of \(i_k\) are distinct, i.e.\ exists \(\sigma \in S_N\) such that \(i_k = \sigma(k)\). Thus
\begin{align*}
  d(A^{(1)}, \dots, A^{(n)}) = \sum_{\sigma \in S_n}^{ } \prod_{k = 1}^{n} a_{\sigma(k), k} \underbrace{d(e_{\sigma(1)}, \dots, e_{\sigma(n)})}_{= \varepsilon(\sigma) d(e_1, \dots, e_n)}
\end{align*}
\end{proof}

\begin{corollary}
  \(\det\) is the unique volume form \(d\) such that \(d(e_1, \dots, e_n) = 1\).
\end{corollary}

\begin{proposition}
  Suppose \(A, B \in \M_n(\F)\), then
  \[
    \det AB =\det A \det B.
  \]
\end{proposition}

\begin{proof}
  Let \(d_A: \F^n \times \dots \F^n \to \F, (v_1, \dots, v_n) \to \det(Av_1|\cdots|Av_n)\), then \(d_A\) is a volume form:
  \begin{itemize}
  \item Multilinear: \(v_i \mapsto A_{v_i}\) is linear and \(\det\) is multilinear.
  \item Alternating: \(v_i = v_j\) implies \(A_{v_i} = A_{v_j}\) and \(\det\) is alternating.
  \end{itemize}
  It follows that
  \begin{align*}
    d_A(Be_1, \dots, Be_n) &= \det B \cdot d_A(e_1, \dots, e_n) = \det B \det A \\
                           &= \det (ABe_1| \cdots | ABe_n) = \det AB
  \end{align*}
\end{proof}

\begin{definition}[Singular]\index{matrix!singular}
  \(A \in \M_n(\F)\) is \emph{singular} if \(\det A = 0\). Otherwise it is \emph{non-singular}.
\end{definition}

\begin{lemma}
  If \(A\) is invertible then it is non-singular and
  \[
    \det A^{-1} = \frac{1}{\det A}.
  \]
\end{lemma}

\begin{proof}
  \[
    1 = \det I_n = \det(AA^{-1}) = \det A \det A^{-1}
  \]
\end{proof}

\begin{theorem}
  Suppose \(A \in \M_n(\F)\) then TFAE:
  \begin{enumerate}
  \item \(A\) is invertible,
  \item \(A\) is non-singular,
  \item \(r(A) = n\).
  \end{enumerate}
\end{theorem}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(1 \Rightarrow 2\): done.
  \item \(2 \Rightarrow 3\): suppose that \(r(A) < n\). By rank-nullity \(n(A) > 0\) so \(\exists \lambda \in \F^n \setminus \{0\}\) such that \(A \lambda = 0\). Say \(\lambda = (\lambda_i)\) and \(\lambda_k \neq 0\). Have \(\sum_{i = 1}^{n}A^{(i)}\lambda_i = 0 \). Let
    \[
      B= (e_1|e_2|\cdots|e_{k-1}|\lambda|e_{k+1}|\cdots|e_n)
    \]
    It follows that \(AB\) has \(k\)th column zero so
    \[
      0 = \det AB = \det A \det B = \lambda_k \det A.
    \]
    So \(\det A = 0\).
  \item \(3 \Rightarrow 1\): by rank-nullity.
  \end{itemize}
\end{proof}

\subsection{Determinant of Linear Maps}

\begin{lemma}
  Conjugate matrices have the same determinant.
\end{lemma}

\begin{proof}
  Let \(B = P^{-1}AP\). Then
  \[
    \det B = \det (P^{-1}AP) = \det P^{-1} \det A \det P = \det (P^{-1}P) \det A = \det A.
  \]
\end{proof}

\begin{definition}[Determinant]\index{determinant}
  Let \(\alpha: V \to V\) where \(V\) is a finite-dimensional vector space. The \emph{determinant} of \(\alpha\) is
  \[
    \det \alpha = \det [\alpha]_{\basis B, \basis B}
  \]
  where \(\basis B\) is any basis for \(V\). 
\end{definition}
This is well-defined by the previous lemma.

\begin{theorem}
  \(\det: L(V, V) \to \F\) satisfies
  \begin{enumerate}
  \item \(\det \id = 1\),
  \item \(\det \alpha \compose \beta = \det \alpha \det \beta\),
  \item \(\det \alpha \neq 0\) if and only if \(\alpha\) is invertible and in this case \(\det (\alpha^{-1}) = \frac{1}{\det \alpha}\).
  \end{enumerate}
\end{theorem}

\begin{proof}
  Restatement of previous results.
\end{proof}

\subsection{Determinant of Block-triangular Matrices}

\begin{lemma}
  Suppose \(A \in \M_k(\F), B \in \M_\ell(\F)\) and \(C \in \M_{k, \ell}(\F)\), then
  \[
    \det
    \begin{pmatrix}
      A & C \\
      0 & B
    \end{pmatrix}
    = \det A \det B.
  \]
\end{lemma}

\begin{proof}
  Let \(n = k + \ell\) and call the block matrix \(X = (x_{ij})\), which is an element of \(\M_n(\F)\). Then
  \begin{align*}
    \det X &= \sum_{\sigma \in S_n}^{ }\varepsilon(\sigma) \prod_{i = 1}^{n} x_{\sigma(i), i} \\
    \intertext{Note that \(x_{\sigma(i), i} = 0\) if \(i \leq k\) and \(\sigma(i) > k\). Thus we are only concerned about \(\sigma\) under which these are in different orbits, i.e.\ \(\sigma = \sigma_1\sigma_2\) where \(\sigma_1 \in \sym_{\{1,\dots, k\}}\) and \(\sigma_2 \in \sym_{\{k+1, \dots, n\}}\).}
           &= \sum_{\sigma_1 \in \sym_{\{1, \dots, k\}}}^{ } \varepsilon(\sigma_1) \prod_{j = 1}^{k} a_{\sigma_1(j),j} \\
           &\times \sum_{\sigma_2 \in \sym_{\{k+1, \dots, n\}}}^{ } \varepsilon(\sigma_1) \prod_{j = k + 1}^{n} a_{\sigma_2(j),j} \\
           &= \det A \det B
  \end{align*}
\end{proof}

\begin{corollary}
  For a sequence of matrices \(A_1, \dots, A_k\),
  \[
    \det
    \begin{pmatrix}
      A_1 & & & & \\
      & A_2 & & * & \\
      & & A_3 & & \\
      & 0 & & \ddots & \\
      & & & & A_k
    \end{pmatrix}
    = \prod_{i = 1}^{k} \det A_i
  \]
\end{corollary}

\begin{proof}
  Apply the previous lemma inductively.
\end{proof}

\begin{caution}
  In general,
  \[
    \det
    \begin{pmatrix}
      A & B \\
      C & D
    \end{pmatrix}
    \neq \det A \det D - \det B \det C.
  \]
\end{caution}

\subsection{Volume Interpretation of Determinant}

In \(\R^2\), the determinant of a matrix
\[
  \det
  \begin{pmatrix}
    a_{11} & a_{12} \\
    a_{21} & a_{22} 
  \end{pmatrix}
\]
can be intepreted as the signed area of the parallelogram spanned by the column vectors \(\binom{a_{11}}{a_{21}}\) and \(\binom{a_{12}}{a_{22}}\) of the matrix.
    
Similarly in \(\R^3\) the determinant of a matrix is the signed volume of the parallelepiped spanned by the column vectors of the matrix.

For higher dimensions, although it can be difficult to visualise, the same interpretation still works: consier a hypercube \(H = [0, 1]^n \subseteq \R^n\). Then a map \(A \in \M_n(\F)\) sends
\begin{align*}
  H &\to A(H) \\
  \sum_{i = 1}^{n} t_ie_i &\mapsto \sum_{i = 1}^{n}t_i A^{(i)}
\end{align*}
and the generalised signed volume of RHS is \(\det A\).

\subsection{Determinant of Elementary Operation}

Consider the determinants of elementary column operation matrices:
\begin{itemize}
\item \(E_1\) swaps two columns so \(\det E_1 = -1\),
\item \(E_2\) multiplies a column by \(\lambda \neq 0\) so \(\det E_2 = \lambda\),
\item \(E_3\) adds \(\lambda\) times of a column to another column so \(\det E_3 = 1\).
\end{itemize}

One could prove properties of \(\det\) by decomposing any matrix into elementary matrices.

\subsection{Column Expansion \& Adjugate Matrices}

\begin{lemma}
  Suppose \(A \in \M_n(\F)\), \(A = (a_{ij})\). Define \(A_{\widehat{ij}} \in \M_{n - 1}(\F)\) by deleting row \(i\) and column \(j\) from \(A\). Then \(\det A\) can be calculated by
  \begin{enumerate}
  \item expansion in column \(j\): for a fixed \(j\),
    \[
      \det A = \sum_{i = 1}^{n} (-1)^{i + j} a_{ij} \det A_{\widehat{ij}}.
    \]
  \item expansion in row \(i\): for a fixed \(i\),
    \[
      \det A = \sum_{j = 1}^{n} (-1)^{i + j} a_{ij} \det A_{\widehat{ij}}.
    \]
  \end{enumerate}
\end{lemma}

\begin{remark}
  It is possible to use one of the expressions above to define determinant iteratively, with base case \(\det a = a\) for \(n = 1\).
\end{remark}

\begin{eg}
  \[
    \begin{vmatrix}
      a & b & c \\
      d & e & f \\
      g & h & i
    \end{vmatrix}
    =
    a
    \begin{vmatrix}
      e & f \\
      h & i
    \end{vmatrix}
    -b
    \begin{vmatrix}
      d & f \\
      g & i
    \end{vmatrix}
    +c
    \begin{vmatrix}
      d & e \\
      g & h
    \end{vmatrix}
  \]
\end{eg}

\begin{proof}
  We prove \(1\):
  
  \begin{align*}
    \det A &= \det \left( A^{(1)} | \cdots | \sum_{i = 1}^{n} a_{ij}e_i | \cdots | A^{(n)} \right) \\
           &= \sum_{i = 1}^{n} a_{ij} \det( A^{(1)} | \cdots | e_i | \cdots | A^{(n)} ) \\
    \intertext{use row and column operations to move the entry to top left corner,}
           &= \sum_{i = 1}^{n} a_{ij} (-1)^{(i - 1) + (j - 1)} \det
             \begin{pmatrix}
               1 & 0 \\
               0 & A_{\widehat{ij}}
             \end{pmatrix} \\
           &= \sum_{i = 1}^{n} a_{ij} (-1)^{i + j} \det A_{\widehat{ij}}
  \end{align*}
\end{proof}

\begin{definition}[Adjugate]\index{matrix!adjugate}
  Let \(A \in \M_n(\F)\). The \emph{adjugate matrix} of \(A\), \(\adj A\), is the \(n \times n\) matrix
  \[
    (\adj A)_{ij} = (-1)^{i + j} \det A_{\widehat{ji}}.
  \]
\end{definition}

Notice the transposition of indices.

\begin{theorem}\leavevmode
  \begin{enumerate}
  \item \((\adj A ) A = \det A \cdot I\),
  \item If \(A\) is invertible then
    \[
      A^{-1} = \frac{\adj A}{\det A}.
    \]
  \end{enumerate}
\end{theorem}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item For a fixed \(j\), \(\det A = \sum_i (\adj A)_{ji} a_{ij} = (\adj A \cdot A)_{jj}\). For \(j \neq k\), replace the \(j\)th column with the \(k\)th:
    \begin{align*}
      0 &= \det(A^{(1)} | \cdots | A^{(k)} | \cdots | A^{(k)} | \cdots | A^{(n)}) \\
        &= \sum_{i}^{ } (\adj A)_{ji} a_{ik} \\
        &= (\adj A \cdot A)_{jk}
    \end{align*}
  \item If \(A\) is invertible then \(\det A \neq 0\) so
    \[
      I = \frac{\adj A}{\det A} A.
    \]
  \end{enumerate}
\end{proof}

\subsection{Application: System of Linear Equations}

A system of linear equations can be written as
\[
  A \V x = \V b
\]
where \(A \in \M_{m, n}(\F)\) and \(\V b \in \M_{m, 1}(\F)\) are known and \(\V x \in \M_{n, 1}(\F)\) is unknown.

The system has a solution if and only if \(r(A) = r(A|\V b)\) where the matrix on RHS is the \emph{augmented matrix} by adding \(\V b\) as a column to \(A\) since this happens if and only if \(\V b\) is a linear combination of columns of \(A\).

The solution is unique if and only if \(r(A) = n\).

In particular, if \(m = n\), if \(A\) is non-singular then there is a unique solution
\[
  \V x = A^{-1} \V b.
\]

Although in theory we could invert the matrix to solve the system of equations, it is terribly inefficient. Instead, we use

\begin{proposition}[Cramer's rule]
  If \(A \in \M_n(\F)\) is invertible then the system
  \[
    A \V x = \V b
  \]
  has unique solution \(\V x = (x_i)\) where
  \[
    x_i = \frac{\det (A_{\hat{i} \V b})}{\det A}
  \]
  where \(A_{\hat{i} \V b}\) is obtained from \(A\) by deleting \(i\)th column and replacing it with \(\V b\).
\end{proposition}

\begin{proof}
  Assume \(\V x\) is a solution of the system.
  \begin{align*}
    \det (A_{\hat i \V b}) &= \det(A^{(1)} | \cdots | \V b | \cdots | A^{(n)}) \\
                           &= \det(A^{(1)} | \cdots | A \V x | \cdots | A^{(n)}) \\
                           &= \sum_{j = 1}^{n} x_j \det(A^{(1)} | \cdots | A^{(j)} | \cdots | A^{(n)}) \\
    \intertext{\(A^{(j)}\) is one of the other columns unless \(j = i\) so}
                           &= x_i \det A
  \end{align*}
\end{proof}

\begin{corollary}
  If \(A \in \M_n(\Z)\) with \(\det A = \pm 1\), then
  \begin{enumerate}
  \item \(A^{-1} \in \M_n(\Z)\).
  \item Given \(\V b \in \Z^n\), \(A \V x = \V b\) has an integer solution.
  \end{enumerate}
\end{corollary}

\section{Endomorphism}

Let \(V\) be an \(\F\)-vector space with \(\dim V = n < \infty\). Let \(\basis B =\{v_1, \dots, v_n\}\) be a basis and \(\alpha \in L(V) = L(V, V)\). The general problem studied in this section is to choose a basis \(\basis B\) such that \([\alpha]_{\basis B}\) has ``nice forms'', for example, amenable to \(\det\) and \(\tr\).

Suppose there is another basis \(\basis B'\) with change-of-basis matrix \(P\). Recall that
\[
  [\alpha]_{\basis B} = P^{-1}[\alpha]_{\basis B'}P.
\]
The above problem is thus euqivalent to: given \(A \in \M_n(\F)\), find \(A'\) conjugate to \(A\) and in a ``nice form''.

What are the nice forms that we desire? The best we can have is

\begin{definition}[Diagonalisable]\index{diagonalisable}
  \(\alpha \in L(V)\) is \emph{diagonalisable} if there exists \(\basis B\) such that \([\alpha]_{\basis B}\) is diagonal.
\end{definition}

A slightly weaker, albeit still ``nice'' enough form is

\begin{definition}[Triangulable]\index{triangulable}
  \(\alpha \in L(V)\) is \emph{triangulable} if there exists \(\basis B\) such that \([\alpha]_{\basis B}\) is upper triangular.
\end{definition}

Equivalent, rephrasing using lanugages of matrices, \(A \in \M_n(\F)\) is diagonalisable (triangulable, respectively) if it is conjugate to a diagonal (upper triangle, respectively) matrix.

\begin{definition}[Eigenvalue, eigenvector, eigenspace]\index{eigenvalue}\leavevmode
  \begin{enumerate}
  \item \(\lambda \in \F\) is an \emph{eigenvalue} of \(\alpha\) if there exists some \(v \in V\setminus\{0\}\) such that \(\alpha(v) = \lambda v\).
  \item \(v \in V\) is an \emph{eigenvector} of \(\alpha\) if \(\alpha(v) = \lambda v\) for some eigenvalue \(\lambda\).
  \item \(V_\lambda = \{v \in V: \alpha(v) = \lambda v\}\) is the \emph{\(\lambda\)-eigenspace} of \(\alpha\).
  \end{enumerate}
\end{definition}

\begin{remark}
  It is easy to check that \(V_\lambda \leq V\).
\end{remark}

\begin{remark}\leavevmode
  \begin{enumerate}
  \item \(V_\lambda = \ker(\alpha - \lambda \iota)\) and
    \begin{align*}
      & \lambda \text{ is an eigenvalue} \\
      \Leftrightarrow & \alpha - \lambda \iota \text{ is singular} \\
      \Leftrightarrow & \det (\alpha - \lambda \iota) = 0
    \end{align*}
  \item If \(\alpha(v_j) = \lambda v_j\) then the \(j\)th column of \([\alpha]_{\basis B}\) is \((0, \dots, \lambda, \dots, 0)^T\).
  \item \([\alpha]_{\basis B}\) is diagonal if and only if \(\basis B\) consists of eigenvectors. \([\alpha]_{\basis B}\) is upper triangular if and only if \(\alpha(v_j) \in \spans{v_1, \dots, v_j}\) for all \(j\). In particular, \(v_1\) is an eigenvector.
  \end{enumerate}
\end{remark}

\subsection{Polynomial Ring, an Aside}

Before discussing polynomials associated with a linear map, we need some knowledge about the ambient polynomial space that we will be working with. The following results should be self-evident and proofs are omitted. Most of them will be studied in detail in IB Groups, Rings and Modules and a proof the Fundamental Theorem of Algebra can be found in IB Complex Analysis.

Let \(\F[t] = \{\text{polynomials with coefficients in } \F\}\) and \(\deg f\) be the degree of \(f\) in \(\F[t]\). By convention \(\deg 0 = -\infty\). We have the following properties:
\begin{enumerate}
\item \(\deg (f + g) \leq \max(\deg f, \deg g), \deg(f g) = \deg f + \deg g\).
\item If \(\lambda \in \F\) is a root of some \(f \in \F[t]\), i.e.\ \(f(\lambda) = 0\) then \((t - \lambda) \divides f\). In other words, \(f(t) = (t - \lambda) g(t)\) for some \(g(t) \in \F[t]\) and \(\deg g = \deg f - 1\).
\item We say \(\lambda\) is a root of \(f \in \F[t]\) with \emph{multiplicity} \(e \in \N\) if \((t - \lambda)^e \divides f\) but \((t - \lambda)^{e + 1} \ndivides f\).
\item A polynomial of degree \(n\) has at most \(n\) roots, counted with multiplicity.
\item Fundamental Theorem of Algebra: any \(f \in \C[t]\) of positive degree has a root (hence \(\deg f\) roots).
\end{enumerate}

\subsection{Characteristic Polynomial of Endormorphism}

\begin{definition}[Characteristic polynomial]\index{polynomial!characteristic}
   The \emph{characteristic polynomial} of \(\alpha \in L(V)\) is
  \[
    \chi_\alpha(t) = \det (\alpha - t \iota).
  \]
  The \emph{characteristic polynomial} of \(A \in \M_n(\F)\) is
  \[
    \chi_A(t) = \det (A - t I).
  \]
\end{definition}

Conjugate matrices have the same characteristic polynomial.

\begin{theorem}
  A linear map \(\alpha\) is triangulable if and only if \(\chi_\alpha(t)\) can be written as a product of linear factors over \(\F\).
\end{theorem}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(\Rightarrow\): suppose \(\alpha\) is triangulable and is represented by
    \[
      \begin{pmatrix}
        a_1 & \cdots & * \\
        & \ddots & \vdots \\
        0 & & a_n
      \end{pmatrix}
    \]
    with respect to some basis. Then
    \[
      \chi_\alpha(t) = \det
      \begin{pmatrix}
        a_1 - t & \cdots & * \\
        & \ddots & \vdots \\
        0 & & a_n -t
      \end{pmatrix}
      = \prod_{i = 1}^{n} (a_i - t)
    \]
  \item \(\Leftarrow\): induction of \(n = \dim V\): if \(n = 1\) then done. Suppose \(n > 1\) and the theorem holds for all endomorphisms of spaces of smaller dimensions. By hypothesis \(\chi_\alpha(t)\) has a root in \(\F\), say \(\lambda\). Let \(U = V_\lambda \neq 0\), then \(\alpha(U) \leq U\) so \(\alpha\) induces \(\overline \alpha: V/U \to V/U\). Pick basis \(v_1, \dots, v_k\) for \(U\) and extend it to a basis \(\basis B = \{v_1, \dots, v_n\}\) for \(V\). With respect to \(\basis B\), \(\alpha\) has representation
    \[
      \begin{pmatrix}
        \lambda I_k & * \\
        0 & C
      \end{pmatrix}
    \]
    so
    \[
      \chi_\alpha(t) = \det(\alpha - t \iota) = (\lambda - t)^k \chi_{\overline \alpha}(t).
    \]
    Thus \(\chi_{\overline \alpha}(t)\) is also a product of linear factors. Since \(\chi_{\overline \alpha}(t)\) acts on a linear space of strictly smaller dimension, by induction hypothesis there is a basis \(w_{k + 1} + U, \dots, w_n + U\) for \(V/U\) with respect to which \(\overline \alpha\) has an upper-triangular matrix representation, say T. Then with respect to basis \(v_1, \dots, v_k, w_{k + 1}, \dots, w_n\), \(\alpha\) has matrix representation
    \[
      \begin{pmatrix}
        \lambda I_k & * \\
        0 & T
      \end{pmatrix}
    \]
  \end{itemize}
\end{proof}

\begin{eg}
  Let \(\F = \R\), \(V = \R^2\) and \(\alpha\) be a rotation. Then with respect to the standard basis \(\alpha\) has representation
  \[
    \begin{pmatrix}
      \cos \theta & \sin \theta \\
      -\sin \theta & \cos \theta
    \end{pmatrix}
  \]
  and thus \(\chi_\alpha(t) = t^2 - 2 \cos \theta t + 1\), which is irreducible in general. Thus \(\alpha\) is not triangulable over \(\R\).
\end{eg}

\begin{lemma}
  Let \(V\) be an \(n\)-dimensional \(\F\)-vector space and \(\alpha \in L(V)\) with \(\chi_\alpha(t) = (-1)^n t^n + c_{n - 1} t^{n - 1} + \dots c_0\). Then
  \begin{itemize}
  \item \(c_0 = \det \alpha\),
  \item \(c_{n - 1} = (-1)^{n - 1} \tr \alpha\) for \(\F = \R\) or \(\C\).
  \end{itemize}
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(c_0 = \chi_\alpha(0) = \det (\alpha - 0) = \det \alpha\).
  \item If \(\F = \R\) then there is an extension of scalars \(\M_n(\R) \embed \M_n(\C)\) induced by \(\R \embed \C\) (i.e.\ complexification). For \(\F = \C\),
    \[
      \chi_\alpha(t) = \det
      \begin{pmatrix}
        a_0 - t & \cdots & * \\
        & \ddots & \vdots \\
        0 & & a_n - t
      \end{pmatrix}
      = \prod_{i = 1}^{n} (a_i - t)
    \]
    where \(\sum_{i = 1}^n a_i = \tr \alpha\).
  \end{itemize}
\end{proof}

\begin{notation}
  Let \(p(t)\) be a polynomial over \(\F\),
  \[
    p(t) = a_nt^n + \dots + a_0 \in \F[t].
  \]
  For \(A \in \M_n(\F)\), define
  \[
    p(A) = a_nA^n + \dots + a_0I \in \M_n(\F).
  \]
  For \(\alpha \in L(V)\), define
  \[
    p(\alpha) = a_n\alpha^n + \dots + a_0\iota \in L(V).
  \]
\end{notation}

\begin{theorem}
  Let \(V\) be an finite-dimensional \(\F\)-vector space. Let \(\alpha \in L(V)\). Then \(\alpha\) is diagonalisable if and only if \(p(\alpha) = 0\) for some \(p \in \F[t]\) which is the product of distinct linear factors.
\end{theorem}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item \(\Rightarrow\): Suppose \(\alpha\) is diagonalisable with distinct eigenvalues \(\lambda_1, \dots, \lambda_k\). Let
    \[
      p(t) = (t - \lambda_1) \cdots (t - \lambda_k).
    \]
    Let \(\basis B\) be a basis of eigenvectors. For \(v \in \basis B\), \(\alpha(v) = \lambda_i v\) for some \(i\). Thus
    \[
      0 = (\alpha - \lambda_i \iota) v = p(\alpha)(v) = 0.
    \]
    As this holds for all \(v \in \basis B\), \(p(\alpha) = 0\).
  \item \(\Leftarrow\): Suppose \(p(\alpha) = 0\) for this \(p\), which is monic wlog. Claim that
    \[
      V = \bigoplus_{i = 1}^k V_{\lambda_i}
    \]
    \begin{proof}
      For \(j = 1, \dots, k\), let
      \[
        q_j(t) = \prod_{i = 1, i \neq j}^k \frac{t - \lambda_i}{\lambda_j - \lambda_i}
      \]
      and \(q(t) = \sum_{j = 1}^k q_j(t)\). \(q(t)\) has degree at most \(k - 1\) and \(q(\lambda_i) = 1\) for all \(i = 1, \dots, k\) so \(q(t) = 1\).

      Let \(\pi_j = q_j(\alpha): V \to V\). By construction
      \[
        \sum_{j = 1}^{k} \pi_j = q(\alpha) = \iota \in L(V)
      \]
      so given \(v \in V\),
      \[
        v = q(\alpha) (v) = \sum_{j = 1}^{k} \pi_j(v).
      \]
      Also
      \[
        (\alpha - \lambda_j \iota)(\pi_j(v)) = (\alpha - \lambda_j\iota)(q_j(\alpha)(v)) = \frac{1}{\sum_{i \neq j}^{ } (\lambda_j - \lambda_i)} \underbrace{p(\alpha)}_{= 0}(v) = 0.
      \]
      We have thus shown that
      \[
        \im \pi_j \subseteq \ker(\alpha - \lambda_j \iota) = V_{\lambda_j}.
      \]
      Thus \(V = \sum_{j = 1}^{k} V_{\lambda_j}\).

      To prove the sum is direct, suppose \(v \in V_{\lambda_j} \cap (\sum_{i \neq j}^k V_{\lambda_i})\) and apply \(\pi_j\) to \(v\):
      \begin{align*}
        v \in V_{\lambda_j} &\Rightarrow \pi_j(v) = \prod_{i = 1, i \neq j}^{k} \frac{\lambda_j - \lambda_i}{\lambda_j - \lambda_i} v = v \\
        v \in \sum_{i \neq j}^{k}V_{\lambda_i} &\Rightarrow \pi_j(v) = 0
      \end{align*}
        so \(v = 0\) and the sum is direct.

        Now take the union of bases for \(V_{\lambda_i}\) as a basis for \(V\).
    \end{proof}
  \end{enumerate}
\end{proof}

\begin{remark}\leavevmode
  \begin{enumerate}
  \item Morally speaking, \(\pi_j\) ``projects'' to \(V_{\lambda_j}\).
  \item The proof shows that for \(k\) distinct eigenvalues \(\lambda_1, \dots \lambda_k\) of \(\alpha\), the sum \(\sum_j V_{\lambda_j}\) is direct. The only way for diagonalisation to fail is if \(\sum_j V_{\lambda_j} \lneq V\).
  \end{enumerate}
\end{remark}

\begin{corollary}
  Suppose \(A \in \M_n(\C)\) has finite order then \(A\) is diagonalisable. 
\end{corollary}

\begin{proof}
  \(p(A) = 0\) for \(p(t) = t^m - 1\) where \(m\) is the order of \(A\). This factorises as \(\prod_{i = 0}^{m - 1} (t - \xi^i)\) where \(\xi\) is a primitive \(m\)th root of unity.
\end{proof}

\begin{theorem}[Simultaneous diagonalisation]
  Let \(\alpha, \beta \in L(V)\) be diagonalisable. Then \(\alpha\) and \(\beta\) are \emph{simultaneous diagonalisable} (there exists a basis with respect to which they are both diagonal) if and only if \(\alpha\) and \(\beta\) commute.
\end{theorem}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(\Rightarrow\): Suppose there is a basis \(\basis B\) such that \(A = [\alpha]_{\basis B}\) and \(B = [\beta]_{\basis B}\) are diagonal. Any two diagonal matrices commute so \(AB = BA\), \(\alpha\beta = \beta\alpha\).
  \item \(\Leftarrow\): Suppose \(\alpha\) and \(\beta\) commute and both are diagonalisable. We have
    \[
      V = V_1 \oplus \cdots \oplus V_k
    \]
    where \(V_i = \ker(\alpha - \lambda_i\iota)\). Claim that \(\beta(V_j) \leq V_j\): suppose \(v \in V_j\),
    \[
      \alpha\beta (v) = \beta\alpha (v) = \beta\lambda_j v = \lambda_j \beta(v).
    \]

    As \(\beta\) is diagonalisable, there is a polynomial \(p\) with distinct linear factors such that \(p(\beta) = 0\). Now
    \[
      p(\beta|_{V_i}) = p(\beta)|_{V_i} = 0
    \]
    so \(\beta|_{V_i} \in L(V_i)\) is diagonal. Pick a basis \(\basis B_i\) of \(V_i\) combining its eigenvectors for \(\beta\). By construction these are also eigenvectors for \(\alpha\). With respect to \(\basis B = \bigcup_i \basis B_i\) both \(\alpha\) and \(\beta\) are diagonal.
  \end{itemize}
\end{proof}

\begin{lemma}[{\(\F[t]\)} as a Euclidean domain]
  Given \(a, b \in \F[t]\) with \(b \neq 0\), there eixst \(q, r \in \F[t]\) with \(\deg r < \deg b\) and \(a = qb + r\).
\end{lemma}

\begin{proof}
  IB Groups, Rings and Modules.
\end{proof}

\begin{definition}[Minimal polynomial]\index{polynomial!minimal}
  Suppose \(\alpha \in L(V)\) and \(V\) is finite-dimensional. The \emph{minimal polynomial} of \(\alpha\), \(m_\alpha\), is the monic non-zero polynomial of smallest degree such that
  \[
    m_\alpha(\alpha) = 0.
  \]
\end{definition}

\begin{remark}
  Let \(\dim V = n < \infty\), \(\dim L(V) = n^2\) so
  \[
    \iota, \alpha, \alpha^2, \dots, \alpha^{n^2} \in L(V)
  \]
  must be linearly dependent so there is a non-trivial relation. Thus minimal polynomial exists.
\end{remark}

\begin{lemma}
  Let \(\alpha \in L(V)\), \(p \in \F[t]\). Then \(p(\alpha) = 0\) if and only if \(m_\alpha(t) | p(t)\).
\end{lemma}

\begin{proof}
  By Euclidean algorithm there exist \(q, r \in \F[t]\) such that
  \[
    p(t) = m_\alpha(t) q(t) + r(t)
  \]
  where \(\deg r < \deg m_\alpha\). Then
  \[
    0 = p(\alpha) = m_\alpha(\alpha) q(\alpha) + r(\alpha)
  \]
  so \(r(\alpha) = 0\). By the minimality of the degree of \(m_\alpha\), \(r = 0\).
\end{proof}

\begin{corollary}
  \(m_\alpha\) is uniquely defined.
\end{corollary}

\begin{proof}
  Suppose \(m_1\) and \(m_2\) are both minimal polynomials of \(\alpha\). Then by the previous lemma \(m_1 \divides m_2\) and vice versa. By assumption both of them are monic so \(m_1 = m_2\).
\end{proof}

\begin{theorem}[Cayley-Hamilton Theorem]
  Let \(V\) be a finite-dimensional \(\F\)-vector space. Let \(\alpha \in L(V)\). Then
  \[
    \chi_\alpha(\alpha) = 0.
  \]
\end{theorem}

First we give a proof for \(\F = \C\):

\begin{proof}
  For some basis \(\basis B = \{v_1, \dots, v_n\}\), \(\alpha\) has matrix representation
  \[
    [\alpha]_{\basis B} =
    \begin{pmatrix}
      a_1 & \cdots & * \\
      & \ddots & \vdots \\
      0 & & a_n
    \end{pmatrix}
  \]
  Let \(U_j = \spans{v_1, \dots, v_j}\). Then \((\alpha - a_j \iota) U_j \leq U_{j - 1}\) so
  \[
    \underbrace{(\alpha - a_1 \iota) (\alpha - a_2 \iota) \cdots \underbrace{(\alpha - a_{n - 1} \iota) \underbrace{(\alpha - a_n \iota) V}_{\leq U_{n - 1}}}_{\leq U_{n - 2}}}_{\leq (\alpha - a_1\iota) U_1 = 0} = 0
  \]
  so
  \[
    \chi_\alpha(\alpha) = 0.
  \]
\end{proof}

However, this proof is unsatisfactory in that it relies on the fact of \(\C\) being algebraically closed which is partially, to say the least, an analytical and not an algebraic property\footnote{While being closed is an algebraic property, the construction of \(\C\) via \(\R\) from \(\Q\) is not. The point here is that Caylay-Hamilton holds for all fields, not just closed ones.}. In actuality, Cayley-Hamilton is a general result that applies to all fields. Thus we give an alternative algebraic proof:

\begin{proof}
  Let \(A \in \M_n(\F)\), then
  \[
    \chi_A(t) \cdot (-1)^n = t^n + a_{n - 1}t^{n - 1} + \dots + a_0 = \det(tI - A).
  \]
  For any matrix \(B\), we have
  \[
    B \adj B = \det B I.
  \]
  Let \(B = tI - A\). Then \(\adj B\) is a matrix whose entries are polynomials in \(t\) of degree smaller than \(n\), i.e.\ polynomials in \(t\) with coefficients in \(\M_n(\F)\).

  Thus
  \[
    (tI - A) \underbrace{(B_{n - 1}t^{n - 1} + \cdots + B_1 t + B_0)}_{\adj B} = \underbrace{(t^n + a_{n - 1}t^{n - 1} + \cdots + a_0)}_{\det B} I
  \]
  Equating the coefficients of each power of \(t\),
  \begin{align*}
    I &= B_{n - 1} \\
    a_{n - 1} I &= B_{n - 2} - AB_{n - 1} \\
    & \vdots \\
    a_0 I &= -AB_0
  \end{align*}
  multiply by \(A^{n - i + 1}\) for the \(i\)th row
  \begin{align*}
    A^n &= A^n B_{n - 1} \\
    a_{n - 1} A^{n - 1} &= A^{n - 1} B_{n - 2} - A^n B_{n - 1} \\
    & \vdots \\
    a_0 I &= -AB_0
  \end{align*}
  and add them up,
  \[
    A^n + a_{n - 1}A^{n - 1} + \dots a_1 A + A_0 I = 0.
  \]
\end{proof}

\begin{definition}[Algebraic multiplicity]\index{multiplicity!algebraic}
  Let \(\lambda\) be an eigenvalue of \(\alpha \in L(V)\) where \(V\) is a finite-dimensional \(\F\)-vector space. Write
  \[
    \chi_\alpha(t) = (t - \lambda)^{a_\lambda} q(t)
  \]
  for some \(q(t) \in \F[t]\) and \((t - \lambda) \ndivides q(t)\). \(a_\lambda\) is the \emph{algebraic multiplicity} of \(\lambda\) as an eigenvalue of \(\alpha\).
\end{definition}

\begin{definition}[Geometric multiplicity]\index{multiplicity!geometric}
  \(g_\lambda = n(\alpha - \lambda \iota)\) is the \emph{geometric multiplicity} of \(\alpha\).
\end{definition}

\begin{lemma}
  If \(\lambda\) is an eigenvalue then
  \[
    1 \leq g_\lambda \leq a_\lambda.
  \]
\end{lemma}

\begin{proof}
  \(1 \leq g_\lambda\) since \(\alpha - \lambda \iota\) is singular.

  Let \(\basis B = \{v_1, \dots v_n\}\) be a basis of \(V\) with \(\{v_1, \dots, v_g\}\) a basis of \(N(\alpha - \lambda \iota)\). Let \(g = g_\lambda\). Then
  \[
    [\alpha]_{\basis B} =
    \begin{pmatrix}
      \lambda I_g & * \\
      0 & A_1
    \end{pmatrix}
  \]
  where \(A_1 \in \M_{n - g}(\F)\). Thus
  \[
    \chi_\alpha(t) = (t - \lambda)^g \alpha_{A_1}(t)
  \]
  and \(g_\lambda \leq a_\lambda\).
\end{proof}

\begin{lemma}
  Let \(\lambda\) be an eigenvalue. Let \(c_\lambda\) be the multiplicity of \(\lambda\) as a root of \(m_\alpha\). Then
  \[
    1 \leq c_\lambda \leq a_\lambda.
  \]
\end{lemma}

\begin{proof}
  As \(m_\alpha \divides \chi_\lambda\), \(c_\lambda \leq a_\lambda\).

  As \(\lambda\) is an eigenvalue, \(\alpha(v) = \lambda v\) for some \(v \neq 0\). Now given \(p \in \F[t]\), \(p(\alpha)(v) = p(\lambda)(v)\). Apply this to \(m_\alpha\),
  \[
    0 = m_\alpha(\alpha)(v) = m_\alpha(\lambda)(v)
  \]
  so \(m_\alpha(\lambda) = 0\).
\end{proof}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Let
  \[
    A =
    \begin{pmatrix}
      1 & 0 & -2 \\
      0 & 1 & 1 \\
      0 & 0 & 2
    \end{pmatrix}
  \]
  then
  \[
    \chi_A(t) = |A - tI| = (2 - t)(1 - t)^2.
  \]
  There are two candidates for the minimal polynomial:
  \begin{itemize}
  \item \((t - 2)(t - 1)^2\),
  \item \((t - 2)(t - 1)\).
  \end{itemize}
  We can check that \((A - I)(A - 2I) = 0\) so the second one is the minimal polynomial. It follows that \(A\) is diagonalisable.
\item Let
  \[
    A =
    \begin{pmatrix}
      \lambda & 1 & & \\
      & \lambda & 1 &  \\
      & & \ddots & 1 \\
      & & & \lambda
    \end{pmatrix}
  \]
  which has \(g_\lambda = 1, a_\lambda = n, c_\lambda = n\).
  \end{enumerate}
\end{eg}

\begin{lemma}
  Let \(\F = \C\), \(\alpha \in L(V)\), then TFAE:
  \begin{enumerate}
  \item \(\alpha\) is diagonalisable,
  \item \(a_\lambda = g_\lambda\) for all eigenvalues \(\lambda\),
  \item \(c_\lambda = 1\) for all eigenvalues \(\lambda\).
  \end{enumerate}
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(1 \Leftrightarrow 2\): Let \(\lambda_1, \dots, \lambda_k\) be eigenvalues of \(\alpha\). Then \(\alpha\) is diagonalisable if and only if \(V = \bigoplus_k V_{\lambda_k}\). Take dimension of both sides,
    \begin{align*}
      \dim V &= n = \deg \chi_\alpha = a_1 + \dots + a_k \\
      \dim \bigoplus_k V_{\lambda_k} &= g_1 + \dots + g_k
    \end{align*}
    But \(g_k \leq a_k\) for all \(k\) so \(\alpha\) is diagonalisable if and only if \(g_k = \alpha_k\) for all \(k\).
  \item \(2 \Leftrightarrow 3\): By the Fundamental Theorem of Algebra, \(m_\alpha\) is a product of linear factors. \(\alpha\) is diagonalisable if and only if these are all distinct, i.e. \(c_\lambda = 1\) for all eigenvalues \(\lambda\).
  \end{itemize}
\end{proof}

\begin{remark}
  Over \(\C\),
  \begin{align*}
    \chi_\alpha(t) &= (\lambda_1 - t)^{a_1} \cdots (\lambda_k - t)^{a_k} \\
    m_\alpha(t) &= (t - \lambda_1)^{c_1} \cdots (t - \lambda_k)^{c_k}
  \end{align*}
  with \(1 \leq c_i \leq a_i\).
\end{remark}

\begin{definition}[Jordan normal form]\index{Jordan normal form}
  \(A \in \M_n(\F)\) is in \emph{Jordan normal form} if it is a block diagonal matrix
  \[
    A =
    \begin{pmatrix}
      J_{n_1}(\lambda_1) & & & \\
      & J_{n_2}(\lambda_2) & & \\
      & & \ddots & \\
      & & & J_{n_k}(\lambda_k)
    \end{pmatrix}
  \]
  where \(k \geq 1\), \(n_1, \dots, n_k \in \N\) with \(\sum_k n_k = n\), \(\lambda_i \in \F\) not necessarily distinct and
  \[
    J_m(\lambda) =
    \begin{pmatrix}
      \lambda & 1 & & \\
      & \lambda & 1 & \\
      & & \ddots & 1 \\
      & & & \lambda
    \end{pmatrix}
    \in M_m(\F)
  \]
  is a \emph{Jordan block}.
\end{definition}

\begin{theorem}
  \label{thm:jordan normal form}
  Every \(A \in \M_n(\C)\) is similar to a matrix in Jordan normal form, unique up to reordering the Jordan blocks.
\end{theorem}

\begin{proof}[Non-examinable]
  It is a consequence of a main theorem on modules in IB Groups, Rings and Modules.
\end{proof}

In the rest of this section assume \(\F = \C\) unless stated otherwise.

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Classification of Jordan normal forms for \(\M_2(\C)\):
  \[
    \begin{array}[h]{c|c|c}
      \begin{psmallmatrix}
        \lambda_1 & \\
        & \lambda_2
      \end{psmallmatrix}
        &
          \begin{psmallmatrix}
            \lambda & \\
            & \lambda
          \end{psmallmatrix}
        &
          \begin{psmallmatrix}
            \lambda & 1 \\
            & \lambda
          \end{psmallmatrix}
      \\ \hline
      (t - \lambda_1)(t - \lambda_2) & t - \lambda & (t - \lambda)^2
    \end{array}
  \]
\item Classification of Jordan normal forms for \(\M_3(\C)\):
  \[
    \begin{array}[h]{c|c|c}
      \begin{psmallmatrix}
        \lambda_1 & & \\
        & \lambda_2 & \\
        & & \lambda_3
      \end{psmallmatrix}
        &
          \begin{psmallmatrix}
            \lambda_1 & & \\
            & \lambda_2 & \\
            & & \lambda_2
          \end{psmallmatrix}
        &
          \begin{psmallmatrix}
            \lambda & & \\
            & \lambda & \\
            & & \lambda
          \end{psmallmatrix}
      \\ \hline
      (t - \lambda_1)(t - \lambda_2)(t - \lambda_3) & (t - \lambda_1)(t - \lambda_2) & t - \lambda \\ \hline \hline
      & & \\
      \begin{psmallmatrix}
        \lambda_1 & & \\
        & \lambda_2 & 1 \\
        & & \lambda_2
      \end{psmallmatrix}
        &
          \begin{psmallmatrix}
            \lambda & & \\
            & \lambda & 1 \\
            & & \lambda
          \end{psmallmatrix}
        &
          \begin{psmallmatrix}
            \lambda & 1 & \\
            & \lambda & 1 \\
            & & \lambda
          \end{psmallmatrix}
      \\ \hline
      (t - \lambda_1)(t - \lambda_2)^2 & (t - \lambda)^2 & (t - \lambda)^3
    \end{array}
  \]
  \end{enumerate}
\end{eg}

\begin{theorem}[Generalised Eigenspace Decomposition]
  \label{thm:generalised eigenspace decomposition}
  Let \(V\) be a finite-dimensional \(\C\)-vector space and \(\alpha \in L(V)\). Suppose that
  \[
    m_\alpha(t) = (t - \lambda_1)^{c_1} \cdots (t - \lambda_k)^{c_k}
  \]
  where \(\lambda_i\)'s are distinct. Then
  \[
    V = \bigoplus_j V_j
  \]
  where
  \[
    V_j = N(\alpha - \lambda_j \iota)^{c_j}
  \]
  is the \emph{generalised eigenspace}.
\end{theorem}

\begin{proof}[Sketch of proof]
  Let
  \[
    p_j(t) = \prod_{i \neq j}^{ } (t - \lambda_1)^{c_i}.
  \]
  The \(p_j\) have no common factor so by Euclidean algorithm we can find \(q_1, \dots, q_k \in \C[t]\) such that
  \[
    \sum_{j}^{ } p_j(t)q_j(t) = 1.
  \]
  Let \(\pi_j = q_j(\alpha)p_j(\alpha) \in L(V)\). Note \(\sum_{j = 1}^{k} \pi_j = \iota\).
  \begin{enumerate}
  \item As \(m_\alpha(\alpha) = 0\), \((\alpha - \lambda_i \iota)^{c_j} \pi_j = 0\) so \(\im \pi_j \leq V_j\).
  \item Suppose \(v \in V\), \(v = \iota(v) = \sum \pi_j(v)\) so \(V = \sum_j V_j\).
  \item To show the sum is direct, \(\pi_i \pi_j = 0\) for \(i \neq j\) so
    \[
      \pi_i = \pi_i \left( \sum_{j = 1}^{k} \pi_j \right) = \pi_i^2
    \]
    i.e.\ \(\pi_i\) is a projection. Then
    \[
      \pi_i |_{V_j} =
      \begin{cases}
        \id & i = j \\
        0 & i \neq j
      \end{cases}
    \]
    Directness follows.
  \end{enumerate}
\end{proof}

\begin{remark}\leavevmode
  \begin{enumerate}
  \item We can use \nameref{thm:generalised eigenspace decomposition} to reduce the proof of \Cref{thm:jordan normal form} to a single eigenvalue.
  \item Considering \(\alpha - \lambda \iota\) can reduce to the case of eigenvalue \(0\).
  \end{enumerate}
\end{remark}

\begin{lemma}
  Let \(\alpha \in L(V)\) with Jordan normal form \(A \in \M_n(\C)\). Then the number of Jordan blocks \(J_\ell(\lambda)\) of \(A\) with \(\ell \geq 1\) is
  \[
    n((\alpha - \lambda \iota)^\ell) - n((\alpha - \lambda \iota)^{\ell - 1}).
  \]
\end{lemma}

\begin{proof}
  Work blockwise, for each \(s \times s\) block, 
  \[
    \begin{array}[h]{c|c|c}
      \begin{psmallmatrix}
        \lambda & 1 & & \\
        & \ddots & \ddots & \\
        & & \ddots & 1 \\
        & & & \lambda
      \end{psmallmatrix}
        &
          \begin{psmallmatrix}
            0 & 1 & & \\
            & & \ddots & \\
            & & & 1 \\
            & & & 0
          \end{psmallmatrix}
                 &
                   \begin{psmallmatrix}
                     0 & 0 & 1 & \\
                     & & \ddots & 1 \\
                     & & & 0 \\
                     & & & 0
                   \end{psmallmatrix}
      \\ \hline
      J_s(\lambda) & J_s(\lambda) - \lambda I & (J_s(\lambda) - \lambda I)^2
    \end{array}
  \]
  so
  \[
    n((J_s(\lambda) - \lambda I)^k) =
    \begin{cases}
      k & k \leq s \\
      s & k \geq s
    \end{cases}
  \]
\end{proof}

\begin{eg}
  Let
  \[
    A =
    \begin{pmatrix}
      0 & -1 \\
      1 & 2
    \end{pmatrix}
  \]
  we want to find a basis \(\basis B = \{v_1, v_2\}\) with respect to which \(A\) is in Jordan normal form.
  \begin{enumerate}
  \item
    \[
      \chi_A(t) =
      \begin{vmatrix}
        -t & -1 \\
        1 & 2 - t
      \end{vmatrix}
      = t^2 - 2t + 1 = (t - 1)^2
    \]
    There are two possibilities:
    \begin{enumerate}
    \item \(m_A(t) = t - 1\). Then the Jordan normal form is
      \[
        \begin{pmatrix}
          1 & 0 \\
          0 & 1
        \end{pmatrix}
      \]
    \item \(m_A = (t - 1)^2\). Then the Jordan normal form is
      \[
        \begin{pmatrix}
          1 & 1 \\
          0 & 1
        \end{pmatrix}
      \]
    \end{enumerate}
    A trick here is to note that if \(A\) is conjugate to \(I\) then \(A = I\). Thus (b) holds.
  \item The eigenspace is spanned by \(v_1 = \binom{1}{-1}\).
  \item \(v_2\) satisfies \((A - I)v_2 = v_1\) so
    \[
      \begin{pmatrix}
        -1 & -1 \\
        1 & 1
      \end{pmatrix}
      v_2 =
      \begin{pmatrix}
        1 \\
        -1
      \end{pmatrix}
    \]
    so \(v_2 = \binom{-1}{0}\). Note that \(v_2\) is not unique.
  \item Finally,
    \[
      A =
      \begin{pmatrix}
        1 & -1 \\
        -1 & 0
      \end{pmatrix}
      \begin{pmatrix}
        1 & 1 \\
        0 & 1
      \end{pmatrix}
      \begin{pmatrix}
        1 & -1 \\
        -1 & 0
      \end{pmatrix}
      ^{-1}
    \]
  \end{enumerate}
\end{eg}

We can use the diagonalisation to calculate powers of matrices:
\[
  A^n = (P^{-1}JP)^n = P^{-1}J^nP = P
  \begin{pmatrix}
    1 & n \\
    0 & 1
  \end{pmatrix}
  P
\]

\begin{remark}
  In Jordan normal form,
  \begin{itemize}
  \item \(a_\lambda\) is the total number of times that \(\lambda\) appears in the diagonal.
  \item \(g_\lambda\) is the number of \(\lambda\)-Jordan blocks.
  \item \(c_\lambda\) is the size the largest \(\lambda\)-Jordan block.
  \end{itemize}
\end{remark}

\section{Bilinear Form II}

In this chapter we are going to studying a special bilinear form in detail. Let \(\varphi: V \times V \to \F\) be a bilinear form on \(V\) and assume we take the same basis for both factors of \(V\), say \(\basis B\). Therefore for \(\dim V < \infty\), \(\varphi\) has matrix representation
\[
  [\varphi]_{\basis B} = [\varphi]_{\basis B, \basis B}.
\]

Recall that
\begin{lemma}
  Let \(\varepsilon: V \times V \to \F, \dim V < \infty\) and \(\basis B\) and \(\basis B'\) are two bases for \(V\). Let \(P = [\id]_{\basis B', \basis B}\). Then
  \[
    [\varphi]_{\basis B'} = P^T[\varphi]_{\basis B}P.
  \]
\end{lemma}

\begin{proof}
  Special case of \Cref{prop:change of basis of bilinear form}.
\end{proof}

This motivates us to define a relation on \(\M_n(\F)\)

\begin{definition}[Congruent]\index{matrix!congruency}
  \(A, B \in \M_n(\F)\) are \emph{congruent} if
  \[
    A = P^TBP
  \]
  for some invertible \(P\).
\end{definition}

\begin{note}
  This is an equivalence relation.
\end{note}

Naturally, we want to find nice forms to which a general bilinear form is congruent. Certainly the nicest form we can have is diagonal matrix. It turns out the property we require a bilinear form to be ``diagonalisable'' is

\begin{definition}[Symmetric]\index{bilinear!symmetric}
  A bilinear form \(\varphi\) on \(V\) is \emph{symmetric} if
  \[
    \varphi(u, v) = \varphi(v, u)
  \]
  for all \(u, v \in V\).
\end{definition}

\begin{note}\leavevmode
  \begin{itemize}
  \item \(A \in \M_n(\F)\) is symmetric if \(A = A^T\). Then \(\varphi\) is symmetric if and only if \([\varphi]_{\basis B}\) is symmetric for any basis \(\basis B\), if and only if \([\varphi]_{\basis B}\) is symmetric for one \(\basis B\).
  \item To be able to represent \(\varphi\) by a diagonal matrix, \(\varphi\) needs to be symmetric:
    \[
      [\varphi]_{\basis B} = P^TAP = D \Rightarrow D^T = D = P^TA^TP \Rightarrow A = A^T
    \]
  \end{itemize}
\end{note}

\begin{definition}[Quadratic form]\index{quadratic form}
  A map \(Q: V \to \F\) is a \emph{quadratic form} if there is a bilinear form \(\varphi\) on \(V\) such that
  \[
    Q(v) = \varphi(v, v)
  \]
  for all \(v \in V\).
\end{definition}

\begin{eg}
  Let \(V = \R^2\). A general quadratic form is
  \[
    \begin{pmatrix}
      x \\
      y
    \end{pmatrix}
    \mapsto
    \begin{pmatrix}
      x & y
    \end{pmatrix}
    %
    \begin{pmatrix}
      a & b \\
      c & d
    \end{pmatrix}
    %
    \begin{pmatrix}
      x \\
      y
    \end{pmatrix}
    = ax^2 + (b + c) xy + dy^2
  \]
\end{eg}

\begin{remark}
  A quadratic form does not change under \(A \mapsto \frac{1}{2}(A + A^T)\) where \(A\) is a representation of the inducing bilinear form.
\end{remark}

\begin{proposition}
  Assume \(\ch \F \neq 2\). If \(Q: V \to \F\) is a quadratic form then there exists a unique symmetric bilinear form \(\varphi\) on \(V\) such that
  \[
    Q(v) = \varphi(v, v)
  \]
  for all \(v \in V\).
\end{proposition}

\begin{proof}
  First we prove the existence. Let \(\psi\) be a bilinear form on \(V\) such that \(Q(v) = \psi(v, v)\) for all \(v \in V\). We want to construct a symmetric bilinear form by adding \(\psi\) and its transpose. Let
  \[
    \varphi(u, v) = \frac{1}{2}(\psi(u, v) + \psi(v, u)).
  \]
  (this is where we require \(\ch \F \neq 2\)) then it is bilinear and symmetric and
  \[
    \varphi(v, v) = \psi(v, v) = Q(u).
  \]

  To show the uniqueness, suppose \(\varphi\) is such a symmetric bilinear form. Consider
  \begin{align*}
    Q(u + v) &= \varphi(u + v, u + v) \\
             &= \varphi(u, u) + \varphi(u, v) + \varphi(v, u) + \varphi(v, v) \\
             &= Q(u) + 2\varphi(u, v) + Q(v)
  \end{align*}
  Rearrange,
  \[
    \varphi(u, v) = \frac{1}{2}(Q(u + v) - Q(u) - Q(v))
  \]
  which is uniquely determined.
\end{proof}

\begin{remark}
  The last identity
  \[
    \varphi(u, v) = \frac{1}{2}(Q(u + v) - Q(u) - Q(v))
  \]
  is called the \emph{polarisation identity} and will appear later.
\end{remark}

The note after the definition of symmetric bilinear form shows that being symmetric is a necessary condition for a bilinear form to be ``diagonalisable''. The following theorem says that it is also sufficient:

\begin{theorem}
  Let \(\varphi\) be a symmetric bilinear form on \(V\), an \(\F\)-vector space and assume \(\ch \F \neq 2\) and \(\dim V < \infty\). Then there is a basis \(\basis B\) of \(V\) such that \([\varphi]_{\basis B}\) is diagonal.
\end{theorem}

\begin{proof}
  Induction on \(n = \dim V\). If \(n = 0\) or \(1\) then obviously true.

  Suppose the theorem holds for all spaces of dimension smaller than \(n\). There are two cases to consider:
  \begin{enumerate}
  \item if \(\varphi(u, u ) = 0\) for all \(u\) then by the polarisation identity \(\varphi = 0\) so diagonal.
  \item otherwise choose \(e_1 \in V\) such that \(\varphi(e_1, e_1) \neq 0\). Let
    \[
      U = \spans{e_1}^\perp = \{u \in V: \varphi(e_1, u) = 0\} = \ker (\varphi(e_1, -): V \to \F)
    \]
    which has dimension \(n - 1\) by rank-nullity. Moreover, \(V = \spans{e_1} \oplus U\) since \(\spans{e_1} \cap U = 0\) and \(\dim(\spans{e_1} \oplus U) = n\). Consider \(\varphi|_U: U \times U \to \F\) which is also symmetric bilinear. By induction hypothesis there is a basis \(e_2, \dots e_n\) of \(U\) with respect to which \(\varphi|_U\) is diagonal. Now \(\varphi\) is diagonal with respect to \(e_1, \dots, e_n\).
  \end{enumerate}
\end{proof}

\begin{notation}
  In \(V = \R^n\) with standard basis \(e_1, \dots, e_n\), write
  \[
    Q(x_1, x_2, \dots, x_n) = Q \left(\sum_{i = 1}^n x_i e_i \right).
  \]
\end{notation}

\begin{eg}
  Let \(V = \R^3\) with standard basis \(e_1, e_2, e_3\) and
  \[
    Q(x_1, x_2, x_3) = x_1^2 + x_2^2 + 2x_3^2 + 2x_1x_2 + 2x_1x_3 -2x_2x_3.
  \]
  We want a basis \(f_1, f_2, f_3\) of \(\R^3\) such that
  \[
    Q(af_1 + bf_2 + cf_3) = \lambda a^2 + \mu b^2 + \nu c^2
  \]
  for some \(\lambda, \mu, \nu \in \R\), which are the diagonal entries.

  The martix representation of \(Q\) with repect to \(e_1, e_2, e_3\) is
  \[
    A =
    \begin{pmatrix}
      1 & 1 & 1 \\
      1 & 1 & -1 \\
      1 & -1 & 2
    \end{pmatrix}
  \]

  We could use the algorithm as outlined in the induction proof above but choose to do it differently by completing the squre:
    \begin{align*}
      Q(x_1, x_2, x_3) &= \underbrace{(x_1 + x_2 + x_3)^2}_{\text{used all terms in } x_1} + x_3^2 - 2x_2x_3 - 2 x_2x_3 \\
                       &= (x_1 + x_2 + x_3)^2 + \underbrace{(x_3 - 2x_2)^2}_{\text{used all terms in } x_3} - (2x_2)^2
    \end{align*}
    From here we can read off the diagonal matrix and the basis: for some \(P\),
    \[
      P^TAP =
      \begin{pmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & -1
      \end{pmatrix}
    \]
    To find \(P\), note that
    \[
      \begin{pmatrix}
        x_1' \\
        x_2' \\
        x_3'
      \end{pmatrix}
      =
      \underbrace{
      \begin{pmatrix}
        1 & 1 & 1 \\
        0 & -2 & 1 \\
        0 & 2 & 0 
      \end{pmatrix}
    }_{P^{-1}}
    %
    \begin{pmatrix}
      x_1 \\
      x_2 \\
      x_3
    \end{pmatrix}
    \]
\end{eg}

\begin{corollary}
  Let \(\varphi\) be a symmetric bilinear form on \(V\), a finite-dimensional \(\C\)-vector space. There there is a basis \(\basis B = \{v_1, \dots v_n\}\) of \(V\) such that
  \[
    [\varphi]_{\basis B} =
    \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}
  \]
  where \(r = r(\varphi)\).
\end{corollary}

\begin{proof}
  Pick basis \(\basis E = \{e_1, \dots e_n\}\) such that
  \[
    [\varphi]_{\basis E} =
    \begin{pmatrix}
      a_1 & & \\
      & \ddots & \\
      & & a_n
    \end{pmatrix}
  \]
  Reorder the \(e_i\)'s such that
  \[
    \begin{cases}
      a_i \neq 0 & 1 \leq i \leq r \\
      a_i = 0 & i > r
    \end{cases}
  \]
  For \(i \leq r\), pick a complex square root of \(a_i\), say \(\sqrt{a_i}\). Now let
  \[
    v_i =
    \begin{cases}
      \frac{e_i}{\sqrt{a_i}} & 1 \leq i \leq r \\
      e_i & i > r
    \end{cases}
  \]
\end{proof}

\begin{corollary}
  Every symmetric matrix \(A \in \M_n(\C)\) is congruent to a unique matrix of the form \(\begin{psmallmatrix} I_r & 0 \\ 0 & 0 \end{psmallmatrix}\).
\end{corollary}

Equivalently,
\[
  Q \left( \sum_{i = 1}^{n} \lambda_i v_i \right) = \sum_{i = 1}^{r} \lambda_i^2.
\]

We have derived a corollary for our favourite field \(\C\), and there is another one corresponding to our second favourite \(\R\):

\begin{corollary}
  Let \(\varphi\) be a symmetric bilinear form on \(V\), a finite-dimensional \(\R\)-vector space. There is a basis \(\basis B = \{v_1, \dots, v_n\}\) such that
  \[
    [\varphi]_{\basis B} =
    \begin{pmatrix}
      I_p & & \\
      & -I_q & \\
      & & 0
    \end{pmatrix}
  \]
  where \(p, q \geq 0\) and \(p + q = r(\varphi)\).
\end{corollary}

\begin{proof}
  The proof is the same as for \(\C\) up to the point of exhibiting a basis with respect to which \(\varphi\) is diagonal. Note that we \emph{cannot} choose a square root for all the entries. Instead, reorder the indices such that
  \[
    \begin{cases}
      a_i > 0 & 1 \leq i \leq p \\
      a_i < 0 & p + 1 \leq i \leq p + q \\
      a_i = 0 & i > p + q
    \end{cases}
  \]
  and let
  \[
    v_i =
    \begin{cases}
      \frac{e_i}{\sqrt{a_i}} & 1 \leq i \leq p \\
      \frac{e_i}{\sqrt{-a_i}} & p + 1 \leq i \leq p + q \\
      e_i & i > p + 1
    \end{cases}
  \]
\end{proof}

Equivalently,
\[
  Q \left( \sum_{i = 1}^{n} \lambda_i v_i \right) = \sum_{i = 1}^{p} \lambda_i^2 - \sum_{i = p + 1}^{q + p} \lambda_i^2.
\]

\begin{definition}[Positive/Negative (semi-)definiteness]\index{matrix!postive definite}
  A symmetric bilinear form \(\varphi\) on a real vector space \(V\) is
  \begin{itemize}
  \item \emph{positive definite} if \(\varphi(u, u) > 0\) for all \(u \in V\setminus \{0\}\).
  \item \emph{positive semi-definite} if \(\varphi(u, u) \geq 0\) for all \(u \in V\setminus \{0\}\).
  \item \emph{positive definite} if \(\varphi(u, u) < 0\) for all \(u \in V\setminus \{0\}\).
  \item \emph{positive semi-definite} if \(\varphi(u, u) \leq 0\) for all \(u \in V\setminus \{0\}\).
  \item \emph{indefinite} if none of the above.
  \end{itemize}
\end{definition}

The same terminologies apply to quadratic forms.

\begin{eg}
  The bilinear form on \(\R^n\) represented by \(\begin{psmallmatrix} I_p & 0 \\ 0 & 0 \end{psmallmatrix} \in \M_n(\R)\) is positive definite is \(p = n\) and positive semi-definite if \(p < n\).
\end{eg}

\begin{definition}[Signature]\index{bilinear!signature}
  The \emph{signature} of a real symmetric bilinear form \(\varphi\) is
  \[
    s(\varphi) = p - q.
  \]
\end{definition}

Again, this applies to quadratic forms as well.

However, we have not even checked whether this is well-defined. Thus we need

\begin{theorem}[Sylvester's Law of Inertia]
  If a real symmetric bilinear bilinear form \(\varphi\) has with respect to basis \(\basis B\) and \(\basis B'\)
  \[
    [\varphi]_{\basis B} =
    \begin{pmatrix}
      I_p & & \\
      & -I_q & \\
      & & 0
    \end{pmatrix}
    \quad
    [\varphi]_{\basis B'} =
    \begin{pmatrix}
      I_p' & & \\
      & -I_q' & \\
      & & 0
    \end{pmatrix}
  \]
  then
  \begin{align*}
    p &= p', \\
    q &= q'.
  \end{align*}
\end{theorem}

It is then immediate that

\begin{corollary}
  Signature is well-defined.
\end{corollary}

\begin{proof}
  For uniqueness of \(p\), show that \(p\) is the largest dimension of a subspace on which \(\varphi\) is positive definite. This suffices as it is a basis invariant characterisation.

  Let \(\basis B = \{v_1, \dots, v_n\}\). Let \(X = \spans{v_1, \dots, v_p}\) and \(Y = \spans{v_{p + 1}, \dots, v_n}\). \(\dim X = p\) and \(\varphi\) is positive definite on \(X\):
  \[
    Q(v) = Q \left( \sum_{i = 1}^{p} \lambda_i v_i \right) = \sum_{i = 1}^{p} \lambda_i^2 > 0
  \]
  for all \(v \neq 0\). Similarly \(\varphi\) is negative semi-definite on \(Y\).

  Suppose is \(\varphi\) is positive definite on some other subspace \(X'\). Then \(X' \cap Y = 0\) since \(Q\) is positive definite on \(X'\) and negative semi-definite on \(Y\). Therefore
  \[
    \dim (Y + X') = \dim Y \oplus X' = \dim Y + \dim X' \leq n
  \]
  but since \(\dim Y = n - p\) we have \(\dim X' \leq p\).

  For \(q\), we can either run the same argument with negative definite spaces, or use the fact that \(q = r(\varphi) - q\) is invariant.
\end{proof}

The zero diagonal block is not very interesting but it does get a speical name:

\begin{definition}[Kernel of symmetric bilinear form]\index{bilinear!kernel}
  The \emph{kernel} of a symmetric bilinear form is
  \[
    K = \{ v \in V: \varphi(u, v) = 0 \text{ for all } u \in V\}.
  \]
\end{definition}

\begin{note}
  \[
    \dim K = n - r(\varphi).
  \]
\end{note}

In our previous notation, the kernel is simply
\[
  K = \spans{v_{p + q + 1}, \dots, v_n}.
\]

\begin{caution}
  There is a subspace \(T\) of dimension \(n - (p + q) + \min(p, q)\) such that \(\varphi|_{T \times T} = 0\): say \(p \geq q\),
  \[
    T = \spans{v_1 + v_{p + 1}, \dots, v_q + v_{p + q}, v_{p + q + 1}, \dots, v_n}.
  \]
\end{caution}

\begin{ex}
  Check that \(T\) above is the largest possible dimension of such a space.
\end{ex}

\section{Sesquilinear Form}

Let \(\F = \C\) throughout this chapter.

The dot product on a real vector space comes naturally as a bilinear form. However, its generalisation to complex vector space, the standard inner product defined by
\[
  \innerproduct{x, y} = \sum_{i = 1}^{n} x_i \conj y_i
\]
is not bilinear: the second coordinate transforms by ``conjugate-linearity'' instead of linearity. Among many other examples of the same spirit\footnote{Formally, any \(\C\)-algebra induced by an inner product.}, this serves as a motivation to modify the definition of bilinear forms for \(\C\)-vector spaces:

\begin{definition}[Sesquilinear form]\index{sesquilinear}
  Let \(V\) and \(W\) be \(\C\)-vector spaces. A \emph{sesquilinear form} is a function \(\varphi: V \times W \to \C\) such that
  \begin{align*}
    \varphi(\lambda_1v_1 + \lambda_2v_2, w) &= \lambda_1 \varphi(v_1, w) + \lambda_2 \varphi(v_2, w) \\
    \varphi(v, \mu_1w_1 + \mu_2w_2) &= \conj \mu_1 \varphi(v, w_1) + \conj \mu_2 \varphi(v, w_2)
  \end{align*}
  for all \(\lambda_1, \mu_1 \in \C\) and \(v, v_1, v_2 \in V\), \(w, w_1, w_2 \in W\).
\end{definition}

Naturally we would expect a sesquilinear form, just like a bilinear form, to have a matrix representation which behaves and transforms accordingly under change-of-basis:

\begin{definition}[Matrix of sesquilinear form]\index{matrix!sesquilinear form, of}
  Same notation as above. Let \(\basis B = \{v_1, \dots, v_m\}\) be a basis for \(V\) and \(\basis C = \{w_1, \dots, v_n\}\) be a basis for \(W\). Then the \emph{matrix} of \(\varphi\) with respect to \(\basis B\) and \(\basis C\) is
  \[
    [\varphi]_{\basis B, \basis C} = \left(\varphi(v_i, w_j)\right)_{i,j}
  \]
\end{definition}

\begin{lemma}
  \[
    \varphi(u, v) = [u]_{\basis B}^T [\varphi]_{\basis B, \basis C} \conj{[v]}_{\basis C}.
  \]
\end{lemma}

\begin{proof}
  Easy.
\end{proof}

\begin{lemma}
  Let \(\basis B, \basis B'\) be bases for \(V\), \(P = [\id]_{\basis B', \basis B}\) and \(\basis C, \basis C\) be bases for \(W\), \(Q = [\id]_{\basis C', \basis C}\). Then
  \[
    [\varphi]_{\basis B', \basis C'} = P^T [\varphi]_{\basis B, \basis C} \conj Q.
  \]
\end{lemma}

\begin{proof}
  Ditto.
\end{proof}

\printindex
\end{document}