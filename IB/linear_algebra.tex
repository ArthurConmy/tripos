\documentclass[a4paper]{article}

\def\npart{IB}

\def\ntitle{Linear Algebra}
\def\nlecturer{A.\ M.\ Keating}

\def\nterm{Michaelmas}
\def\nyear{2017}

\input{header}


\newcommand*{\M}{\matrixring}
\newcommand*{\spans}{\generation}

\newcommand*{\ann}{\circ}

\newcommand*{\basis}{\mathcal}

\theoremstyle{definition}
\newtheorem*{caution}{Caution}

\begin{document}

\input{titlepage}

\tableofcontents

\section{Vector Spaces}

\begin{convention}
  Throughout this course, $\mathbb{F}$ denotes a general field. If you wish, think of it as $\mathbb{R}$ or $\mathbb{C}$.
\end{convention}

\subsection{Definition}

\begin{definition}[Vector space]
  An $\mathbb{F}$-\emph{vector space} (or a vector space over $\mathbb{F}$) is an abelian group $(V, +)$ equipped with a function, called \emph{scalar multiplication}:
  \begin{align*}
    \mathbb{F}\times V &\to V \\
    (\lambda, v) &\mapsto \lambda\cdot v
  \end{align*}
  satisfying the axioms
  \begin{itemize}
  \item distributive over vectors: $\lambda(v_1+v_2) = \lambda(v_1+v_2)$,
  \item distributive over scalars: $(\lambda_1+\lambda_2)v= \lambda_1 v+\lambda_2 v$,
  \item $\lambda(\mu v) = \lambda \mu v$,
  \item $1\cdot v = v$.
  \end{itemize}
\end{definition}

The additive unit of $V$ is denoted by $\V 0$.

\begin{eg}\leavevmode
  \label{eg:matrix as V}
  \begin{enumerate}
  \item $\forall n \in \mathbb{N}, \mathbb{F}^n$ is the space of column vectors of length $n$ with entries in $\mathbb{F}$. It is an vector space by entry-wise addition and entry-wise scalar multiplication.
  \item $\\M_{m,n}(\mathbb{F})$, the set of $m\times n$ matrices with entries in $\mathbb{F}$, with the operation defined as entry-wise addition.
    \item For any set $X$, $\mathbb{R}^X = \{f: X \to \mathbb{R}\}$, the set of $\mathbb{R}$-valued functions on $X$, with addition and scalar multiplication defined pointwise. For instance, $(f_1+f_2)(x) = f_1(x)+f_2(x)$.
  \end{enumerate}
\end{eg}

\begin{ex}\leavevmode
  \begin{enumerate}
  \item Check the above examples satisfy the axioms.
    \item $0\cdot v = \V 0$ and $(-1)\cdot v = -v$ for all $v \in V$.
  \end{enumerate}
\end{ex}

\subsection{Vector Subspace}

\begin{definition}[Vector Subspace]
  Let $V$ be an $\mathbb{F}$-vector space. A subset $U \subseteq V$ is a \emph{subspace}, denoted $U \leq V$, if
  \begin{itemize}
  \item $\V 0 \in U$,
  \item $U$ is closed under addition: $\forall u_1, u_2 \in U, u_1+u_2 \in U$,
    \item $U$ is closed under scalar multiplication: $\forall u \in U, \forall \lambda \in \mathbb{F}, \lambda u \in U$.
  \end{itemize}
\end{definition}

\begin{ex}
  If $U$ is a subspace of $V$, then $U$ is also an $\mathbb{F}$-vector space.
\end{ex}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item $V = \mathbb{R}^{\mathbb{R}}$, the set all functions from $\mathbb{R}$ to itself, has a (proper) subspace $C(\mathbb{R})$, the space of continuous functions on $\mathbb{R}$ as continuous functions are closed under addition and scalar multiplication. $C(\mathbb{R})$ in turn has a proper subspace $P(\mathbb{R})$, the set of all polynomials in $\mathbb{R}$.
    \item $\{(x_1,x_2,x_3) \in \mathbb{R}^3: x_1+x_2+x_3 = t\}$ where $t$ is some fixed constant is a subspace of $\mathbb{R}^3$ if and only if $t = 0$.
  \end{enumerate}
\end{eg}

\begin{proposition}
  Let $V$ be an $\mathbb{F}$-vector space, $U, W \leq V$. Then $U \cap W \leq V$.
\end{proposition}

\begin{proof}\leavevmode
  \begin{itemize}
  \item $\V 0 \in U, \V 0 \in V$ so $\V 0 \in U \cap W$.
    \item Suppose $u, w \in U \cap W$. Fix $\lambda, \mu \in \mathbb{F}$. As $U \leq V$, $\lambda u + \mu w \in U$. As $W \leq V$, $\lambda u +\mu w \in W$ so $\lambda u + \mu w \in U \cap W$. Take $\lambda = \mu = 1$ for vector addition and $\mu = 0$ for scalar multiplication.
  \end{itemize}
\end{proof}

\begin{eg}
  $V = \mathbb{R}^3, U = \{(x,y,z): x=0\}, W=\{(x,y,z):y=0\}$, then $U\cap W=\{(x,y,z):x=y=0\}$.
\end{eg}

\begin{note}
The union of a family of subspaces is \emph{almost never} a subspace. For example, $V = \mathbb{R}^2$, $U, V$ be $x$- and $y$-axis.
\end{note}

\begin{definition}[Sum of vector spaces]
  Let $V$ be an $\mathbb{F}$-vector space, $U, W \leq V$, the \emph{sum} of $U$ and $W$ is the set
  \[
    U + W = \{u+w: u\in U, w\in W\}
  \]
\end{definition}

\begin{eg}
  Use the definition from the previous example, $U+W=V$.
\end{eg}

\begin{proposition}
  $U+W \leq V$.
\end{proposition}

\begin{proof}\leavevmode
  \begin{itemize}
  \item $\V 0 = \V 0 + \V 0 \in U+W$,
  \item $u_1,u_1\in U, w_1,w_2\in W$, $(u_1+w_2) + (u_2+w_2) = (u_1+u_2)+(w_1+w_2) \in U+W$,
    \item similar for scalar multiplication. Left as an exercise.
  \end{itemize}
\end{proof}

\begin{note}
  $U+W$ is the smallest subspace containing both $U$ and $W$. This is because all elements of the form $u+w$ are in such a space by closure under addition.
\end{note}

\begin{definition}[Quotient vector space]
  Let $V$ be an $\mathbb{F}$-vector space, $U \leq V$. The \emph{quotient space} $V/U$ is the abelian gropup $V/U$ equipped with scalar multiplication
  \begin{align*}
    \mathbb{F} \times V/U &\to V/U \\
    (\lambda, v+U) &\mapsto \lambda v+U
  \end{align*}
\end{definition}

\begin{proposition}
  This is well-defined and $V/U$ is an $\mathbb{F}$-vector space.
\end{proposition}

\begin{proof}
  First check it is well-defined. Suppose $v_1+U= v_2+U \in V/U$. Then $v_1-v_2\in U$. Now use closure under scalar multiplication and distributivity, $\lambda v_1 - \lambda v_2 = \lambda(v_1-v_2)\in U$ so $\lambda v_1 + U = \lambda v_2 +U\in V/U$.
  Now check vector space axioms of $V/U$, which will follow from the axioms for $V$:
  \begin{itemize}
  \item $\lambda(\mu(v+U)) = \lambda(\mu v+U) = \lambda(\mu v)+U = (\lambda\mu) v+U = \lambda\mu(v+U)$,
  \item other axioms are left as an exercise.
  \end{itemize}
\end{proof}

\subsection{Span, Linear Independence \& Basis}

\begin{definition}[Span]
  Let $V$ be a $\F$-vector space, $S \subseteq V$ be a subset. The \emph{span} of $S$
  \[
    \spans S = \Big\{\sum_{s\in S} \lambda_s s : \lambda_s \in F \Big\}
  \]
  is the set of all the finite linear combinations of elements (i.e.\ all but finitely many of the $\lambda$ are zero) of $S$.
\end{definition}

\begin{remark}
  $\spans S$ is the smallest subspace of $V$ containing all elements of $S$.
\end{remark}

\begin{convention}
  $\spans \emptyset = \{\V 0\}$
\end{convention}

\begin{eg}\leavevmode
  \begin{enumerate}
 \item $V=\R^3$, $S = \{(1,0,0),(0,1,2),(3,-2,-4)\}$, $\spans S = \{(a,b,2b): a,b\in \R \}$
 \item For any set $X$, $\R^X$ is a vector space. For $x \in X$, define $\delta_x: X \to \R, \delta_x(x) = 1, \delta_x(y) = 0 \: \forall y \neq x$, then
   \[
     \spans{\delta_x: x\in X} = \{f\in \R^X: f \text{ has finite support} \}
   \]
  \end{enumerate} 
\end{eg}

\begin{definition}[Span]
  $S$ spans $V$ if $\spans S = V$.
\end{definition}

\begin{definition}[Finite-dimensional]
  $V$ is \emph{finite-dimensional} over $\F$ if it is spanned by a finite set.
\end{definition}

\begin{definition}[Linear independence]
  The vectors $v_1,\ldots, v_n$ are \emph{linearly independent} over $\F$ if
  \[
    \sum_{i=1}^n \lambda_i = 0 \Rightarrow \lambda_i = 0 \: \forall i
  \]
  A subset $S \subset V$ is \emph{linearly independent} if every finite subset of $S$ is linearly independent.

  A subset if \emph{linearly dependent} if it is not linearly independent.
\end{definition}

\begin{eg}
  In the first example above, the three vectors are not linearly independent.
\end{eg}

\begin{ex}
  The set $\{\delta_x: x \in X\}$ is linearly independent.
\end{ex}

\begin{definition}[Basis]
  $S$ is a \emph{basis} of $V$ if it is linearly independent and spans $V$.
\end{definition}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item $\F^n$ has standard basis $\{e_1,e_2,\ldots,e_n\}$ where $e_i$ is the column vector with $1$ in the $i$th entry and $0$ elsewhere.
  \item $V=\C$ over $\C$ has natural basis $\{1\}$, but over $\R$ it has natural basis $\{1, i\}$.
  \item $V=P(\R)$, the space of real polynomials, has natural basis
    \[
      \{1, x, x^2, \dots \}.
    \]
    It is an exercise to check this carefully.
    \end{enumerate}
\end{eg}

\begin{lemma}
  Let $V$ be a $\F$-vector space. The vectors $v_1,\ldots,v_n$ form a basis of $V$ if and only if each vector $v\in V$ has a unique expression
  \[
    v = \sum_{i=1}^n \lambda_i v_i, \lambda_i \in \F.
  \]
  
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item $\Rightarrow:$ Fix $v\in V$. The $v_i$ span $V$, so exists $\lambda_i \in \F$ such that $v = \sum \lambda_i v_i$. Suppose also $v = \sum \mu_i v_i$ for some $\mu_i \in \F$. Then the difference
  \[
    \sum (\mu_i - \lambda_i) v_i = \V 0.
  \]
  Since the $v_i$ are linearly independent, $\mu_i-\lambda_i = 0$ for all $i$.
\item $\Leftarrow:$ The $v_i$ span $V$ by assumption. Suppose $\sum_{i=1}^n \lambda_i v_i = \V 0$. Note that $\V 0 = \sum_{i=0}^n 0 \cdot v_i$. By appying uniqueness to $\V 0$, $\lambda_i = 0$ for all $i$.
  \end{itemize}
\end{proof}

\begin{lemma}
  If $v_1,\ldots, v_n$ spans $V$ over $\F$, then some subset of $v_1,\ldots,v_n$ is a basis of $V$ over $\F$.
\end{lemma}

\begin{proof}
  If $v_1,\ldots, v_n$ is linearly independent then done. Otherwise for some $\ell$, there exist $\alpha_1, \ldots, \alpha_{\ell-1} \in \F$ such that
  \[
    v_\ell = \sum_{i=1}^{\ell-1} \alpha_i v_i.
  \]
  (If $\sum \lambda_i v_i = 0$, not all $\lambda_i$ is zero. Take $\ell$ maximal with $\lambda_\ell \neq 0$, then $\alpha_i = -\frac{\lambda_i}{\lambda_\ell}$.)

  Now $v_1,\ldots,v_{\ell-1},v_{\ell+1},\ldots,v_n$ still span $V$. Continue iteratively until we have linear independence.
\end{proof}

\begin{theorem}[Steinitz Exchange Lemma]
  Let $V$ be a finite-dimensional vector space over $\F$. Take $v_1,\ldots,v_m$ to be linearly independent, $w_1,\ldots,w_n$ to span $V$. Then
  \begin{itemize}
  \item $m \leq n$, and
    \item reordering the $w_i$ if needed, $v_1,\ldots, v_m, w_{m+1},\ldots,w_n$ spans $V$.
  \end{itemize}
\end{theorem}

\begin{proof}
  Proceed by induction. Suppose that we have replaced $\ell \geq 0$ of the $w_i$. Reordering $w_i$ if needed, $v_1,\ldots,v_\ell,w_{\ell+1},\ldots,w_n$ spans $V$.
  \begin{itemize}
  \item If $\ell = m$, done.
  \item If $\ell < m$, then $v_{\ell+1} = \sum_{i=1}^\ell \alpha_i v_i + \sum_{i> \ell} \beta_i w_i$. As the $v_i$ are linearly independent, $\beta_i \neq 0$ for some $i$. After reordering, $\beta_{\ell+1} \neq 0$,
    \[
      w_{\ell+1} = \frac{1}{\beta_{\ell+1}} (v_{\ell+1}-\sum_{i\leq \ell} \alpha_i v_i - \sum_{i>\ell+1} \beta_i w_i).
    \]
    Thus $v_1,\ldots, v_\ell, v_{\ell+1},w_{\ell+2},\ldots, w_n$ also spans $V$. After $m$ steps, we will replace $m$ of the $w_i$ by $v_i$. Thus $m \leq n$.
  \end{itemize}
\end{proof}

\subsection{Dimension}

\begin{theorem}
  If $V$ is a finite-dimensional vector space over $\F$, then any two bases for $V$ have the same cardinality, which is called the \emph{dimension} of $V$, donoted $\dim_\F V$.
\end{theorem}

\begin{proof}
  If $v_1,\ldots, v_n$ and $w_1,\ldots,w_m$ are both bases, then $\{v_i\}$ is linearly independent and $\{w_i\}$ spans $V$ so $n \leq m$. Similarly $m \leq n$.
\end{proof}

\begin{eg}
  $\dim_\C \C = 1$, but $\dim_\R \C = 2$.
\end{eg}

\begin{lemma}
  Let \(V\) be a finite-dimensional \(\F\)-vector space. If \(w_1,\ldots,w_\ell\) is a linearly independent set of vectors, we can extend it to a basis \(w_1,\ldots,w_\ell,w_{\ell+1},\ldots,w_n\).
\end{lemma}

\begin{proof}
  Apply Steinitz exchange lemma to \(w_1,\ldots, w_\ell\) and any basis \(v_1,\ldots, v_n\).

  Or more direcly, if \(V=\langle w_1,\ldots, w_\ell \rangle\), done. Otherwise take \(v_{\ell+1} \in V\setminus\langle w_1,\ldots, w_\ell\rangle\). Now \(w_1,\ldots, w_\ell,w_{\ell+1}\) is linearly independent. Iterate.
\end{proof}

\begin{corollary}
  Let \(V\) be a finite-dimensional vector space of dimension \(n\). Then
  \begin{enumerate}
  \item Any linearly independent set of vectors has at most \(n\) elements, with equality if and only if the set is a basis.
  \item Any spanning set of vectors has at least \(n\) elements, with equaility if and only if the set is a basis.
  \end{enumerate}
\end{corollary}

\begin{slogan}
  Choose the best basis for the job.
\end{slogan}

\begin{theorem}
  Let \(U, W\) be subspaces of \(V\). If \(V\) and \(W\) are finite-dimensional, so is \(U+W\) and
  \[
\dim(U+W) = \dim U + \dim W - \dim(U\cap W).
  \]
\end{theorem}

\begin{proof}
  Pick basis \(v_1,\ldots, v_\ell\) of \(U\cap W\). Extend it to basis \(v_1,\ldots,v_\ell,u_1,\ldots,u_m\) of \(U\) and \(v_1,\ldots,v_\ell,w_1,\ldots,w_n\) of \(W\). Claim \(v_1,\ldots, v_\ell,u_1,\ldots,u_m,w_1,\ldots,w_n\) is a basis for \(U+W\):
  \begin{itemize}
  \item spanning: if \(u\in U\), then \(u= \sum \alpha_iv_i + \sum \beta_iu_i\) and if \(w\in W\), \(w = \sum_{}^{}\gamma_iv_i + \sum_{}^{}\delta_iw_i\), so \(u+w = \sum_{}^{}(\alpha_i + \gamma_i)v_i + \sum_{}^{}\beta_iu_i + \sum_{}^{}\delta_iu_i\).
  \item linear independence: assume \(\sum \alpha_iv_i + \sum \beta_iu_i+ \sum \gamma_iw_i=0\). Rearrange, \(\sum\alpha_iv_i + \sum\beta_iu_i = -\sum\gamma_iw_i \in U\cap W\) so it equals to \(\sum\delta_iv_i\) for some \(\delta_i\in \F\) because \(v_i\) is a basis for \(U\cap W\). As \(v_i\) and \(w_i\) are linearly independent, \(\gamma_i=\delta_i=0\) for all \(i\). Thus \(\sum\alpha_iv_i + \sum\beta_iv_i=0\), so \(\alpha_i=\beta_i=0\) since \(v_i\) and \(u_i\) form a basis for \(U\).
  \end{itemize}
\end{proof}

\begin{theorem}
  Let \(V\) be a finite-dimensional vector space over \(\F\) and \(U \leq V\), then \(U\) and \(V/U\) are also finite dimensional and
  \[
\dim V = \dim U + \dim V/U.
  \]
\end{theorem}

\begin{proof}
  Left as an exercise. Outline: first show \(U\) is finite-dimensional, then let \(u_1,\ldots,u_\ell\) be a basis for \(U\). Extend it to a basis for \(V\), say \(u_1,\ldots,u_\ell,w_{\ell+1},\ldots,w_n\) of \(V\). Check \(w_{\ell+1}+U,\ldots,w_n+U\) form a basis for \(V/U\).
\end{proof}

\begin{corollary}
  If \(U\) is a proper subspace of \(V\), which is finite-dimensional, then \(\dim U < \dim V\).
\end{corollary}

\begin{proof}
  \(V/U \neq 0\) so \(\dim V/U > 0\).
\end{proof}

\subsection{Direct Sum}

\begin{definition}[Direct sum]
  Let \(V\) be a vector space over \(\F\), \(U, W\leq V\). Then
  \[
V = U \oplus W
  \]
  if every element of \(V\) can be written as \(v=u+w\) for some unique \(u\in U, w\in W\). This is called the \emph{internal direct sum}. \(W\) is a \emph{direct complement} of \(U\) in \(V\).
\end{definition}

\begin{lemma}
  Suppose \(U,W\leq V\), TFAE:
  \begin{enumerate}
  \item \(V = U \oplus W\),
  \item \(V=U+W\) and \(U\cap W = 0\),
  \item Given \(\basis B_1\) any basis of \(U\), \(\basis B_2\) any basis of \(V\), \(\basis B = \basis B_1\cup \basis B_2\) is a basis of \(V\).
  \end{enumerate}
\end{lemma}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item \(2 \Rightarrow 1\): any \(v\in V\) is \(u+w\) for some \(u\in U, w\in W\). Suppose \(u_1+w_1=u_2+w_2\), then \(u_1-u_2 = w_2-w_1 \in U\cap W = 0\). Thus \(u_1=u_2,w_1=w_2\).
  \item \(2 \Rightarrow 1\): \(\basis B\) spans as any \(v\in V\) is \(u+w\). Write \(u\) in terms of \(\basis B_1\) and \(w\) in terms of \(\basis B_2\). Then \(u+w\) is a linear combination of elements of \(\basis B\). To show \(\basis B\) is linearly independent, suppose \(\sum_{v\in \basis B} \lambda_v v = \V 0 = \V 0_V + \V 0_W\). Write LHS as \(\sum_{v\in \basis B_1} \lambda_vv + \sum_{v\in \basis B_2}\lambda_vv\). By uniqueness of expression, \(\sum_{v\in \basis B_1}\lambda_vv=\V 0_V\) and \(\sum_{w\in \basis B_2}\lambda_ww=\V 0_w\). As \(\basis B_1, \basis B_2\) are bases, all of the \(\lambda_v, \lambda_w\) are zero.
  \item \(2 \Rightarrow 1\): if \(v\in V, v=\sum_{x\in V}\lambda_xx = \sum_{u\in B_1}\lambda_uu + \sum_{w\in \basis B_1}\lambda_ww\) so \(v\in U+W\). Conversely, if \(v\in U\cap W, v = \sum_{u\in \basis B_1}\lambda_uu=\sum_{w\in \basis B_2}\lambda_ww\) so all \(\lambda_u, \lambda_v\) are zero since \(\basis B_1\cup \basis B_2\) is linearly independent.
  \end{enumerate}
\end{proof}

\begin{lemma}
  Let \(V\) be a finite-dimensional vector space over \(\F\) and \(U\leq V\). Then there exists a direct complement to \(U\) in \(V\).
\end{lemma}

\begin{proof}
  Let \(u_1,\ldots, u_\ell\) be a basis for \(U\). Extend this to a basis \(u_1,\ldots, u_\ell,w_{\ell+1},\ldots,w_n\) for \(V\). Then \(\spans{w_{\ell+1},\ldots,w_n}\) is a direct complement of \(U\).
\end{proof}

\begin{caution}
  Direct complements are \emph{not} unique.
\end{caution}

\begin{definition}
  Suppose \(V_1,\ldots, V_\ell \leq V\), then
  \[
\sum_i V_i = V_1+\cdots+V_\ell = \{v_1+\cdots+v_\ell: v_i\in V_i\}.
  \]
  The direct sum is direct if
  \[
    v_1+\cdots+v_\ell = v_1'+\cdots+ v_\ell' \Rightarrow v_i = v_i' \text{ for all } i.
  \]
  It is denoted
  \[
V = \bigoplus_{i=1}^\ell V_i.
  \]
\end{definition}

\begin{ex}
  \(V_1,\ldots, V_\ell \leq V\), TFAE:
  \begin{enumerate}
  \item The sum \(\sum_i V_i\) is direct,
  \item \(V_i \cap \sum_{j\neq i}V_j = 0\) for all \(i\),
  \item For any basis \(B_i\) of \(V_i\), the union \(B=\bigcup_{i=1}^\ell B_i\) is a basis for \(\sum_i V_i\).
  \end{enumerate}
\end{ex}

\begin{definition}[Direct Sum]
  Let \(U, W\) be vector spaces over \(\F\). The \emph{external direct sum} is
  \[
U\oplus W = \{(u,w): u\in U, w\in W\}
  \]
  with pointwise addition and scalar multiplication.
\end{definition}

\section{Linear Maps}

\subsection{Definition}

\begin{definition}[Linear map]
  \(V, W\) two \(\F\)-vector space, a map \(\alpha: V\to W\) is \emph{linear} if
  \begin{itemize}
  \item \(\alpha(v_1 + v_2) = \alpha(v_1) + \alpha(v_2)\),
  \item \(\alpha(\lambda v) = \lambda \alpha(v)\).
  \end{itemize}
  This is equivalent to
  \[
\alpha(\lambda_1v_1+ \lambda_2v_2) = \lambda_1\alpha(v_1) + \lambda_2\alpha(v_2).
  \]
\end{definition}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Given an \(n\times m\) matrix \(A\) with coefficients in \(\F\), the map \(\alpha: \F^m\to \F^n, v\to Av\).
  \item Differentiation \(D: P(\R) \to P(\R), f\mapsto \frac{df}{dx}\).
  \item Integration \(I: C[0,1] \to C[0,1], f\to I(f)\) where \(I(f)(x) = \int_0^x f(t)dt\).
  \item Fix \(x\in [0,1]\), the map \(C[0,1]\to \R, f\mapsto f(x)\).
  \end{enumerate}
\end{eg}

\begin{note}[Category of \(\mathbf{Vect}_\F\)]
  Suppose \(U, V, W\) are \(\F\)-vector spaces, then
  \begin{enumerate}
  \item \(\id: V\to V\) is linear.
  \item \(U \stackrel{\alpha}{\to} V \stackrel{\beta}{\to} W\), if \(\alpha, \beta\) are linear then so is \(\beta \compose \alpha\).
  \end{enumerate}
\end{note}

\begin{lemma}[Free functor \(\mathbf{Set} \to \mathbf{Vect}_\F\)]
  Suppose \(V, W\) are \(\F\)-vector spaces and \(\basis B\) is a basis for \(V\). If \(\alpha_0: \basis B\to W\) is \emph{any} map, then there is a \emph{unique} linear map \(\alpha: V\to W\) extending \(\alpha_o\).
\end{lemma}

\begin{proof}
  Let \(v\in V\). Write \(v = \sum \lambda_iv_i\) in a unique way. By linearity \(\alpha(v) = \alpha(\sum \lambda_iv_i) = \sum \lambda_i \alpha(v_i) = \sum \lambda_i \alpha_0(v_i)\). Uniqueness follows.
\end{proof}

\begin{note}\leavevmode
  \begin{itemize}
  \item This is true for infinite-dimensional vector spaces as well.
  \item Very often, to define a linear map, define it on a basis and extend it linearly to the vector space.
  \item Two linear maps \(\alpha_1,\alpha_2: V\to W\) are equal if and only if they agree on a basis.
  \end{itemize}
\end{note}

\subsection{Isomorphism of Vector Spaces}

\begin{definition}[Isomorphism]
  Given \(V, W\) two \(\F\)-vector spaces, the map \(\alpha:V\to W\) is an \emph{isomorphism} if it is linear and bijective, denoted \(V \cong W\).
\end{definition}

\begin{lemma}
  \(\cong\) is an equivalence relation on the class of all \(\F\)-vector spaces.
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item symmetric: obvious.
  \item reflexive: blah blah in lecture. Left as an exercise to reader.
  \item transitive: obvious.
  \end{itemize}
\end{proof}

\begin{theorem}
  If \(V\) is an \(\F\)-vector space, then \(V \cong \F^n\) for some \(n\).
\end{theorem}

\begin{proof}
  Choose a basis for \(V\), say \(v_1,\ldots, v_n\). Define a map
  \begin{align*}
    V &\to \F^n \\
    \sum_{i}^{ }\lambda_iv_i &\mapsto (\lambda_1,\ldots,\lambda_n)
  \end{align*}
which is an isomorphism.
\end{proof}

\begin{remark}
  Choosing an isomorphism \(V \cong \F^n\) is equivalent to choosing a basis for \(V\). i.e.\ there is a bijection \(\{\alpha\in\Hom(V,\F^n), \alpha\text{ bijective}\} \leftrightarrow \{\text{bases of } V\} \).
\end{remark}

\begin{theorem}
  Given two finite-dimensional \(\F\)-vector spaces \(V, W\), they are isomorphic if and only if they have the same dimension.
\end{theorem}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(\Leftarrow\): \(V \cong \F^{\dim V} = \F^{\dim W} \cong W\).
  \item \(\Rightarrow\): let \(a:V\to W\) be an isomorphism and \(\basis B\) be a basis for \(V\). Claim \(\alpha(\basis B)\) is a basis for \(W\): \(\alpha(\basis B)\) spans \(W\) due to surjectivity and \(\alpha(\basis B)\) is linearly independent due to injectivity.
  \end{itemize}
\end{proof}

\begin{definition}[Kernel \& Image]
  Given \(\alpha: V\to W\),
  \begin{itemize}
  \item \(N(\alpha) = \ker \alpha = \{v\in V: \alpha(v) = 0\} \leq V\),
  \item \(\im \alpha = \{w\in W: \exists v\in V, \alpha(v) = w \} \leq W\).
  \end{itemize}
\end{definition}

\begin{proposition}\leavevmode
  \begin{itemize}
  \item \(\alpha\) is injective if and only if \(N(\alpha) = 0\),
  \item \(\alpha\) is surjective if and only if \(\im \alpha = W\).
  \end{itemize}
\end{proposition}

\begin{proof}
  Easy.
\end{proof}

\begin{eg}
  Let \(\alpha: C^\infty(\R) \to C^\infty(\R), \alpha(f)(t) = f''(t)+2f'(t)+5f(t)\). \(\ker \alpha = \{f:f''+2f'+5f=0\}\) and \(g\in \im \alpha\) if and only if there exists an \(f\) such that \(f''+2f'+5f=g\).
\end{eg}

\begin{theorem}[First Isomorphism Theorem]
  Let \(\alpha: V\to W\) be a linear map. It induces an isomprhism
  \begin{align*}
    \bar \alpha: V/\ker \alpha &\to \im \alpha \\
    v + \ker \alpha &\mapsto \alpha(v)
  \end{align*}
\end{theorem}

\begin{proof}
  Check the following:
  \begin{itemize}
  \item \(\bar \alpha\) is well-defined,
  \item \(\bar \alpha\) is linear: immediate from linearity of \(\alpha\),
  \item \(\bar \alpha\) is surjective.
  \end{itemize}
\end{proof}

\begin{definition}\leavevmode
  \begin{itemize}
  \item \(r(\alpha) = rk(\alpha) = \dim( \im \alpha)\) is the \emph{rank} of \(\alpha\),
  \item \(n(\alpha) = \dim N(\alpha)\) is the \emph{nullity} of \(\alpha\).
  \end{itemize}
\end{definition}

\begin{theorem}[Rank-nullity]
  Let \(U, V\) be \(\F\)-vector spaces, \(\dim U < \infty\). Let \(\alpha:U\to V\) be a linear map. Then
  \[
\dim U = r(\alpha) + n(\alpha).
  \]
\end{theorem}

\begin{proof}
  \(U/\ker \alpha \cong \im \alpha\) so \(\dim U - \dim (\ker \alpha) = \dim (\im \alpha)\). Rearrange.
\end{proof}

\begin{lemma}
  Let \(V, W\) be \(\F\)-vector spaces with equal, finite dimension. Let \(\alpha:V\to W\) be linear, then TFAE:
  \begin{enumerate}
  \item \(\alpha\) is injective,
  \item \(\alpha\) is surjective,
  \item \(\alpha\) is an isomorphism.
  \end{enumerate}
\end{lemma}

\begin{proof}
  Rank-nullity theorem.
\end{proof}

%\subsection{\texorpdfstring{\(\Hom_\F(V,W)\)}{Hom\textsubscript{F}(V,W)} as \texorpdfstring{\(\F\)}{F}-vector space}
\subsection{Linear Maps as Vector Space}

Suppose \(V\) and \(W\) are \(\F\)-vector spaces. Let \(L(V,W) = \{\alpha:V\to W, \alpha \text{ linear}\}\).

\begin{proposition}
  \(L(V,W)\) is an \(\F\)-vector space, under operations
  \begin{align*}
    (\alpha_1+\alpha_2)(v) &= \alpha_1(v) + \alpha_2(v) \\
    (\lambda\alpha)(v) &= \lambda(\alpha(v))
  \end{align*}
\end{proposition}

\begin{proof}
  \(\alpha_1+\alpha_2, \lambda\alpha\) as above are well-defined linear maps. The vector space axioms can be easily checked.
\end{proof}

\begin{proposition}
  \label{prop:dimension of linear map space}
  If both \(V\) and \(W\) are finite-dimensional over \(\F\) then so is \(L(V,W)\) and \(L(V,W) = \dim V \cdot \dim W\).
\end{proposition}

\begin{proof}
  See Lemma~\ref{cor:dim of hom}.
\end{proof}

\subsubsection{Matrices, an Interlude}

\begin{definition}[Matrix]
  An \emph{\(m\times n\) matrix} over \(\F\) is an array with \(m\) rows and \(n\) columns with entries in \(\F\). We write
  \[
A = (a_{ij}), a_{ij}\in\F, 1\leq i \leq m, 1\leq j \leq n.
  \]
\end{definition}

\begin{definition}
  \(\M_{m,n}(\F)\) is the set of all such \(m\times n\) matrices.
\end{definition}

\begin{proposition}
  \(\M_{m,n}(\F)\) is an \(\F\)-vector space and \(\dim \M_{m,n}(\F) = m\cdot n\).
\end{proposition}

\begin{proof}
  See Example~\ref{eg:matrix as V} for the proof of vector space axioms. For the dimension claim, a standard basis for \(\M_{m,n}(F)\) is
  \[
    E_{ij}=
    \begin{pmatrix}
      0 & \dots & 0 \\
      \vdots & \ddots & \vdots \\
      0 & 1 & 0 \\
      \vdots & \ddots & \vdots \\
      0 & \dots & 0 
    \end{pmatrix}
  \]
  with \(1\) in the \((i,j)\)th entry so \(a_{ij} = \sum_{i,j}^{} a_{ij}E_{ij}\), from which span and linear independence follow. The basis has cardinality \(m\cdot n\).
\end{proof}

\subsubsection{Representation of Linear Maps by Matrices}

Let \(V\) and \(W\) be finite-dimensional \(\F\)-vector space, \(\alpha: V\to W\) linear. Let \(\basis B = \{v_1,\ldots,v_n\}\) be a basis for \(V\), \(\basis C = \{w_1,\ldots,w_m\}\) be a basis for \(W\). If \(v=\sum_{i}\lambda_iv_i \in V\), write
\[
[v]_{\basis B} =
\begin{pmatrix}
  \lambda_1 \\
  \vdots \\
  \lambda_n
\end{pmatrix}
\in \F^n
\]
which is called the \emph{coordinate vector of \(v\) with respect to \(\basis B\)}. Similarly \([w]_{\basis C}\in \F^m\).

\begin{definition}[Matrix representation]
  \([\alpha]_{\basis B, \basis C}\) is the matrix representation of \(\alpha\) with respect to \(\basis B\) and \(\basis C\) with
  \begin{align*}
    [\alpha]_{\basis B, \basis C} &= \Big( [\alpha(v_1)]_{\basis C} \: \Big| \: [\alpha(v_2)]_{\basis C} \: \Big | \: \cdots \: \Big| \: [\alpha(v_n)]_{\basis C} \Big) \\
    &= (a_{ij})
  \end{align*}
  The matrix says
  \[
\alpha(v_j) = \sum_{i}^{ }a_{ij}w_i.
  \]
\end{definition}

\begin{lemma}
  For any \(v\in V\),
  \[
[\alpha(v)]_{\basis C} = [\alpha]_{\basis B, \basis C}\cdot [v]_{\basis B}
  \]
  where \(\cdot\) is matrix multiplication applied to vectors.
\end{lemma}

\begin{proof}
  Fix \(v =\sum_{j=1}^{n}\lambda_jv_j \in V\), so
  \[
[v]_{\basis B} =
\begin{pmatrix}
  \lambda_1 \\
  \vdots \\
  \lambda_n
\end{pmatrix}
\]
\begin{align*}
  \alpha(v) &= \alpha\Big( \sum_{j}^{ }\lambda_jv_j \Big) \\
            &= \sum_{j}^{ }\lambda_j\alpha(v_j) \\
            &= \sum_{j}^{ }\lambda_j\Big( \sum_{i}^{ }\alpha_{ij}w_i \Big) \\
            &= \sum_{i}^{ }\Big( \sum_{j}^{} \alpha_{i,j}\lambda_j \Big) w_i
\end{align*}
so the \(i\)th entry of \([\alpha]_{\basis B, \basis C}\) is \([v]_{\basis B}\).
\end{proof}

\begin{lemma}
  Suppose \(U \stackrel{\beta}{\to} V \stackrel{\alpha}{\to} W\) with \(\alpha, \beta\) linear, with \(\alpha\compose\beta: U\to W\). Let \(\basis A, \basis B, \basis C\) be bases for \(U,V,W\) respectively. Then
  \[
[\alpha\compose\beta]_{\basis A, \basis C} = [\alpha]_{\basis B,\basis C}\cdot[\beta]_{\basis A, \basis B}.
  \]
\end{lemma}

\begin{proof}
  \begin{align*}
    (\alpha\compose\beta)(u_\ell) &= \alpha(\beta(u_\ell)), \: u_\ell\in A\\
                                  &= \alpha\Big( \sum_{j}^{ }b_{jl}v_j \Big), \: v_j\in B \\
                                  &= \sum_{j}^{ }b_{jl}\alpha(v_j) \\
                                  &= \sum_{j}^{ }b_{jl}\sum_{i}^{ }a_{ij}w_i, \: w_i\in W \\
                                  &= \sum_{i}^{ }\Big( \sum_{j}^{ }a_{ij}b_{jl})w_i
  \end{align*}
\end{proof}

\begin{proposition}
  Let \(V\) and \(W\) be \(\F\)-vector spaces with \(\dim V = n, \dim W = m\), then
  \[
L(V,W) \cong \M_{m,n}(\F).
  \]
\end{proposition}

\begin{proof}
  Fix bases \(B=\{v_1\ldots,v_n\}, C=\{w_1,\ldots,w_m\}\) for \(V\) and \(W\) respectively. Claim
  \begin{align*}
    \theta: L(V,W) &\to \M_{m,n}(\F) \\
    \alpha &\mapsto [\alpha]_{\basis B, \basis C}
  \end{align*}
  is an isomorphism:
  \begin{itemize}
  \item linearity: \([\lambda_1\alpha_1+\alpha_2\alpha_2]_{\basis B, \basis C} = \lambda_1[\alpha_1]_{\basis B, \basis C} + \lambda_2[\alpha_2]_{\basis B, \basis C}\).
  \item surjectivity: given \(A = (a_{ij})\), let \(\alpha:v_j\mapsto \sum_{i=1}^{m}a_{ij}w_i \) and extend linearly. It follows that \(\alpha\in L(V,W)\) and \(\theta(\alpha) = A\).
  \item injectivity: \([\alpha]_{\basis B, \basis C} = \V 0\) implies that \(\alpha\) is the zero map.
  \end{itemize}
\end{proof}

\begin{corollary}
  \label{cor:dim of hom}
  \[
\dim L(V,W) = \dim V \cdot \dim W.
  \]
\end{corollary}

\begin{eg}
  Suppose \(\alpha:V\to W\), \(Y\leq V, Z\leq W\) with \(\alpha(Y)\leq Z\). Let \(\basis B'=\{v_1,\ldots,v_k\}\) be a basis of \(Y\) and extend to \(\basis B=\{v_1,\ldots,v_k,v_{k+1},v_n\}\) a basis for \(V\). Similarly \(\basis C' = \{w_1,\dots,w_l\}\) and \(\basis C\) for \(Z\) and \(W\).
  \begin{itemize}
  \item \([\alpha]_{\basis B, \basis C} =
\begin{pmatrix}
  A & B \\
  0 & C
\end{pmatrix}
\) for some \(A, B, C\) because for \(1\leq j \leq k\), \(\alpha(v_j)\) is a linear combination of \(w_i\) where \(1\leq i \leq l\).
  \item \([\alpha|_Y]_{B',C'} = A. \)
  \item \(\alpha\) induces a map
    \begin{align*}
      \bar\alpha: V/Y &\to W/Z \\
      v+ Y &\mapsto \alpha(v) + Z
    \end{align*}
    This is well-defined. Linearity follows from that of \(\alpha\). A basis for \(V/Y\) is \(\basis B''=\{v_{k+1}+Y,\ldots,v_n+Y\}\) and similarly for \(W/Z\). It is an exercise to show \([\bar\alpha]_{\basis B'', \basis C''} = C \).
  \end{itemize}
\end{eg}

\subsubsection{Change of Bases}

Throughout this section, let \(V\) and \(W\) be \(\F\)-vector spaces and suppose they have the following bases:
\begin{table}[htbp]
  \centering
  \begin{tabular}{|c||c|c|}
    \hline
    Vector space & \(V\) & \(W\) \\ \hline
    Basis 1 & \(\basis B = \{v_1,\dots,v_n\}\) & \(\basis C = \{w_1,\dots,w_m\}\) \\ \hline
    Basis 2 & \(\basis B' = \{v_1',\dots,v_n'\}\) & \(\basis C' = \{w_1',\dots,w_m'\}\) \\ \hline
  \end{tabular}
\end{table}

\begin{definition}[Change-of-basis matrix]
  The \emph{change-of-basis matrix} from \(\basis B'\) to \(\basis B\) is \(P = (p_{ij})\) given by
  \begin{align*}
    v_j' &= \sum_{i}^{ }p_{ij}v_i \\
    P &= \Big( [v_1']_{\basis B} \: \Big| \: [v_2']_{\basis B} \: \Big| \: \dots \Big| \: [v_n']_{\basis B} \Big) = [\id]_{\basis B', \basis B}
  \end{align*}
\end{definition}

\begin{lemma}
  \[
    [v]_{\basis B} = P[v]_{\basis B'}.
  \]
\end{lemma}

\begin{proof}
  \[
    P[v]_{\basis B'} = [\id]_{\basis B', \basis B}[v]_{\basis B'} = [v]_{\basis B}.
  \]
\end{proof}

\begin{lemma}
  \(P\) is an invertible \(n\times n\) matrix and \(P^{-1}\) is the change-of-basis matrix from \(\basis B\) to \(\basis B'\).
\end{lemma}

\begin{proof}
  \begin{align*}
    [\id]_{\basis B, \basis B'}[\id]_{\basis B', \basis B} &= [\id]_{\basis B', \basis B'} = I_n \\
    [\id]_{\basis B', \basis B}[\id]_{\basis B, \basis B'} &= [\id]_{\basis B, \basis B} = I_n
  \end{align*}
\end{proof}

Let \(Q\) be the change-of-basis matrix from \(\basis C'\) to \(\basis C\). Then \(Q\) is an invertible \(m\times m\) matrix.

\begin{proposition}
  Let \(\alpha: V\to W\) be a linear map, \(A = [\alpha]_{\basis B,\basis C}\), \(A' = [\alpha]_{\basis B',\basis C'}\), then
  \[
    A' = Q^{-1}AP.
  \]
\end{proposition}

\begin{proof}
  \[
    \underbrace{[\id]_{\basis C,\basis C'}}_{Q^{-1}} [\alpha]_{\basis B,\basis C} \underbrace{[\id]_{\basis B',\basis B}}_P = \underbrace{[\id\compose\alpha\compose\id]_{\basis B',\basis C'}}_{A'}
  \]
\end{proof}

\begin{definition}[Equivalence of matrices]
  \(A, A' \in \M_{m,n}(\F)\) are \emph{equivalent} if
  \[
    A' = Q^{-1}AP
  \]
  for some invertible \(P\in \M_{n,n}(\F)\) and \(Q\in \M_{m,m}(\F)\).
\end{definition}

\begin{note}
  This defines an equivalence relation on \(\M_{m,n}(\F)\).
\end{note}

\begin{proposition}
  Let \(V, W\) be \(\F\)-vector spaces of dimension \(n\) and \(m\) respectively. Let \(\alpha:V\to W\) be a linear map. Then there exist bases \(\basis B\) of \(V\), \(\basis C\) of \(W\), and some \(r\leq m,n\) such that
  \[
    [\alpha]_{\basis B,\basis C} =
    \begin{pmatrix}
      I_r & 0 \\
      0 & 0 
    \end{pmatrix}
  \]
  where \(I_r\) the is \(r\times r\) the identity matrix.
\end{proposition}

\begin{note}
  \(r = rk(\alpha) = r(\alpha)\).
\end{note}

\begin{proof}
  Fix \(r\) such that \(\dim N(\alpha) = n-r\). Fix a basis for \(N(\alpha)\), say \(v_{r+1},\dots,v_n\). Extend this to a basis \(\basis B\) for \(V\), say \(v_1,\dots,v_r,v_{r+1},\dots,v_n\). Now \(\alpha(v_1),\dots,\alpha(v_r)\) is a basis for \(\im(\alpha)\):
  \begin{itemize}
  \item span: \(\alpha(v_1),\dots, \alpha(v_n)\) certainly span \(\im(\alpha)\). Since \(v_{r+1},\dot,v_n \in \ker \alpha\), \(\alpha(v_{r+1}),\dots,\alpha(v_n) = 0\) so we can remove them from the spanning set.
  \item linear independence: assume \(\sum_{i=1}^{n}\lambda_i \alpha(v_i) =\V 0 \). Then \(\alpha \big(\sum_{i=1}^n\lambda_iv_i\big) =\V0\). This implies that
    \[
      \sum_{i=1}^{n}\lambda_iv_i = \sum_{j=r+1}^{n}\mu_jv_j.
    \]
    As \(v_1,\dots v_n\) are linearly independent, \(\lambda_i=\mu_j=0\) for all \(i,j\).
  \end{itemize}
  Extend \(\alpha(v_1),\dots,\alpha(v_r)\) to a basis for \(W\), say \(\basis C\). By construction,
  \[
    [\alpha]_{\basis B,\basis C} =
    \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}
  \]
\end{proof}

\begin{remark}
  In the proof above we didn't need to assume that \(r = r(\alpha)\). This gives us another way prove Rank-nullity Theorem.
\end{remark}

\begin{corollary}
  Any \(m\times n\) matrix is equivalent to
  \[
  \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}
  \]
  for some \(r\).
\end{corollary}

\begin{definition}[Row and column rank]
  Let \(A\in \M_{m,n}(\F)\).
  \begin{itemize}
  \item The \emph{column rank} of \(A\), \(r(A)\) is the dimension of the subspace of \(\F^m\) spanned by the columns of \(A\).
  \item The \emph{row rank} of \(A\) is the column rank of \(A^T\).
  \end{itemize}
\end{definition}

\begin{note}
  If \(\alpha\) is a linear map represented by \(A\) with respect to any choice of bases, then \(r(\alpha) = r(A)\).
\end{note}

\begin{proposition}
  Two \(m\times n\) matrices \(A, A'\) are equivalent if and only if
  \[
    r(A) = r(A').
  \]
\end{proposition}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(\Leftarrow\): Both \(A\) and \(A'\) are equivalent to \(
    \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}
    \) and matrix equivalence is transitive.
  \item \(\Rightarrow\): Let \(\alpha:\F^n\to \F^m\) be the linear map represented by \(A\) with repect to, say, the standard basis. Since \(A'=Q^{-1}AP\) for some invertible \(P\) and \(Q\), \(A'\) represents the same \(\alpha\) with respect to another bases. \(r(\alpha)\) is defined in a basis-invariant way so \(r(A) = r(\alpha) = r(A')\).
  \end{itemize}
\end{proof}

\begin{theorem}
  \label{thm:upper corner matrix}
  \[
    r(A) = r(A^T).
  \]
\end{theorem}

\begin{proof} \(
    Q^{-1}AP =
    \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}_{n,m}
  \) where \(P\) and \(Q\) are invertible. Take transpose of the whole equation:
  \[
    \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}_{n,m}
    =(Q^{-1}AP)^T = P^TA^T(Q^T)^{-1}
  \]
  so \(A^T\) is equivalent to
  \[
      \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}
  \]
\end{proof}

Note a special case for change of basis: \(V=W\), \(\basis C=\basis B\), the other basis is \(\basis B'\). \(P\), the change-of-basis matrix from \(\basis B'\) to \(\basis B\), given a mpar \(\alpha \in L(V,V)\)
\[
  [\alpha]_{\basis B',\basis B'} = P^{-1}[\alpha]_{\basis B,\basis B}P
\]

\begin{definition}[Similar matrices]
  Given \(A, A' \in \M_{n,n}(\F)\), \(A\) and \(A'\) are \emph{similar}, or \emph{conjugate} if
  \[
    A' = P^{-1}AP
  \]
  for some invertible \(P\).
\end{definition}

\subsubsection{Elementary Matrices and Operations}

\begin{definition}[Elementary column operation]
  \emph{Elementary column operation} on a \(m\times n\) matrix \(A\) is one of the following operations:
  \begin{enumerate}
  \item swap column \(i\) and \(j\) (wlog \(i\neq j\)),
  \item scale column \(i\) by \(\lambda\)  (\(\lambda\neq0\)),
  \item add \(\lambda\) times column \(i\) to column \(j\) (\(i\neq j,\lambda\neq 0\)).
  \end{enumerate}
\end{definition}

\begin{definition}[Elementary row operation]
  Defined analoguously, replacing ``column'' by ``row''.
\end{definition}

\begin{note}
  All of these operations are invertible.
\end{note}

\begin{definition}[Elementary matrix]
The elementary row (column, respectively) have corresponding elementary matrices: th effect of performing these column (row, respectively) operations on \(I_n\) (\((I_m\), respectively):
\begin{enumerate}
\item
  \[
    \begin{pmatrix}
      1 & 0 & \cdots & 0 & 0 & 0\\
      \vdots & \ddots & & & & \vdots\\
      & & 1 & & \cdots & 0\\
      0 & \cdots & 0 & \ddots & 0 & 0 \\
      & & 0 & 0 & 1 & &\\
      \vdots & && \ddots& &\\
      0 & & \cdots & & \cdots & 0
    \end{pmatrix}
  \]
\item
  \[
    \begin{pmatrix}
      1 & 0 & 0 & 0 \\
      \vdots & \ddots & & \vdots \\
      0 & \cdots & \lambda & 0 \\
      \vdots & & & \vdots \\
      0 & \cdots & 0 & 1
    \end{pmatrix}
  \]
\item \(I_n+\lambda E_{ij}\) where \(E_{ij}\) is the matrix with \(1\) on \(ij\)th entry and \(0\) elsewhere.
\end{enumerate}
\end{definition}

An elementary column (row, respectively) operation on \(A\in \M_{m,n}(\F)\) can be performed by multiplying \(A\) by these corresponding elementary matrices on the right (left respectively).

\begin{eg}
  \[
    \begin{pmatrix}
      1 & 2 \\ 3 & 4
    \end{pmatrix}
    \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} = \begin{pmatrix} 2 & 1 \\ 4 & 3 \end{pmatrix}
  \]
\end{eg}

Given the elementary matrices, we can give a constructive proof that any \(m\times n\) matrix is equivalent to \(\begin{pmatrix} I_r & 0 \\ 0 & 0 \end{pmatrix}\) for some \(r\):

\begin{proof}[Constructive proof of Theorem~\ref{thm:upper corner matrix}]
  Start with \(A\). If all entries of \(A\) are zero then done. If not the some \(a_{ij} = \lambda \neq 0\). Perform the following:
  \begin{enumerate}
  \item swap row \(1\) and \(i\), swap column \(1\) and \(j\) so \(\lambda\) is in position \((1,1)\),
  \item multiply column \(1\) by \(1/\lambda\) to get \(1\) in position \((1,1)\),
  \item add \((-a_{1,2})\) times column \(1\) to column \(2\). Do so for the other entries in row \(1\). Also use row operations to clear out all other entries in column \(1\). Now the matrix is in the form
    \[
      \begin{pmatrix}
        1 & 0 \\
        0 & A'
      \end{pmatrix}
    \]
  \item iterate for \(A'\). Stop when the new \(A' = 0\).
  \end{enumerate}
  The result of these operations is
  \[
    \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}
    =\underbrace{E'_{\ell}E'_{\ell-1}\dots E'_1}_{Q^{-1}} A \underbrace{E_1E_2\dots E_{\ell-1}E_\ell}_{P}.
  \]
  As elementary operations are invertible, the elementary matrices are invertible so
  \[
    Q^{-1}AP = 
     \begin{pmatrix}
      I_r & 0 \\
      0 & 0
    \end{pmatrix}
  \]
\end{proof}

Some variations of the algorithm:
\begin{enumerate}
\item If you only use elementary row operations, we can get the \emph{row echelon form} of a matrix:
  \[
    \begin{pmatrix}
      a & b & \dots & c \\
      0 & d & \dots & e \\
      \vdots & & \ddots & \vdots \\
      0 & 0 & \dots & f 
    \end{pmatrix}
  \]
\end{enumerate}

\begin{lemma}
  If \(A\) is an \(n\times n\) invertible matrix then we can obtain \(I_n\) by using only elementary row/column operations.
\end{lemma}

\begin{proof}
  We prove the column operation case. Use induction on \(n\), the number of rows. Suppose we have got \(\begin{pmatrix} I_k & 0 \\ \star & \ast \end{pmatrix}\) for some \(k\geq 0\). There exists \(j>k\) such that \(a_{k+1,j}\neq 0\), i.e.\ in the \(\ast\) block) as otherwise \((0,\dots,1,\dots, 0)\) with \(1\) is \((k+1)\)th position would not be in the span of the column vectors, contradicting the invertiblity. Next we carry out the following operations:
  \begin{enumerate}
  \item swap column \(k+1\) and \(j\),
  \item divide volume \(k+1\) by \(\lambda\) so have \(1\) in \((k+1,k+1)\) position,
  \item use column operation to clear other entries of \((k+1)\)th row.
  \end{enumerate}
  Proceed inductively.
\end{proof}

Note that the equality
\[
  AE_1E_2\dots E_c = I_n
\]
gives
\[
  A^{-1} = E_1E_2\dots E_c,
\]
one way to compute inverses.

\begin{proposition}
  Any invertible matrix can be written as a product of elementary ones.
\end{proposition}

\section{Dual Spaces \& Dual Maps}

\subsection{Definition}

Let \(V\) be an \(\F\)-vector space.

\begin{definition}
  The \emph{dual space} of \(V\) is defined to be
  \[
    V^* = L(V,\F) = \{\alpha:V\to \F, \alpha \text{ linear}\}.
  \]
\end{definition}

\(V^*\) is itself an \(\F\)-vector space. Its elements are sometimes called \emph{linear functionals}.

\begin{eg}\leavevmode
  \begin{enumerate}
  \item \(\R^3 \to \R, (a,b,c)\mapsto a-c\) is an element of \(V^*\).
  \item \(\tr: \M_{n,n}(\F)\to \F, A\mapsto \sum_i A_{ii}\) is an element of \(\M_{n,n}(\F)^*\).
  \end{enumerate}
\end{eg}

\begin{lemma}[Dual basis]
  Let \(V\) be a finite-dimensional \(\F\)-vector space with basis \(\basis B = \{e_1,\dots,e_n\}\). Then there is a basis for \(V^*\), given by
  \[
    \basis B* = \{\varepsilon_1,\dots, \varepsilon_n\}
  \]
  where
  \[
    \varepsilon_j \Big( \sum_{i=1}^{n} a_i e_i \Big) = a_j
  \]
  for \(1\leq j\leq m\).

  \(\basis B^*\) is called the \emph{dual basis} to \(\basis B\).
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item linear independence: suppose
    \[
      \sum_{j=1}^{n}\lambda_j\varepsilon_j = 0.
    \]
    Apply the relation to basis vectors,
    \[
      0 = \Big( \sum_{j=1}^n \lambda_j\varepsilon_j \Big) e_i = \sum_{j=1}^n \lambda_j\varepsilon_j(e_i)
      \]
      The last expression is 
      \[
        \varepsilon_j(e_i) = 
      \begin{cases}
        0 & \text{ if } i \neq j \\
        1 & \text{ if } i = j
      \end{cases}
    \]
    so \(\lambda_i=0\) for all \(1 \leq i \leq n\).
  \item span: if \(\alpha \in V^*\), then
    \[
      \alpha = \sum_{i=1}^{n}\alpha(e_i)\varepsilon_i
    \]
    since linear maps are uniquely determined by the action on basis.
  \end{itemize}
\end{proof}

\begin{corollary}
  If \(V\) is a finite-dimensional \(\F\)-vector space then
  \[
    \dim V = \dim V^*.
  \]
\end{corollary}

\begin{remark}
  Sometimes it is useful to think about \((\F^n)^*\) as the space of row vectors of length \(n\) over \(\F\).
\end{remark}

\subsection{Dual Map}

It turns out dual spaces have maps between them. Before studying them in detail later, we introduce this concept add richness to the theory of dual map:

\begin{definition}[Annihilator]
  If \(U \subseteq V\), the \emph{annihilator} of \(U\) is
  \[
    U^\ann = \{\alpha\in V^*: \forall u \in U,\,\alpha(u) = 0 \}.
  \]
\end{definition}

\begin{lemma}\leavevmode
  \begin{enumerate}
  \item \(U^\ann \leq V^*\),
  \item If \(U \leq V\) and \(\dim V = n < \infty\) then
    \[
      \dim V = \dim U + \dim U^\ann.
    \]
  \end{enumerate}
\end{lemma}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item \(0 \in U^\ann\). If \(\alpha\) and \(\alpha'\) are in \(U^\ann\) then
    \[
      (\alpha+ \alpha')(u) = \alpha(u) + \alpha'(u) = 0+0 = 0
    \]
    for all \(u\in U\). Similarly \(\lambda\alpha\in U^\ann\) for any \(\lambda \in \F\).
  \item Let \(\basis B = \{e_1,\dots, e_k\}\) be a basis for \(U\) and extend it to a basis for \(V\), say \(e_1,\dots,e_k,e_{k+1},\dots,e_n\). Let \(\basis B^*=\{\varepsilon_1,\dots,\varepsilon_n\}\) be its dual basis. Claim \(\varepsilon_{k+1},\dots,\varepsilon_n \) is a basis for \(U^\ann\):
    \begin{itemize}
    \item If \(i>k,j\leq k\) then \(\varepsilon_i(e_j) = 0 \) so \(\varepsilon_i\in U^\ann\)
    \item Linear independence comes from the fact that \(B^*\) is a basis.
    \item If \(\alpha\in U^\ann\), \(\alpha = \sum_{i=1}^na_i\varepsilon_i\) for some \(\alpha_i\in \F\). Then for any \(j\leq k\),
      \[
        \Big( \sum_{i=1}^{n}a_i\varepsilon_i \Big) (e_j) = 0
      \]
      so \(a_j=0\). It follows that \(\alpha \in \langle \varepsilon_{k+1},\dots,\varepsilon_n \rangle\).
    \end{itemize}
  \end{enumerate}
\end{proof}

\begin{lemma}[Dual space as a contravariant functor]
  Let \(V\) and \(W\) be \(\F\)-vector spaces. Let \(\alpha \in L(V,W)\). Then the map
  \begin{align*}
    \alpha^*: W^* &\to V^* \\
    \varepsilon &\mapsto \varepsilon \compose \alpha
  \end{align*}
  is linear. \(\alpha^*\) is called the \emph{dual} of \(\alpha\).
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(\varepsilon \compose \alpha \in V^*\) since composition preserves linearity.
  \item Fix \(\theta_1,\theta_2\in W^*\),
    \begin{align*}
      \alpha^*(\theta_1+\theta_2) &= (\theta_1+\theta_2)\compose \alpha \\
                                  &= \theta_1\compose \alpha + \theta_2 \compose \alpha \\
      &= \alpha^*\theta_1 + \alpha^*\theta_2
    \end{align*}
  \item Similarly \(\alpha^*(\lambda\theta) = \lambda\alpha^*(\theta)\).
  \end{itemize}
\end{proof}

\begin{proposition}
  Let \(V\) and \(W\) be \(\F\)-vector spaces with bases \(\basis B\) and \(\basis C\) repectively. Let \(\basis B^*\) and \(\basis C^*\) be the dual bases. Consider \(\alpha\in L(V,W)\) with dual \(\alpha^*\), then
  \[
    [\alpha^*]_{\basis C^*,\basis B^*} = [\alpha]^T_{\basis B,\basis C}.
  \]
\end{proposition}

\begin{proof}
  Say \(\basis B = \{b_1,\dots,b_n\}\), \(\basis B^* = \{\beta_1,\dots,\beta_n\}\), \(\basis C = \{c_1,\dots,c_m\}\) and \(\basis C^* = \{\gamma_1,\dots,\gamma_n\}\). Further let \([\alpha]_{\basis B,\basis C} = (a_{ij})\), an \(m\times n\) matrix.
  \begin{align*}
    \alpha^*(\gamma_r)(b_s) &= \gamma_r \compose \alpha(b_s) \\
                            &= \gamma_r(\alpha(b_s)) \\
                            &= \gamma(r) \Big( \sum_{t}^{ }a_{ts}c_t \Big) \\
                            &= \sum_{t}^{ } a_{ts} \gamma_r(c_t) \\
                            &= a_{rs} \\
                            &= \Big( \sum_{i}^{ } a_{ri}\beta_i \Big) (b_s)
  \end{align*}
  Thus
  \[
    \alpha^*(\gamma_r) = \sum_{i}^{ } a_{ri}\beta_i
  \]
  so
  \[
    [\alpha^*]_{\basis C^*,\basis B^*} = [\alpha]^T_{\basis B,\basis C}.
  \]
\end{proof}


It follows that
\begin{lemma}
  Let \(V\) be a finite-dimensioanl \(\F\)-vector space with bases \(\basis E = \{e_1,\dots,e_n\}\) and \(\basis F = \{f_1,\dots,f_n\}\). They have correponding dual bases \(\basis E^* = \{\varepsilon_1,\dots, \varepsilon_n\}\) and \(\basis F^*\). Then the change of basis matrix from \(\basis F^*\) to \(\basis E^*\) is
  \[
    (P^{-1})^T.
  \]
\end{lemma}

\begin{proof}
  \[
    [\id]_{\basis F^*,\basis E^*} = [\id]_{\basis E, \basis F}^T = ([\id]_{\basis F, \basis E}^{-1})^T.
  \]
\end{proof}

\begin{caution}
  \(V \cong V^*\) only if \(V\) is finite dimensional. Let \(V = P\), the space of all real polynomials. It has basis \(\{p_j\}_{j\in \N}\) where \(p(j) = t^j\). In example sheet 2 we will see
\begin{align*}
  P^* &\cong \R^\N \\
  \varepsilon &\to (\varepsilon(p_0), \varepsilon(p_1), \dots)
\end{align*}
and on example sheet 1 we prove
\[
  P \ncong \R^\N
\]
as the latter does not have a countable basis.
\end{caution}

Now we move on to more discussion about annhilator.

\begin{lemma}
  Let \(V\) and \(W\) be \(\F\)-vector spaces. Fix \(\alpha \in L(V,W)\) and let \(\alpha^* \in L(W^*, V^*)\) be its dual. Then
  \begin{itemize}
  \item \(N(\alpha^*) = (\im \alpha)^\ann\), so if \(\alpha^*\) is injectve if and only if \(\alpha\) is surjective.
  \item \(\im \alpha^* \leq N(\alpha)^\ann\), with equality if \(V\) and \(W\) are both finite-dimensional, in which case \(\alpha^*\) is surjective if and only if \(\alpha\) is injective.
  \end{itemize}
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item Let \(\varepsilon \in W^*\), then
  \begin{align*}
    & \varepsilon \in N(\alpha^*) \\
    \Leftrightarrow & \alpha^* (\varepsilon) = 0 \\
    \Leftrightarrow & \varepsilon \compose \alpha = 0 \\
    \Leftrightarrow & \varepsilon(u) = 0 \, \forall u \in \im \alpha \\
    \Leftrightarrow & \varepsilon \in (\im \alpha)^\ann
  \end{align*}

\item Let \(\varepsilon \in \im \alpha^*\), Then \(\varepsilon = \alpha^* (\phi)\) for some \(\phi \in W^*\). For any \(u \in N(a)\),
  \[
    \varepsilon(u) = (\alpha^* \phi)(u) = (\phi\compose \alpha)(u) = \phi(\alpha(u)) = \phi(0) = 0
  \]
  so \(\varepsilon \in N(\alpha)^\ann\).

  Now use the fact that \(V\) and \(W\) are finite-dimensional:
  \[
    \dim \im(\alpha^*) = r(\alpha^*) = r(\alpha)
  \]
  as \(r(A) = r(A^T)\). On the other hand,
  \[
    r(\alpha) = \dim V - \dim N(\alpha) = \dim (N(\alpha))^\ann
  \]
  Thus they are equal.
\end{itemize}
\end{proof}

\subsection{Double Dual}

Let \(V\) be an \(\F\)-vector space. Then \(V^* = L(V,\F)\) is its dual space. The natrual next step is

\begin{definition}[Double dual]
  The \emph{double dual} of \(V\) is
  \[
    V^{**} = V^* = L(V^*, \F).
  \]
\end{definition}

\begin{theorem}[Double dual as natural transformation]
  If \(V\) is an \(\F\)-vector space, then the map
  \begin{align*}
    \hat \cdot: V &\to V^{**} \\
    v &\mapsto \hat v
  \end{align*}
  where \(\hat v(\varepsilon) = \varepsilon(v)\), is an natural homomorphism. In particular when \(V\) is finite-dimensional this is a natural isomorphism.
\end{theorem}

\begin{proof}\leavevmode
  \begin{itemize}
  \item For \(v\in V\), the map \(\hat v: V^* \to \F\) is linear so \(\hat \cdot\) does give a map from \(V\) to \(V^{**}\).
  \item Linear: for \(v_1, v_2 \in V\), \(\lambda_1, \lambda_2 \in \F\), \(\varepsilon \in V*\), then
    \[
      \reallywidehat{\lambda_1v_1 + \lambda_2v_2}(\varepsilon) = \varepsilon(\lambda_1v_1 + \lambda_2v_2) = \lambda_1 \varepsilon(v_1) + \lambda_2 \varepsilon(v_2) = \lambda_1 \hat v_1(\varepsilon) + \lambda_2 \hat v_2(\epsilon)
    \]
  \item Injectivity: let \(e \in V\setminus\{0\}\). Extend it to a basis of \(V\), say \(e, e_2,\dots, e_n\). Let \(\varepsilon, \varepsilon_2,\dots, \varepsilon_n\) be its corresponding dual basis. Now
    \[
      \hat e(\varepsilon) = \varepsilon(e) = 1 
    \]
    so \(\hat e\) is non-zero. \(\hat \cdot\) is injective.
  \item Finally, if \(V\) is finite-dimensional, \(\dim V = \dim V^* = \dim V^{**}\) so \(\hat \cdot\) is an isomorphism.
  \end{itemize}
\end{proof}

\begin{lemma}
  Let \(V\) be a finite-dimensional \(\F\)-vector space and \(U \leq V\). Then
  \[
    \hat U = U^{\ann \ann}.
  \]
  So \(U = U^{\ann \ann}\).
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item First show \(\hat U \leq U^{\ann \ann}\): given \(u \in U\), for all \(\varepsilon \in U^\ann\), \(\varepsilon(u) = 0\) so \(\hat u(\varepsilon) = 0\). Thus \(\hat u \in (U^\ann)^\ann = U^{\ann \ann}\).
  \item
    \[
      \dim U^{\ann \ann} = \dim V^* - \dim U^\ann = \dim V - \dim U^\ann = \dim U
    \] so \(\hat U = U^{\ann \ann}\).
  \end{itemize}
\end{proof}

\begin{lemma}[Galois Connection]
  Let \(V\) be a finite-dimensional \(\F\)-vector space and \(U_1, U_2 \leq V\). Then
  \begin{itemize}
  \item \((U_1 + U_2)^\ann = U_1^\ann \cap U_2^\ann\),
  \item \((U_1 \cap U_2)^\ann = U_1^\ann + U_2^\ann\).
  \end{itemize}
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item Let \(\theta \in V^*\), \(\theta \in (U_1+U_2)^\ann\) if and only if \(\theta(u_1+u_2) = 0\) for all \(u_1\in U_1, u_2 \in U_2\), if and only if \(\theta(u) = 0\) for all \(u \in U_1 \cup U_2\), so \(\theta\in U_2^\ann \cap U_2^\ann\).
  \item Apply \(^\ann\) to the first result.
  \end{itemize}
\end{proof}

\section{Bilinear Forms I}

\begin{definition}[Bilinear form]
  Let \(U\) and \(V\) be \(\F\)-vector spaces. A map \(\varphi: U \times V \to \F\) is \emph{bilinear} if it is linear in both arguments, i.e.
  \begin{align*}
    \forall u \in U, \, \varphi(u, -) &\in V^* \\
    \forall v \in V, \, \varphi(-, v) &\in U^*
  \end{align*}
\end{definition}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item \(V \times V^* \to \F, (v, \theta) \mapsto \theta(v)\).
  \item \(U = V = \R^n, \varphi(x, y) = \sum_{i=1}^{n}x_iy_i\).
  \item \(A \in \M_{m,n}(\F), \varphi: \F^m \times \F^n \to \F, (u, v) \mapsto u^TAv\).
  \item \(U = V = C([0, 1], \R), (f, g) \mapsto \int_{0}^{1} f(t)g(t) dt \)
  \end{enumerate}
\end{eg}

\begin{definition}[Matrix of bilinear form]
  Let \(\basis B = \{e_1,\dots,e_m\}\) be a basis for \(U\) and \(\basis C = \{f_1,\dots,f_n\}\) be a basis for \(V\). Given a bilinear map \(\varphi: U \times V \to \F\), the \emph{matrix of \(\varphi\)} with respect to \(\basis B\) and \(\basis C\) is
  \[
    [\varphi]_{\basis B, \basis C} = \left( \varphi(e_i, f_j) \right)_{m \times n}.
  \]
\end{definition}

\begin{lemma}
  \[
    \varphi(u, v) = [u]_{\basis B}^T [\varphi]_{\basis B, \basis C} [v]_{\basis C}.
  \]
\end{lemma}

\begin{proof}
  Let \(u = \sum_{i}^{ }\lambda_ie_i, v = \sum_{j}^{} \mu_jf_j\), then
  \begin{align*}
    \varphi(u, v) &= \varphi\left( \sum_{i}^{ }\lambda_ie_i, \sum_{j}^{} \mu_jf_j \right) \\
                  &= \sum_{i}^{ }\lambda_i \varphi\left(e_i, \sum_{j}^{} \mu_jf_j \right)\\
                  &= \sum_{i, j}^{ }\lambda_i \varphi(e_i, f_j) \mu_j
  \end{align*}
\end{proof}

\begin{note}\leavevmode
  \begin{enumerate}
  \item \([\varphi]_{\basis B, \basis C}\) is the unique matrix with this property.
  \item A bilinear \(\varphi: U \times V \to \F\) induces linear maps
    \begin{align*}
      \varphi_L: U &\to V^* \\
      u &\mapsto \varphi(u, -) \\
      \varphi_R: V &\to U^* \\
      v &\mapsto \varphi(-, v)
    \end{align*}
  \end{enumerate}
\end{note}

\begin{lemma}
  Let \(\basis B = \{e_1,\dots, e_m\}\) be a basis for \(U\), \(\basis B^* = \{\varphi_1,\dots, \varphi_m\}\) a basis for \(U^*\), \(\basis C = \{f_1,\dots, f_n\}, \basis C^* = \{\eta_1,\dots, \eta_n\}\) for \(V\) and \(V^*\). If \([\varphi]_{\basis B, \basis C} = A\) then
  \begin{align*}
    [\varphi_L]_{\basis B, \basis C^*} &= A^T, \\
    [\varphi_R]_{\basis C, \basis B^*} &= A. \\
  \end{align*}
\end{lemma}

\begin{proof}
  \begin{align*}
    \varphi_L(e_i)(f_j) = A_{ij} &\Longrightarrow \varphi_L(e_i) = \sum_{j}^{ }A_{ij}\eta_j \\
    \varphi_R(f_j)(e_i) = A_{ij} &\Longrightarrow \varphi_R(f_j) = \sum_{i}^{ }A_{ij}\varepsilon_j
  \end{align*}
\end{proof}

\begin{definition}[Left and right kernel]
  The \emph{left (right, respectively) kernel} of \(\varphi\) is \(\ker \varphi_L\) (\(\ker \varphi_R\), respectively).
\end{definition}

\begin{definition}[Degeneracy]
  \(\varphi\) is \emph{non-degenerate} if \(\ker \varphi_L = 0\) and \(\ker \varphi_R = 0\). Otherwise, \(\varphi\) is \emph{degenerate}.
\end{definition}

\begin{lemma}
  Let \(U, V\) have bases as before, and \(\varphi, A\) as before. Then \(\varphi\) is non-degenerate if and only if \(A\) is invertible.
\end{lemma}

\begin{proof}
  \begin{align*}
    & \varphi \text{ is non-degenerate} \\
    \Leftrightarrow & \ker \varphi_L = 0 \text{ and } \ker \varphi_R = 0 \\
    \Leftrightarrow & n(A^T) = n(A) = 0 \\
    \Leftrightarrow & r(A^T) = \dim V, r(A) = \dim U \\
    \Leftrightarrow & A \text{ is invertible}
  \end{align*}
\end{proof}

\begin{corollary}
  If \(\varphi\) is non-degenerate and \(U\) and \(V\) are finite-dimensional and \(\dim U = \dim V\).
\end{corollary}

\begin{corollary}
  When \(U\) and \(V\) are finite-dimensional, choosing a non-degenerate bilinear form \(\varphi: U \times V \to \F\) is equivalent to picking an isomorphism \(\varphi_L: U \to V^*\).
\end{corollary}

\begin{definition}
  For \(T \subseteq U\),
  \begin{align*}
    T^\perp &= \{ v\in V: \varphi(t, v) = 0 \, \forall t\in T \} \leq V \\
    ^\perp S &= \{ u\in U: \varphi(u, s) = 0 \, \forall s\in S \} \leq U
  \end{align*}
\end{definition}
They are generalisation of annihilators.

\begin{proposition}
  Suppose \(U\) have basese \(\basis B, \basis B'\) and \(V \) have bases \(\basis C, \basis C'\), \(P = [\id]_{\basis B', \basis B}, \Q = [\id]_{\basis C',\basis C}\). Let \(\varphi: U \times V \to \F\) be a bilinear form. Then
  \[
    [\varphi]_{\basis B',\basis C'} = P^T[\varphi]_{\basis B,\basis C}Q.
  \]
\end{proposition}

\begin{proof}
  \begin{align*}
    \varphi(u, v) &= [u]_{\basis B}^T [\varphi]_{\basis B, \basis C} [v]_{\basis C} \\
                  &= (P[u]_{\basis B'})^T [\varphi]_{\basis B, \basis C} (Q[v]_{\basis C'}) \\
                  &= [u]_{\basis B'}^T P^T[\varphi]_{\basis B, \basis C}Q[v]_{\basis C'} 
  \end{align*}
\end{proof}

\begin{definition}[Rank of bilinear form]
  The \emph{rank} of \(\varphi\), \(r(\varphi)\), is the rank of its matrix representation (which is well-defined by the previous proposition).
\end{definition}

\begin{note}
  \[
    r(\varphi) = r(\varphi_L) = r(\varphi_R).
  \]
\end{note}

\section{Determinant \& Trace}

\subsection{Trace}

\begin{definition}[Trace]
  For \(A \in \M_n(\F) = \M_{n,n}(\F)\), the \emph{trace} of \(A\) is
  \[
    \tr(A) = \sum_{i=1}^{n}A_{ii}.
  \]
\end{definition}

\begin{lemma}
  For \(A, B\in \M_n(\F)\),
  \[
    \tr(AB) = \tr(BA).
  \]
\end{lemma}

\begin{proof}
  \[
    \tr(AB) = \sum_{i}^{ }\sum_{j}^{ }a_{ij}b_{ji} = \sum_{j}^{ }\sum_{i}^{ }b_{ji}a_{ij} = \tr(BA).
  \]
\end{proof}

\begin{lemma}
  Similar (or conjugate) matrices have the same trace.
\end{lemma}

\begin{proof}
  Suppose \(A\) and \(B\) are conjugates, they there exists \(P\) such that \(B = P^{-1}AP\) so
  \[
    \tr(B) = \tr(P^{-1}AP) = \tr(APP^{-1}) = \tr(A).
  \]
\end{proof}

\begin{definition}[Trace of map]
  Let \(\alpha: V \to V\) be a linear, the \emph{trace} of \(\alpha\) is
  \[
    \tr \alpha = \tr [\alpha]_{\basis B, \basis B}
  \]
  with repsect to a basis \(\basis B\). This is well-defined by the previous lemma.
\end{definition}

\begin{lemma}
  Let \(\alpha: V \to V\) be linear and \(\alpha^*: V^* \to V^*\) be its dual. Then
  \[
    \tr \alpha = \tr \alpha^*.
  \]
\end{lemma}

\begin{proof}
  \[
    \tr \alpha = \tr [\alpha]_{\basis B} = \tr [\alpha]_{\basis B}^T = \tr[\alpha^*]_{\basis B^*} = \tr \alpha^*.
  \]
\end{proof}

\subsection{Determinant}

Let \(S_n\) be the permutations group of the set \(\{1, 2, \dots, n\}\) and \(\varepsilon: S_n \to \{1, -1\}\) be the signature of a permutation, i.e.
\[
  \varepsilon(\sigma) =
  \begin{cases}
    1 &\text{if \(\sigma\) is a product of even number of transpotitions}\\
    0 &\text{otherwise}
  \end{cases}
\]

\begin{definition}[Determinant]
  Suppose \(A \in \M_n(\F)\), \(A = (a_{ij})\), the \emph{determinant} of \(A\) is
  \[
    \det A = \sum_{\sigma \in S_n}^{ } \varepsilon(\sigma) a_{\sigma(1), 1} a_{\sigma(2), 2} \dots a_{\sigma(n), n}.
  \]
\end{definition}

There are \(n!\) terms in this summation and each is the signed product of \(n\) elements (one from each row and each column).

\begin{eg}
  For \(n = 2\),
  \[
    \det
    \begin{pmatrix}
      a_{11} & a_{12} \\
      a_{21} & a_{22}
    \end{pmatrix}
    =a_{11}a_{22} - a_{21}a_{12}
  \]
\end{eg}

\begin{lemma}
  If \(A = (a_{ij})\) is an upper-triangular matrix (i.e.\ \(a_{ij} = 0\) for all \(i > j\)) then
  \[
    \det A = a_{11}a_{22}\dots a_{nn}.
  \]
  Similar for lower-trianglular matrices.
\end{lemma}

\begin{proof}
  In the summation
  \[
    \det A = \sum_{\sigma \in S_n}^{ }\varepsilon(\sigma) a_{\sigma(1), 1}\dots a_{\sigma(n),n},
  \]
  for a summand to be non-zero, need \(\sigma(j) \leq j\) for all \(j\). Thus \(\sigma = \id\).
\end{proof}

\begin{lemma}
  \[
    \det A = \det A^T
  \]
\end{lemma}

\begin{proof}
  \begin{align*}
    \det A &= \sum_{\sigma \in S_n}^{ } \varepsilon(\sigma) \prod_{i = 1}^n a_{\sigma(i), i} \\
           &= \sum_{\sigma \in S_n}^{ } \varepsilon(\sigma) \prod_{i = 1}^n a_{i, \sigma^{-1}(i)} \\
           &= \sum_{\sigma \in S_n}^{ } \varepsilon(\sigma^{-1}) \prod_{i = 1}^n a_{i, \sigma^{-1}(i)} \\
           &= \sum_{\tau \in S_n}^{ } \varepsilon(\tau) \prod_{i = 1}^n a_{i, \tau(i)} \text{ where \(\tau = \sigma^{-1}\)} \\
           &= \det A^T
  \end{align*}
\end{proof}

\begin{definition}[Volume form]
  A \emph{volume form} on \(\F^n\) is a function
  \[
    d: \underbrace{\F^n \times \F^n \times \dots \times \F^n}_{\text{\(n\) copies}} \to \F
  \]
  which is:
  \begin{itemize}
  \item multilinear: for any \(i\) and \(v_1, \dots, v_{i-1}, v_i, v_{i + 1}, \dots, v_n \in \F^n\),
    \[
      d(v_1, \dots, v_{i-1}, -, v_{i+1}, \dots, v_n) \in (\F^n)^*.
    \]
  \item alternating: if \(v_i = v_j\) for \(i \neq j\), \(d(v_1, \dots, v_n) = 0\).
  \end{itemize}
\end{definition}

\begin{notation}
  Given \(A = (a_{ij})\), write \(A\) in column form
  \[
    \left( A^{(1)} | \cdots | A^{(n)} \right).
  \]
\end{notation}
For example, if \(\{e_i\}\) is a standard basis for \(\F^n\) then
\[
  I = \left(e_1 | \dots | e_n \right).
\]

\begin{lemma}
  \begin{align*}
    \det: \F^n \times \dots \times \F^n &\to \F \\
    (A^{(1)}, \dots, A^{(n)}) &\mapsto \det A
  \end{align*}
  is a volume form.
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item Multilinear: for any fixed \(\sigma \in S_n\), \(\prod_{i = 1}^n a_{\sigma(i), i}\) contains exactly one term from each column so it is multilinear. Multilinearity is preserved under addition.
  \item Alternating: suppose \(A^{(k)} = A^{(l)}\) for some \(l \neq k\). Let \(\tau = (kl)\). Then \(a_{ij} = a_{i \tau(j)}\) for all \(i, j\). Also \(S_n\) can be expressed as a union of two disjoint cosets \(A_n\) and \(\tau A_n\) so
    \begin{align*}
      \det A &= \sum_{\sigma \in A_n}^{ } \prod_{i = 1}^n a_{i, \sigma(i)} - \sum_{\sigma \in A_n}^{ } \prod_{i = 1}^n a_{i, \tau\sigma(i)} \\
             &= \sum_{\sigma \in A_n}^{ } \prod_{i = 1}^n a_{i, \sigma(i)} - \sum_{\sigma \in A_n}^{ } \prod_{i = 1}^n a_{i, \sigma(i)} \\
             &= 0
    \end{align*}
  \end{itemize}
\end{proof}

In the rest of the section, we are going to prove that the converse is true, i.e.\ all volume forms are determinants up to a scaling constant.

\begin{lemma}
  Let \(d\) be a volume form. Then swapping two entries changes the sign:
  \[
    d(v_1, \dots, v_i, \dots, v_j, \dots, v_n) = - d(v_1, \dots, v_j, \dots, v_i, \dots, v_n).
  \]
\end{lemma}

\begin{proof}
  \begin{align*}
    0 &= d(v_1, \dots, v_{i-1}, v_i + v_j, v_{i+1}, \dots, v_{j-1}, v_i + v_j, v_{j + 1}, \dots, v_n) \\
      &= \underbrace{(v_1, \dots, v_i, \dots, v_i, \dots, v_n)}_{ = 0} + d(v_1, \dots, v_j, \dots, v_i, \dots, v_n) \\
      &+ d(v_1, \dots, v_i, \dots, v_j, \dots, v_n) + \underbrace{d(v_1, \dots, v_j, \dots, v_j, \dots, v_n)}_{ = 0} \\
  \end{align*}
  Rearrange.
\end{proof}

\begin{corollary}
  If \(\sigma \in S_n\),
  \[
    d(v_{\sigma(1)}, \dots, v_{\sigma(n)}) = \varepsilon(\sigma) d(v_1, \dots, v_n).
  \]
\end{corollary}

\begin{theorem}
  Let \(d\) be a volume form on \(\F^n\), \(A = (A^{(1)}|\dots | A^{(n)})\), then
  \[
    d(A^{(1)}, \dots, A^{(n)}) = \det A \cdot d(e_1, \dots, e_n).
  \]
\end{theorem}

\begin{proof}
  \begin{align*}
    d(A^{(1)}, \dots, A^{(n)}) &= d \left( \sum_{i=1}^{n} a_{i1}e_i, A^{(2)}, \dots, A^{(n)} \right) \\
                               &= \sum_{i = 1}^{n}a_{i1} d(e_i, A^{(2)}, \dots, A^{(n)}) \\
                               &= \sum_{i}^{} \sum_{j}^{ } a_{i1} a_{j2} d(e_i, e_j, \dots, A^{(n)}) \\
                               &= \sum_{i_1, i_2, \dots, i_n}^{ } \prod_{k = 1}^{n} a_{{i_k},k} d(e_{i_1}, \dots e_{i_n})
\end{align*}
The last term is \(0\) unless all of \(i_k\) are distinct, i.e.\ exists \(\sigma \in S_N\) such that \(i_k = \sigma(k)\). Thus
\begin{align*}
  d(A^{(1)}, \dots, A^{(n)}) = \sum_{\sigma \in S_n}^{ } \prod_{k = 1}^{n} a_{\sigma(k), k} \underbrace{d(e_{\sigma(1)}, \dots, e_{\sigma(n)})}_{= \varepsilon(\sigma) d(e_1, \dots, e_n)}
\end{align*}
\end{proof}

\begin{corollary}
  \(\det\) is the unique volume form \(d\) such that \(d(e_1, \dots, e_n) = 1\).
\end{corollary}

\begin{proposition}
  Suppose \(A, B \in \M_n(\F)\), then
  \[
    \det AB =\det A \det B.
  \]
\end{proposition}

\begin{proof}
  Let \(d_A: \F^n \times \dots \F^n \to \F, (v_1, \dots, v_n) \to \det(Av_1|\cdots|Av_n)\), then \(d_A\) is a volume form:
  \begin{itemize}
  \item Multilinear: \(v_i \mapsto A_{v_i}\) is linear and \(\det\) is multilinear.
  \item Altenating: \(v_i = v_j\) implies \(A_{v_i} = A_{v_j}\) and \(\det\) is alternating.
  \end{itemize}
  It follows that
  \begin{align*}
    d_A(Be_1, \dots, Be_n) &= \det B \cdot d_A(e_1, \dots, e_n) = \det B \det A \\
                           &= \det (ABe_1| \cdots | ABe_n) = \det AB
  \end{align*}
\end{proof}

\begin{definition}[Singular]
  \(A \in \M_n(\F)\) is \emph{singular} if \(\det A = 0\). Otherwise it is \emph{non-singular}.
\end{definition}

\begin{lemma}
  If \(A\) is invertible then it is non-singular and
  \[
    \det A^{-1} = \frac{1}{\det A}.
  \]
\end{lemma}

\begin{proof}
  \[
    1 = \det I_n = \det(AA^{-1}) = \det A \det A^{-1}
  \]
  The result follows.
\end{proof}

\begin{theorem}
  Suppose \(A \in \M_n(\F)\) then TFAE:
  \begin{enumerate}
  \item \(A\) is invertible,
  \item \(A\) is non-singular,
  \item \(r(A) = n\).
  \end{enumerate}
\end{theorem}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(1 \Rightarrow 2\): done.
  \item \(2 \Rightarrow 3\): suppose that \(r(A) < n\). By rank-nullity \(n(A) > 0\) so \(\exists \lambda \in \F^n \setminus \{0\}\) such that \(A \lambda = 0\). Say \(\lambda = (\lambda_i)\) and \(\lambda_k \neq 0\). Have \(\sum_{i = 1}^{n}A^{(i)}\lambda_i = 0 \). Let
    \[
      B= (e_1|e_2|\cdots|e_{k-1}|\lambda|e_{k+1}|\cdots|e_n)
    \]
    It follows that \(AB\) has \(k\)th column zero so
    \[
      0 = \det AB = \det A \det B = \lambda_k \det A.
    \]
    So \(\det A = 0\).
  \item \(3 \Rightarrow 1\): by rank-nullity.
  \end{itemize}
\end{proof}

\subsection{Determinant of Linear Maps}

\begin{lemma}
  Conjugate matrices have the same determinant.
\end{lemma}

\begin{proof}
  Let \(B = P^{-1}AP\). Then
  \[
    \det B = \det (P^{-1}AP) = \det P^{-1} \det A \det P = \det (P^{-1}P) \det A = \det A.
  \]
\end{proof}

\begin{definition}[Determinant]
  Let \(\alpha: V \to V\) where \(V\) is a finite-dimensional vector space. The \emph{determinant} of \(\alpha\) is
  \[
    \det \alpha = \det [\alpha]_{\basis B, \basis B}
  \]
  where \(\basis B\) is any basis for \(V\). 
\end{definition}
This is well-defined by the previous lemma.

\begin{theorem}
  \(\det: L(V, V) \to \F\) satisfies
  \begin{enumerate}
  \item \(\det \id = 1\),
  \item \(\det \alpha \compose \beta = \det \alpha \det \beta\),
  \item \(\det \alpha \neq 0\) if and only if \(\alpha\) is invertible and in this case \(\det (\alpha^{-1}) = \frac{1}{\det \alpha}\).
  \end{enumerate}
\end{theorem}

\begin{proof}
  Restatement of previous results.
\end{proof}

\subsection{Determinant of Block-triangular Matrices}

\begin{lemma}
  Suppose \(A \in \M_k(\F), B \in \M_\ell(\F)\) and \(C \in \M_{k, \ell}(\F)\), then
  \[
    \det
    \begin{pmatrix}
      A & C \\
      0 & B
    \end{pmatrix}
    = \det A \det B.
  \]
\end{lemma}

\begin{proof}
  Let \(n = k + \ell\) and call the block matrix \(X = (x_{ij})\), which is an element of \(\M_n(\F)\). Then
  \begin{align*}
    \det X &= \sum_{\sigma \in S_n}^{ }\varepsilon(\sigma) \prod_{i = 1}^{n} x_{\sigma(i), i} \\
    \intertext{Note that \(x_{\sigma(i), i} = 0\) if \(i \leq k\) and \(\sigma(i) > k\). Thus we are summing over all \(\sigma\) such that \(\sigma = \sigma_1\sigma_2\) where \(\sigma_1 \in \sym_{\{1,\dots, k\}}\) and \(\sigma_2 \in \sym_{\{k+1, \dots, n\}}\).}
           &= \sum_{\sigma_1 \in \sym_{\{1, \dots, k\}}}^{ } \varepsilon(\sigma_1) \prod_{j = 1}^{k} a_{\sigma_1(j),j} \\
           &\cdot \sum_{\sigma_2 \in \sym_{\{k+1, \dots, n\}}}^{ } \varepsilon(\sigma_1) \prod_{j = k + 1}^{k} a_{\sigma_2(j),j} \\
           &= \det A \det B
  \end{align*}
\end{proof}

\begin{corollary}
  For a sequence of matrices \(A_1, \dots, A_k\),
  \[
    \det
    \begin{pmatrix}
      A_1 & & & & \\
      & A_2 & & * & \\
      & & A_3 & & \\
      & 0 & & \ddots & \\
      & & & & A_k
    \end{pmatrix}
    = \prod_{i = 1}^{k} \det A_i
  \]
\end{corollary}

\begin{proof}
  Apply the previous lemma inductively.
\end{proof}

\begin{caution}
  In general,
  \[
    \det
    \begin{pmatrix}
      A & B \\
      C & D
    \end{pmatrix}
    \neq \det A \det D - \det B \det C.
  \]
\end{caution}

\subsection{Volume Interpretation of Determinant}

In \(\R^2\), the determinant of a matrix
\[
  \det
  \begin{pmatrix}
    a_{11} & a_{12} \\
    a_{21} & a_{22} 
  \end{pmatrix}
\]
can be intepreted as the signed area of the parallelogram spanned by the column vectors of the matrix \(\binom{a_{11}}{a_{21}}\) and \(\binom{a_{12}}{a_{22}}\).
    
Similarly in \(\R^3\) the determinant of a matrix is the signed volume of the parallelepiped spanned by the column vectors of the matrix.

For higher dimensions, although it can be difficult to visualise, the same interpretation still works: consier a hypercube
%
%
%
%

\subsection{Elementary Operations and Determinant}

Consider the determinants of elementary column operation matrices:
\begin{itemize}
\item \(E_1\) swaps two columns so \(\det E_1 = -1\),
\item \(E_2\) multiplies a column by \(\lambda \neq 0\) so \(\det E_2 = \lambda\),
\item \(E_3\) adds \(\lambda\) times of a column to another column so \(\det E_3 = 1\).
\end{itemize}

One could prove properties of \(\det\) by decomposing any matrix into elementary matrices.

\subsection{Column Expansion \& Adjugate Matrices}

\begin{lemma}
  Suppose \(A \in \M_n(\F)\), \(A = (a_{ij})\). Define \(A_{\widehat{ij}} \in M_{n - 1}(\F)\) by deleting row \(i\) and column \(j\) from \(A\). Then \(\det A\) can be calculated by
  \begin{enumerate}
  \item expansion in column \(j\): for a fixed \(j\),
    \[
      \det A = \sum_{i = 1}^{n} (-1)^{i + j} a_{ij} \det A_{\widehat{ij}}.
    \]
  \item expansion in row \(i\): for a fixed \(i\),
    \[
      \det A = \sum_{j = 1}^{n} (-1)^{i + j} a_{ij} \det A_{\widehat{ij}}.
    \]
  \end{enumerate}
\end{lemma}

\begin{remark}
  It is possible to use one of the expressions above to define determinant iteratively, with base case \(\det a = a\) for \(n = 1\).
\end{remark}

\begin{eg}
  \[
    \begin{vmatrix}
      a & b & c \\
      d & e & f \\
      g & h & i
    \end{vmatrix}
    =
    a
    \begin{vmatrix}
      e & f \\
      h & i
    \end{vmatrix}
    -b
    \begin{vmatrix}
      d & f \\
      g & i
    \end{vmatrix}
    +c
    \begin{vmatrix}
      d & e \\
      g & h
    \end{vmatrix}
  \]
\end{eg}

\begin{proof}
  We prove \(1\):
  
  \begin{align*}
    \det A &= \det( A^{(1)} | \cdots | \sum_{i = 1}^{n} a_{ij}e_i | \cdots | A^{(n)} ) \\
           &= \sum_{i = 1}^{n} a_{ij} \det( A^{(1)} | \cdots | e_i | \cdots | A^{(n)} ) \\
    \intertext{use row and column operations to move the entry to top left corner,}
           &= \sum_{i = 1}^{n} a_{ij} (-1)^{(i - 1) + (j - 1)} \det
             \begin{pmatrix}
               1 & 0 \\
               0 & A_{\widehat{ij}}
             \end{pmatrix} \\
           &= \sum_{i = 1}^{n} a_{ij} (-1)^{i + j} \det A_{\widehat{ij}}
  \end{align*}
\end{proof}

\begin{definition}[Adjugate]
  Let \(A \in \M_n(\F)\). The \emph{adjugate matrix} of \(A\), \(\adj A\), is the \(n \times n\) matrix
  \[
    (\adj A)_{ij} = (-1)^{i + j} \det A_{\widehat{ji}}.
  \]
\end{definition}

\begin{theorem}\leavevmode
  \begin{enumerate}
  \item \((\adj A ) A = \det A \cdot I\),
  \item If \(A\) is invertible then
    \[
      A^{-1} = \frac{\adj A}{\det A}.
    \]
  \end{enumerate}
\end{theorem}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item For a fixed \(j\), \(\det A = \sum_i (\adj A)_{ji} a_{ij} = A_{jj}\). For \(j \neq k\), replace the \(j\)th column with the \(k\)th:
    \begin{align*}
      0 &= \det(A^{(1)} | \cdots | A^{(k)} | \cdots | A^{(k)} | \cdots | A^{(n)}) \\
        &= \sum_{i}^{ } (\adj A)_{ji} a_{ik} \\
        &= A_{jk}
    \end{align*}
  \item If \(A\) is invertible then \(\det A \neq 0\) so
    \[
      I = \frac{\adj A}{\det A} A.
    \]
  \end{enumerate}
\end{proof}

\subsection{System of Linear Equations}

A system of linear equations can be written as
\[
  A \V x = \V b
\]
where \(A \in \M_{m, n}(\F)\) and \(\V b \in \M_{m, 1}(\F)\) are known and \(\V x \in \M_{n, 1}(\F)\) is unknown.

The system has a solution if and only if \(r(A) = r(A|\V b)\) where the matrix on RHS is the \emph{augmented matrix} by adding \(\V b\) as a column to \(A\) since this happens if and only if \(\V b\) is a linear combination of columns of \(A\).

The solution is unique if and only if \(r(A) = n\).

In particular, if \(m = n\), if \(A\) is non-singular then there is a unique solution
\[
  \V x = A^{-1} \V b.
\]

Although in theory we could invert the matrix to solve the system of equations, it is terribly inefficient. Instead, we use

\begin{proposition}[Cramer's rule]
  If \(A \in \M_n(\F)\) is invertible then the system
  \[
    A \V x = \V b
  \]
  has unique solution \(\V x = (x_i)\) where
  \[
    x_i = \frac{\det (A_{\hat{i} \V b})}{\det A}
  \]
  where \(A_{\hat{i} \V b}\) is obtained from \(A\) by deleting \(i\)th column and replacing it with \(\V b\).
\end{proposition}

\begin{proof}
  Assume \(\V x\) is a solution of the system.
  \begin{align*}
    \det (A_{\hat i \V b}) &= \det(A^{(1)} | \cdots | \V b | \cdots | A^{(n)}) \\
                           &= \det(A^{(1)} | \cdots | A \V x | \cdots | A^{(n)}) \\
                           &= \sum_{j = 1}^{n} x_j \det(A^{(1)} | \cdots | A^{(j)} | \cdots | A^{(n)}) \\
    \intertext{\(A^{(j)}\) is one of the other columns unless \(j = i\) so}
                           &= x_i \det A
  \end{align*}
\end{proof}

\begin{corollary}
  If \(A \in \M_n(\Z)\) with \(\det A = \pm 1\), then
  \begin{enumerate}
  \item \(A^{-1} \in \M_n(\Z)\).
  \item Given \(\V b \in \Z^n\), \(A \V x = \V b\) has an integer solution.
  \end{enumerate}
\end{corollary}

\section{Endomorphism}

Let \(V\) be a \(\F\)-vector space with \(\dim V = n < \infty\). Let \(\basis B =\{v_1, \dots, v_n\}\) be a basis and \(\alpha \in L(V) = L(V, V)\).

The general problem studied in this section is to choose a basis \(\basis B\) such that \([\alpha]_{\basis B}\) has ``nice forms''.

Suppose there is another basis \(\basis B'\) and the change-of-basis matrix \(P\). Recall that
\[
  [\alpha]_{\basis B} = P^{-1}[\alpha]_{\basis B'}P.
\]
The above proble is thus euqivalent to given \(A \in \M_n(\F)\), want \(A'\) conjugate to \(A\) and in a ``nice form''.

What are the nice forms that we desire? The best we can have is

\begin{definition}[Diagonalisable]
  \(\alpha \in L(V)\) is \emph{diagonalisable} if there exists \(\basis B\) such that \([\alpha]_{\basis B}\) is diagonal.
\end{definition}

A slightly weaker, albeit still ``nice'' enough form is

\begin{definition}[Triangulable]
  \(\alpha \in L(V)\) is \emph{triangulable} if there exists \(\basis B\) such that \([\alpha]_{\basis B}\) is upper triangular.
\end{definition}

Equivalent, rephrasing using lanugages of matrices, \(A \in \M_n(\F)\) is diagonalisable (triangulable, respectively) if it is conjugate to a diagonal (upper triangle, respectively) matrix.

\begin{definition}[Eigenvalue, eigenvector, eigenspace]\leavevmode
  \begin{enumerate}
  \item \(\lambda \in \F\) is an \emph{eigenvalue} of \(\alpha\) if there exists some \(v \in V\setminus\{0\}\) such that \(\alpha(v) = \lambda v\).
  \item \(v \in V\) is an \emph{eigenvector} of \(\alpha\) if \(\alpha(v) = \lambda v\) for some eigenvalue \(\lambda\).
  \item \(V_\lambda = \{v \in V: \alpha(v) = \lambda v\}\) is the \emph{\(\lambda\)-eigenspace} of \(\alpha\).
  \end{enumerate}
\end{definition}

\begin{remark}
  It is easy to check that \(V_\lambda \leq V\).
\end{remark}

\begin{remark}\leavevmode
  \begin{enumerate}
  \item \(V_\lambda = \ker(\alpha - \lambda \iota)\) and
    \begin{align*}
      & \lambda \text{ is an eigenvalue} \\
      \Leftrightarrow & \alpha - \lambda \iota \text{ is singular} \\
      \Leftrightarrow & \det (\alpha - \lambda \iota) = 0
    \end{align*}
  \item If \(\alpha(v_j) = \lambda v_j\) then the \(j\)th column of \([\alpha]_{\basis B}\) is \((0, \dots, \lambda, \dots, 0)^T\).
  \item \([\alpha]_{\basis B}\) is diagonal if and only if \(\basis B\) consists of eigenvectors. \([\alpha]_{\basis B}\) is upper triangular if and only if \(\alpha(v_j) = \spans{v_1, \dots, v_j}\) for all \(j\). In particular, \(v_1\) is an eigenvector.
  \end{enumerate}
\end{remark}

\subsection{Polynomial Ring, an Aside}

Before discussing polynomials associated with a linear map, we need a little knowledge of the ambient polynomial space that we will be working with. The following results should be self-evident and proofs are omitted. Most of them will be studied in detail in IB Groups, Rings and Modules and a proof the Fundamental Theorem of ALgebra can be found in IB Complex Analysis.

Let \(\F[t] = \{\text{polynomials with coefficients in } \F\) and \(\deg f\) be the degree of an \(f\) in \(\F[t]\). By convention \(\deg 0 = -\infty\). We have the following properties:
\begin{enumerate}
\item \(\deg (f + g) \leq \max(\deg f, \deg g), \deg(f g) = \deg f + \deg g\).
\item If \(\lambda \in \F\) is a root of some \(f \in \F[t]\), i.e.\ \(f(\lambda) = 0\) then \((t - \lambda) \divides f\). In other words, \(f(t) = (t - \lambda) g(t)\) for some \(g(t) \in \F[t]\) and \(\deg g = \deg f - 1\).
\item We say \(\lambda\) is a root of \(f \in \F[t]\) with \emph{multiplicity} \(e \in \N\) if \((t - \lambda)^e \divides f\) but \((t - \lambda)^e \ndivides f\).
\item A polynomial of degree \(n\) has at most \(n\) roots, counted with multiplicity.
\item Fundamental Theorem of Algebra: any \(f \in \C[t]\) of positive degree has a root (hence \(\deg f\) roots).
\end{enumerate}

\begin{definition}[Characteristic polynomial]
   The \emph{characteristic polynomial} of \(\alpha \in L(V)\) is
  \[
    \chi_\alpha(t) = \det (\alpha - t \iota).
  \]
  The \emph{characteristic polynomial} of \(A \in \M_n(\F)\) is
  \[
    \chi_A(t) = \det (A - t I).
  \]
\end{definition}

Conjugate matrices have the same characteristic polynomial.

\begin{theorem}
  A linear map \(\alpha\) is triangulable if and only if \(\chi_\alpha(t)\) can be written as a product of linear factors over \(\F\).
\end{theorem}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(\Rightarrow\): suppose \(\alpha\) is triangulable and is represented by
    \[
      \begin{pmatrix}
        a_1 & \cdots & * \\
        & \ddots & \vdots \\
        0 & & a_n
      \end{pmatrix}
    \]
    with respect to some basis. Then
    \[
      \chi_\alpha(t) = \det
      \begin{pmatrix}
        a_1 - t & \cdots & * \\
        & \ddots & \vdots \\
        0 & & a_n -t
      \end{pmatrix}
      = \prod_{i = 1}^{n} (a_i - t)
    \]
  \item \(\Leftarrow\): induction of \(n = \dim V\): if \(n = 1\) then done. Suppose \(n > 1\) and the theorem holds for all endomorphisms of spaces of smaller dimensions. By hypothesis \(\chi_\alpha(t)\) has a root in \(\F\), say \(\lambda\). Let \(U = V_\alpha \neq 0\), then \(\alpha(U) \leq U\) so \(\alpha\) induces \(\overline \alpha: V/U \to V/U\). Pick basis \(v_1, \dots, v_k\) for \(U\) and extend it to a basis \(\basis B = \{v_1, \dots, v_n\}\) for \(V\). With respect to \(\basis B\), \(\alpha\) has representation
    \[
      \begin{pmatrix}
        \lambda I_k & * \\
        0 & C
      \end{pmatrix}
    \]
    so
    \[
      \chi_\alpha(t) = \det(\alpha - t \iota) = (\lambda - t)^k \chi_{\overline \alpha}(t).
    \]
    Thus \(\chi_{\overline \alpha}(t)\) is also a product of linear factors. Since \(\chi_{\overline \alpha}(t)\) acts on a linear space of strictly smaller dimension, by induction hypothesis there is a basis \(w_{k + 1} + U, \dots, w_n + U\) for \(V/U\) with respect to which \(\overline \alpha\) has an upper-triangular matrix representation, say T. Then with respect to basis \(v_1, \dots, v_k, w_{k + 1}, \dots, w_n\), \(\alpha\) has matrix representation
    \[
      \begin{pmatrix}
        \lambda I_k & * \\
        0 & T
      \end{pmatrix}
    \]
  \end{itemize}
\end{proof}

\begin{eg}
  Let \(\F = \R\), \(V = \R^2\) and \(\alpha\) is a rotation. Then with respect to the standard basis \(\alpha\) has representation
  \[
    \begin{pmatrix}
      \cos \theta & \sin \theta \\
      -\sin \theta & \cos \theta
    \end{pmatrix}
  \]
  and thus \(\chi_\alpha(t) = t^2 - 2 \cos \theta t + 1\), which is irreducible in general. Thus \(\alpha\) is not triangulable over \(\R\).
\end{eg}

\begin{lemma}
  Let \(V\) be a \(n\)-dimensional \(\F\)-vector space and \(\alpha \in L(V)\) with \(\chi_\alpha(t) = (-1)^n t^n + c_{n - 1} t^{n - 1} + \dots c_0\). Then
  \begin{itemize}
  \item \(c_0 = \det \alpha\),
  \item \(c_{n - 1} = (-1)^{n - 1} \tr \alpha\) for \(\F = \R\) or \(\C\).
  \end{itemize}
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(c_0 = \chi_\alpha(0) = \det (\alpha - 0) = \det \alpha\).
  \item If \(\F = \R\) then there is a extension of scalars \(\M_n(\R) \embed \M_n(\C)\) induced by \(\R \embed \C\) (i.e.\ complexification). For \(\F = \C\),
    \[
      \chi_\alpha(t) = \det
      \begin{pmatrix}
        a_0 - t & \cdots & * \\
        & \ddots & \vdots \\
        0 & & a_n - t
      \end{pmatrix}
      = \prod_{i = 1}^{n} (a_i - t)
    \]
    where \(\sum_{i = 1}^n a_i = \tr \alpha\).
  \end{itemize}
\end{proof}
\end{document}