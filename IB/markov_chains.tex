\documentclass[a4paper]{article}

\def\npart{IB}

\def\ntitle{Markov Chains}
\def\nlecturer{G.\ R.\ Grimmett}

\def\nterm{Michaelmas}
\def\nyear{2017}

\input{header}

\newcommand*{\tot}{\leftrightarrow}

\begin{document}

\input{titlepage}

\tableofcontents

\section{Definition}

Let $X:\Omega \to S$ be a random variable. Then $(X_0,X_1, \ldots)$, a sequence of random variables, is called a \emph{stochastic/random process}. The problem we are interested in is whether there is any dependence between the random variables. Another example of a stochastic process is $(X_t, t \in \mathbb{R})$, representing, for example, the evolution of a system with respect to time.

\begin{definition}
  Let $X=(X_n:n=0,1,2,\ldots)$ be a sequence taking values in some \emph{state space} $S$, which is either finite or countably infinite. $X$ is a \emph{Markov chain} if it satisfies the \emph{Markov condition}:
  \begin{multline*}
    \prob(X_{n+1}=i_{n+1}|X_0=i_0,X_1=i_1,\ldots,X_n=i_n) = \prob(X_{n+1}=i_{n+1}|X_n=i_n) \\
    \forall n\geq0, i_0,\ldots,i_{n+1}\in S
  \end{multline*}

  $X$ is called \emph{homogeneous} if $\prob(X_{n+1}=j|X_n=i)$ does not depend on the value of $n$.
\end{definition}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Random walk is a Markov chain: let $Z_1, Z_2, \ldots$ be independent, $\prob(Z_i=1) = p,\prob(Z_i=-1) =1-p$, $X_n=Z_1+\cdots+Z_n$.
  \item Branching process: let $X_n$ be the size of the $n$th generation, then $(X_n)$ is a Markov chain.
  \end{enumerate}
\end{eg}

\begin{convention}
  Henceforth, unless contradicted, all chains are assumed to be homogeneous.
\end{convention}

Two quantities associated with a chain are:
\begin{enumerate}
\item initial distribution $\lambda = (\lambda_i: i\in S)$ where $\lambda_i = \prob(X_0=i)$, the probability mass function at $0$,
\item transition matrix $P = (p_{i,j}: i,j\in S)$ given by $p_{i,j} = \prob(X_1=j|X_0=i)$.
\end{enumerate}

\begin{proposition}\leavevmode
  \begin{enumerate}
  \item $\lambda$ is a distribution in that $\lambda_i\geq 0$ and $\sum_i \lambda_i = 1$.
  \item $P$ is a \emph{stochastic matrix} in that $p_{i,j} \geq 0, \sum_j p_{i,j} = 1$.
  \end{enumerate}
\end{proposition}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item $\lambda_i = \prob(X_0=i) \geq 0$, $\sum_i \lambda_i = \sum_i\prob(X_0=i)=1$, i.e. $\{X_0=i\}_{i\in X}$ partitions $\Omega$.
  \item $p_{i,j} = \prob(X_1=j|X_0=i) \geq 0$, $\sum_j p_{i,j} = \sum_j \prob(X_1=j|X_0=i) = 1$.
  \end{enumerate}
\end{proof}

\begin{theorem}
  Let $\lambda$ be a distribution on $S$ and $P$ be a stochastic matrix. The sequence $X=(X_n:n \geq 0)$ is a Markov chain with initial distribution $\lambda$ and transition matrix $P$ if and only if
  \begin{multline}\label{eqn:joint probability}
    \prob(X_0=i_0, X_1=i_1, \ldots, X_n=i_n) = \lambda_{i_0} p_{i_0,i_1} p_{i_1,i_2} \cdots p_{i_{n-1}, i_n} \\
    \forall n\geq 0, i_0,\cdots, i_n \in S \tag{$\ast$}
  \end{multline}
    
\end{theorem}

\begin{proof}
  Let $A_k = \{X_k=i_k\}$. Equation~\eqref{eqn:joint probability} is
\begin{equation}
  \label{eqn:joint probability2}
  \prob(A_0\cap A_1 \cap \cdots \cap A_n) = \lambda_{i_0} p_{i_0,i_1} p_{i_1,i_2} \cdots p_{i_{n-1}, i_n}
  \tag{$\star$}
\end{equation}

Suppose $X$ is a $(\lambda,P)$ Markov chain. Proof of equation~\eqref{eqn:joint probability2} by induction on $n$. When $n=0$, $\prob(X_0=i_0)=\lambda_{i_0}$. Suppose equation~\eqref{eqn:joint probability2} holds for $n<N$.
\begin{align*}
  \prob(A_0\cap\cdots\cap A_N) &= \prob(A_0\cap\cdots \cap A_N|A_0\cap\cdots\cap A_{N-1}) \prob(A_0\cap\cdots \cap A_{N-1}) \\
                            &= \prob(A_N|A_0\cap\cdots\cap A_{N-1})\prob(A_0\cap\cdots\cap A_{N-1}) \\
                            &\stackrel{\text{MP}}{=} \prob(A_N|A_{N-1}) \lambda_{i_0} p_{i_0,i_1} \cdots p_{i_{N-2},i_{N-1}}
  \end{align*}

Conversly, suppose equation~\eqref{eqn:joint probability2} holds. By the equation, when $n=0$,
\[
  \prob(X_0=i_0) = \lambda_{i_0},
\]
so $X_0$ has p.m.f. $\lambda$. Then
\[
  \prob(A_{n+1}|A_0\cap\cdots\cap A_n) = \frac{\prob(A_0\cap\cdots\cap A_{n+1})}{\prob(A_0\cap\dots\cap A_n)} = p_{i_n,i_{n+1}}.
    \]
    Therefore $X$ is a Markov chain with transition matrix $P$.
\end{proof}

\begin{theorem}[Extended Markov Property]
  Let $X$ be a Markov chain and $n\geq1$. Let $H$ be a historic event, i.e. $H$ is given in terms of $\{X_0,X_1,\ldots,X_{n-1}\}$, and let $F$ be a future event, i.e. $F$ is given in terms of $\{X_{n+1},X_{n+2},\ldots\}$. Then
  \[
    \prob(F|H, X_n=i) = \prob(F|X_n=i).
  \]
\end{theorem}

\begin{proof}
  For $F$ that depends only on finitely many of the future variables,
  \begin{align*}
    \prob(F|H,X_n=i) &= \frac{\sum_{>n}\sum_{<n}\lambda_{i_0}p_{i_0,i_1}\cdots p_{i_n,i}p_{i,i_{n+1}}\cdots}{\sum_{<n} \lambda_{i_0}p_{i_0,i_1}\cdots p_{i_{n-1},i}} \\
                  &= \sum_{>n} p_{i,i_{n+1}} \cdots \\
                  &= \prob(F|X_n=i)
  \end{align*}
  The case for infinite variables can be deduced using continuity of probability measure.
\end{proof}

\begin{notation}
  $\sum_{<n}$ denotes the summation over all $i_0,\ldots, i_{n-1}$ contibutions to $H/F$.
\end{notation}

\section{Transition Probabilities}

The one-step transition probability is $p_{i,j} = \prob(X_1=j|X_0=i)$. The $n$-step transition probability is $p_{i,j}(n) = \prob(X_n=j|X_0=i)$.

\begin{question}
  How to compute the $n$-step probabilities from the one-step probabilities?
\end{question}

The answer is: matrix. The one-step transition matrix is $P=(p_{i,j})_{i,j\in S}$. Similarly $P(n) = (p_{i,j}(n))_{i,j\in S}$.

\begin{theorem}
  \[
    P(n) = P^n.
  \]
\end{theorem}

\begin{proposition}[Chapman-Kolmogorov equations]
  \[
    p_{i,j}(m+n) = \sum_{k\in S} p_{i,k}(m) p_{k,j}(n).
  \]
 
\end{proposition}

\begin{proof}
  \begin{align*}
    \prob(X_{m+n}=j|X_0=i) &= \sum_{k\in S} \prob(X_{m+n}=j,X_m=k|X_0=i) \\
    \intertext{Use the equality $\prob(A\cap B|C) = \prob(A|B\cap C)\prob(B|C)$,}
                        &= \sum_{k\in S} \prob(X_{m+n}=j| X_m=k,X_0=i)\prob(X_m=k|X_0=i) \\
    \intertext{By Markov property, the first term can be simplified}
                        &= \sum_{k\in S} p_{i,j}(m) p_{k,j}(n)
  \end{align*}
\end{proof}

\begin{proof}[Proof of Theorem]
  By Chapman-Kolmogorov equation, $P(m+n) = P(m)P(n)$. Thus $P(n) = P(1)P(n-1)=\cdots =P(1)^n=P^n$.
\end{proof}

\begin{eg}
  Let $S = \{1,2\}$, $P=\begin{psmallmatrix} 1-\alpha & \alpha \\ \beta & 1-\beta\end{psmallmatrix}$ where $0<\alpha, \beta < 1$.
  \begin{enumerate}
  \item Diagonalisation method: $\det(P-\kappa I)=0$, has roots $\kappa_1=1,\kappa_2=1-\alpha-\beta$. Then
  \[
    P = U^{-1}
    \begin{pmatrix}
      1 & 0\\
      0 & 1-\alpha-\beta
    \end{pmatrix}U,
  \]
  for some invertible $U$. Then
  \[
    P^n = U^{-1}
    \begin{pmatrix}
      1^n & 0 \\
      0 & (1-\alpha-\beta)^n
    \end{pmatrix} U
    \]
    Thus we may write
    \[
      P_{1,1}(n) = A+B(1-\alpha-\beta)^n,
    \]
    with $P_{1,1}(0)=1,P_{1,1}(1)=1-\alpha$. Solve to get
    \begin{align*}
      A &= \frac{\beta}{\alpha+\beta} \\
      B &= \frac{\alpha}{\alpha+\beta}
    \end{align*}
    For the other entries, note $P_{1,2}(n)=1-P_{1,1}(n)$ and $P_{2,1}(n)$ and $P_{2,2}(n)$ can be obtained by exchanging $\alpha$ and $\beta$ due to symmetry.

  \item Difference equation method:
    \begin{align*}
      p_{1,1}(n+1) &= \sum_{k=1,2} p_{1,k}(n)p_{k,1}(1) \\
                   &= p_{1,1}(n)(1-\alpha) + p_{1,2}(n)\beta \\
                   &= p_{1,1}(n)(1-\alpha) + (1-p_{1,1}(n))\beta
    \end{align*}

    Thus $\pi_{n}=p_{1,1}(n)$ satisfies
    \[
      \pi_{n+1} = (1-\alpha-\beta)\pi_n+\beta
    \]
    which can be solved.
  \end{enumerate}
\end{eg}

It is convenient to think of \(\lambda\) as a row vector and \(P\) as a matrix. Then \(\prob(X_1=j) = \sum \lambda_ip_{i,j}\) becomes \((\prob(X_i=j):j\in S) = \lambda P\), i.e. pre-multiplying \(P\) by \(\lambda\).

\section{Class Structure}

\begin{definition}
  We say a state \(i\) \emph{leads} to a state \(j\) if \(p_{i,j}(n) > 0\) for some \(n\geq0\), write \(i \to j\).
  
  If \(i\to j\) and \(j\to i\), we say \(i\) and \(j\) \emph{communicate} and write \(i \tot j\).
\end{definition}

\begin{proposition}
  \(\tot\) is an equivalence relation.
\end{proposition}

\begin{proof}\leavevmode
  \begin{itemize}
  \item reflexive: \(p_{i,i}=1>0\),
  \item symmetric: if \(i\tot j\) then \(j\tot i\) by definition,
  \item transitive: if \(i\tot j, j\tot k\) then exist \(m, n\) such that \(p_{i,j}(m)>0, p_{j,k}(n)>0\). Then
    \[
      p_{i,k}(m+n) \stackrel{\text{CK}}{=} \sum_{r}^{ }p_{i,r}(m)p_{r,k}(n) \geq p_{i,j}(m)p_{j,k}(n) >0
      \]
      so \(i\to k\). Similarly \(k\to i\).
  \end{itemize}
\end{proof}

\begin{definition}
The equivalence classes of \(\tot\), i.e. subsets of \(S\) of the form \(C_i=\{j\in S: i\tot j\}\), are called \emph{communicating classes}.
\end{definition}

\begin{definition}
  If there is a unique equivalence class \(S\), call \(S\) (or the chain) \emph{irreducible}.
\end{definition}

\begin{definition}
  A subset \(C \subseteq S\) is called \emph{closed} if
  \[
i\in C, i\to j \Rightarrow j\in C.
\]

  If \(i \in S\) is such that \(\{i\}\) is closed, \(i\) is called \emph{absorbing}.
\end{definition}

A closed set is different from communicating class as it is ``one-way''.

\begin{proposition}
  \(C \subseteq S\) is closed if and only if
  \begin{equation}
    \label{eqn:closed subset}
    p_{i,j}=0 \text{ for } i\in C, j\notin C.
    \tag{\(\ast\)}
    \end{equation}
\end{proposition}

\begin{proof}
  Let \(C \subseteq S\). If \eqref{eqn:closed subset} fails, then there exists \(i\in C, j\notin C\) such that \(p_{i,j}>0\) so \(i\to j\) and so \(C\) is not closed.

  Suppose \eqref{eqn:closed subset} holds and exists \(m>0\) such that \(p_{i,j}(m) >0\). Then
  \[
0 < p_{i,j}(m) = \sum_{x_1,\ldots,x_{m-1}\in S} p_{i,x_1}p_{x_1,x_2}\cdots p_{x_{m-1},j}.
  \]
  Thus exists \(x_1,\ldots,x_{m-1}\in S\) such that a summand on RHS is larger than \(0\). By \eqref{eqn:closed subset} \(x_1,\ldots,x_{m-1},j \in C\) so \(C\) is closed.
\end{proof}

\begin{eg}
  \begin{minipage}[t]{0.45\textwidth}
  \[
    P=
    \begin{pmatrix}
      \frac{1}{2} & \frac{1}{2} & 0& 0&0 &0 \\
      0& 0&1&0&0&0 \\
      \frac{1}{3} &0&0&\frac{1}{3} & \frac{1}{3} &0 \\
      0&0&0&\frac{1}{2}&\frac{1}{2}& 0 \\
      0&0&0&0&0& 1 \\
      0&0&0&0&1&0
    \end{pmatrix}
  \]
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\linewidth}
  \begin{center}
   \begin{tikzpicture}[->,auto,node distance=1.5cm]
    \node[state] (1) {\(1\)};
    \node[state,above right of=1] (2) {\(2\)};
    \node[state,right of=1] (3) {\(3\)};
    \node[state,below right of=3] (4) {\(4\)};
    \node[state,right of=3] (5) {\(5\)};
    \node[state,right of=5] (6) {\(6\)};

    \draw
    (1)
    edge[loop left] node{} (1)
    edge[] node{} (2)
    (2)
    edge[] node{} (3)
    (3)
    edge[] node{} (1)
    edge[] node{} (4)
    edge[] node{} (5)
    (4)
    edge[loop left] node{} (4)
    edge[] node{} (5)
    (5)
    edge[bend right] node{} (6)
    (6)
    edge[bend right] node{} (5);
  \end{tikzpicture}
  \end{center}
\end{minipage}
  has communicating classes \(\{1,2,3\}, \{4\},\{5,6\}\), only the last of which is closed.
\end{eg}

\section{Recurrence \& Transience}

\subsection{Definition}

Introduce notation \(\prob_i(\cdot) = \prob(\cdot| X_0 = i), \E_i(\cdot) = \E(\cdot|X_0=i)\).

\begin{definition}
The \emph{first-passage time} of \(j\in S\) is
\[
T_j = \min \{n\geq 1: X_n=j\}.
\]
The \emph{first-passage probability} are
\[
f_{i,j}(n) = \prob_i(T_j=n).
\]
\end{definition}

\begin{definition}
  \(i\in S\) is \emph{recurrent (or persistent)} if
  \[
\prob_i(T_i < \infty) = 1,
  \]
  and \emph{transient} otherwise.
\end{definition}

\subsection{Recurrent Condition}

\begin{theorem}
  \label{thm:recurrent}
  \(i\) is recurrent if and only if
  \[
    \sum_{n\geq0}^{}p_{i,i}(n) = \infty.
  \]
\end{theorem}

Before we prove the theorem, introduce two generating functions:
\begin{align*}
  P_{i,j}(s) &= \sum_{n\geq0}^{ }p_{i,j}(n)s^n \\
  F_{i,j}(s) &= \sum_{n\geq0}^{ }f_{i,j}(n)s^n
\end{align*}
with the convention that
\begin{align*}
  p_{i,j}(0) &= \delta_{i,j} =
                 \begin{cases}
                   1 & i = j \\
                   0 & i \neq j
                 \end{cases}\\
  f_{i,j}(0) &= 0 \, \forall i,j
\end{align*}

Note that
\begin{align*}
  p_{i,j}(n) &= \sum_{m=1}^{n}\prob_i(X_n=j|T_j=m)\prob_i(T_j=m) \\
             &\stackrel{\text{MP}}{=} \sum_{m=1}^{n}p_{j,j}(n-m)f_{i,j}(m)
\end{align*}
which is the convolution of \(p\) and \(f\).

It follows that for \(n\geq 1\),
\begin{align*}
  \sum_{n\geq1}^{ } p_{i,j}(n)s^n &= \sum_{n\geq1}^{ } \sum_{m=1}^{n} \Big( f_{i,j}(m)s^m \Big) \Big(p_{j,j}(n-m)s^{n-m} \Big) \\
  P_{i,j}(s) -\delta_{i,j} &= \sum_{m=1}^{\infty} \sum_{n=m}^{\infty} \Big( f_{i,j}(m)s^m \Big) \Big(p_{j,j}(n-m)s^{n-m} \Big) \\
                                  &= \sum_{m\geq1}^{ } f_{i,j}(m)s^m \sum_{r\geq0}^{} p_{j,j}(r)s^r \\
                                  &= F_{i,j}(s) P_{j,j}(s)
\end{align*}
be careful when dealing with double summations.

\begin{theorem}
  \[
    P_{i,j}(s) = \delta_{i,j} + F_{i,j}(s)P_{j,j}(s) \text{ for } |s| < 1.
  \]
\end{theorem}

The \(|s| < 1\) condition: since \(|F_{i,j}(s)| < \infty\) if \(|s| < 2\),
\[
  |P_{i,j}(s)| \leq \sum_{n}^{ }|s|^n < \infty 
\]

A lemma from analysis before we finally prove Theorem~\ref{thm:recurrent}:
 \begin{lemma}[Abel's Lemma]
  Let \((u_i)_{i\geq0}\) be a non-negative sequence such that
  \[
    \mathcal U(s) = \sum_{i=0}^{\infty}u_is^i
  \]
  converges when \(|s| <1\), then
  \[
    \sum_{i=0}^{\infty}u_i = \lim_{s\to 1^-} \mathcal U(s),
  \]
  whether or not this limit is finite.
\end{lemma}

\begin{proof}
  Exercise.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:recurrent}]
  For \(|s| < 1\), then
  \begin{align*}
    p_{i,i}(s) &= 1+ F_{i,i}P_{i,i} \\
    P_{i,i} &= \frac{1}{1-F_{i,j}}
  \end{align*}
Let \(s \to 1^-\), then by Abel's Lemma
\begin{equation}
  \label{eqn:recurrent P}
  P_{i,i}(1) = \infty \Leftrightarrow F_{i,i}(1) =1
\end{equation}
i.e. \(i\) is recurrent.
\end{proof}

\subsection{Properties of Recurrence}

\begin{theorem}[Recurrence as a class property]
  Let \(C\) be a communicating class, then
  \begin{enumerate}
  \item either every state in \(C\) is recurrent or every state is transient,
  \item if \(C\) contains some recurrent state then \(C\) is closed.
  \end{enumerate}
\end{theorem}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item Let \(i,j \in C\), \(i \neq j\). There exist \(m,n\geq 1\) such that
    \[
      \alpha = p_{i,j}(m)p_{j,i}(n) >0.
    \]
    This is simply a restatement of the communicating property. Then
    \begin{align*}
      p_{i,j}(m+k+n) \geq p_{i,j}(m)p_{j,j}(k)p_{j,i}(n) = \alpha p_{j,j}(k)
    \end{align*}
    Intuitively, the middle term is the probability of going from \(i\) to \(i\) by passing \(j\) at step \(m\) and \(m+k\).
    Thus
    \[
      \sum_{k}^{}p_{i,i}(m+k+n) \geq \alpha \sum_{k}^{ }p_{j,j}(k).
    \]
    Thus \(j\) recurrent implies that \(i\) is recurrent. Vice versa.
  \item Suppose \(i\in C\) is recurrent but \(C\) is not closed. Then there exists \(j\in C, k\notin C\) with \(p_{j,k} > 0\). By the previous part \(j\) is recurrent so
    \[
      1 = \prob_j(T_j < \infty) = 1 - \prob_j(\text{no return to } j) \leq 1- p_{j,k} < 1
    \]
    Absurd.
  \end{enumerate}
\end{proof}

\begin{theorem}
  \label{thm:recurrent irreducible chain}
  Assume \(|S| < \infty\), then
  \begin{enumerate}
  \item \(S\) contains some recurrent state,
  \item if the chain is irreducible, all states are recurrent.
  \end{enumerate}
\end{theorem}

Similar as before, we need a proposition before the proof:
  \begin{proposition}
    If \(j\) is a transient state then
    \[
      \forall i,\, p_{i,j}(n) \to 0 \text{ as } n\to\infty
    \]
  \end{proposition}

  \begin{proof}
    Asuume \(j\) is transient. By \eqref{eqn:recurrent P},
    \[
      P_{j,j}(1) < \infty.
    \]
    By Theorem~\ref{thm:recurrent},
    \[
      \sum_{n}^{ }p_{i,j}(n) = P_{i,j}(1) < \infty
    \]
    so \(n\)th term \(p_{i,j}(n)\) tends to \(0\) as \(n\to \infty\).
  \end{proof}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item \(\sum_{j\in S}^{ }p_{i,j}(n) = 1 \) since \(P\) is a stochastic matrix. If \(j\) is transient then each summand tends to \(0\) as \(n\to \infty\), which is absurd since \(|S| < \infty\).
  \item Obvious.
  \end{enumerate}
\end{proof}

\subsection{Random Walks and Pólya's Theorem}

In this section, we discuss simple symmetric random walk on \(d\)-dimensional lattices, i.e.\ \(\Z^d\), in particular answering the question when the chain is recurrent.\footnote{Note that this chain is irreducible so by Theorem~\ref{thm:recurrent irreducible chain} either we can talk about recurrence as a chain property.}  It turns out that there is a surprisingly beautiful result.

We call two points \(x\) and \(y\) \emph{neighbours} if
\[
  \sum_{i}^{ }|x_i-y_i| = 1,
\]
i.e.\ they differ by \(1\) in only one coordinate. Let \(X\) be a symmetric random walk on \(\Z^d\) where \(d \geq 1\), i.e.\ \(X = (X_1, X_2, \dots)\) is a Markov chains with state space \(S = \Z^d\) and transition probability
\[
  \prob(X_{n+1} = y | X_n = x) =
  \begin{cases}
    0 & \text{ if } x \text{ and } y \text{ are not neighbours} \\
    \frac{1}{2d} & \text{ if } x \text{ and } y \text{ are neighbours}
  \end{cases}
\]

\begin{theorem}[Pólya's]
  \(X\) is recurrent if \(d \leq 2\) and transient if \(d \geq 3\).
\end{theorem}

\begin{proof}
  First set \(d = 1\). Recall that \(0\) is recurrent if and only if \(\sum_n p_{0,0}(n) = 0\). However, it is not possible to return to the same place after an odd number of steps so the expression simplifies to
  \begin{equation}
    \label{eqn:polya}
    \sum_n p_{0, 0}(2n)
    \tag{\(\ast\)}
  \end{equation}
  In the even case, the random walk returns to \(0\) if and only if there are equal number of movement to either direction so by applying the binomial distribution,
  \[
    p_{0,0}(2n) = \left( \frac{1}{2} \right)^{2n} \binom{2n}{n}
  \]
  To simplify this, recall Sterling's formula
  \[
    n! \sim \left( \frac{n}{e} \right)^n \sqrt{2\pi n} \text{ as } n \to \infty.
  \]
  So
  \[
    p_{0,0}(2n) \sim \frac{1}{\sqrt{\pi n}}
  \]
  and the sum~\eqref{eqn:polya} tend to infinity.

  Now let \(d = 2\). By the same reasoning and generalising binomial to multinomial coefficients, we get
  \begin{align*}
    p_{0,0}(2n) &= \left( \frac{1}{4} \right)^{2n} \sum_{m=0}^{n} \binom{2n}{m, m, n-m, n-m} \\
                &= \left( \frac{1}{4} \right)^{2n} \sum_{m=0}^{n} \frac{(2n)!}{(m!)^2 ((n-m)!)^2} \\
                &= \left( \frac{1}{4} \right)^{2n} \frac{(2n)!}{(n!)^2} \sum_{m=0}^{n} \binom{n}{m} \binom{n}{n-m} \\
    \intertext{Now pause and think: the summation represents the number of ways to take \(m\) balls from a bag of \(n\) balls and take \(n-m\) balls from another bag of \(n\) balls, for \(0 \leq m \leq n\), but this is precisely the number of ways to take \(n\) balls from \(2n\) balls!}
                &= \left( \frac{1}{4} \right)^{2n} \binom{2n}{n} \binom{2n}{n} \\
                &= \left( p_{0,0}^{d=1}(2n) \right)^2
  \end{align*}
  thus the sum~\eqref{eqn:polya} also tends to infinity and \(0\) is recurrent.

  Let \(d = 3\) (similar for \(d \geq 4\)) and we have
  \begin{align*}
    p_{0,0}(2n) &= \left( \frac{1}{6} \right)^{2n} \sum_{i+j+k = n} \binom{2n}{i,i,j,j,k,k} \\
                &= \left( \frac{1}{6} \right)^{2n} \sum_{i+j+k = n}^{}\frac{(2n)!}{(i! j! k!)^2} \\
                &= \left( \frac{1}{6} \right)^{2n} \binom{2n}{n} \sum_{i+j+k = n}^{} \left( \frac{n!}{i! j! k!} \right)^2 \\ 
                &=  \left( \frac{1}{2} \right)^{2n} \binom{2n}{n} \sum_{i+j+k = n}^{} \left( \frac{n!}{3^n i! j! k!} \right)^2 \\
                &\leq \left( \frac{1}{2} \right)^{2n} M_n \sum_{i+j+k = n}^{}\frac{1}{3^n i! j! k!} \\
    \intertext{ where \(M_n = \max \{ \frac{n!}{3^n i!j!k!}, i+j+k = n \} \).}
    \intertext{The reason we introduce \(3^n\) becomes apparent in this step: \( \frac{n!}{3^n i! j! k!}\) is the probability of, upon throwing \(n\) balls into \(3\) urns, finding \(i, j, k\) in each respectively. Thus they sum up to \(1\).}
                &\leq \left( \frac{1}{2} \right)^{2n} \binom{2n}{n} \frac{n!}{3^n \left( \floor{n/3}! \right)^3}
  \end{align*}
  The upper bound of \(M_n\) is left as an exercise. By Sterling's formula,
  \[
    p_{0,0}(2n) \leq \frac{C}{n^{3/2}}
  \]
  so the sum is finite and \(0\) is recurrent.
\end{proof}

We have seen that the probability \(p_{0, 0}(2n)\) when \(d = 2\) is the square of the probability when \(d = 1\), but when \(d = 3\) it doesn't become cubed. It should inspire us to suspect that the \(d = 2\) case is simple enough such that the random walks in two directions are ``independent'', but when \(n \geq 3\) there is some hidden structure that destroys such independence. What is so special about dimension two?

There is an alternative way to tackle this problem that might be more lucid and shed some light on the magical property of \(d = 2\). Instead of cartesian coordinates \(X_n = (A_n, B_n)\), rotate the axes by \(45^\circ\) clockwise. The new coordinates, scaled by a constant factor for convenience, are
\[
  Y_n =
  \begin{pmatrix}
    U_n \\
    V_n
  \end{pmatrix}
  = \sqrt 2
  \begin{pmatrix}
    \cos 45^\circ & -\sin 45^\circ \\
    \sin 45^\circ & \cos 45^\circ
  \end{pmatrix} X_n
  =
  \begin{pmatrix}
    A_n - B_n \\
    A_n + B_n
  \end{pmatrix}
\]
\iffalse
\begin{align*}
  U_n &= A_n - B_n \\
  V_n &= A_n + B_n \\
  Y_n &= (U_n, V_n)
\end{align*}
\fi

Claim \(U = (U_n)\) and \(V = (V_n)\) are \emph{independent} random walks on \(\Z\):

\begin{proof}
  \begin{align*}
    \prob(Y_{n+1} &= Y_n+ (1,1)) = \prob(X_{n+1} = X_n + (1,0)) = \frac{1}{4} \\
                  &= \prob(U_{n+1} - U_n = 1, V_{n+1} - V_n = 1)
  \end{align*}
  since during one step \(X_n\) can only change by \(1\) in one coordinate. Similar for the other three cases.

  So
  \[
    \prob(U_{n+1} - U_n = \alpha, V_{n+1} - V_n = \beta) = \left( \frac{1}{2} \right)^2 = \prob(U_{n+1} - U_n = \alpha) \prob(V_{n+1} - V_n = \beta)
  \]
  for \(\alpha, \beta = \pm 1\). \(U_n\) and \(V_n\) are independent and each generates a random walk on \(\Z\).
\end{proof}

By independence
\[
  \prob_0(X_n = (0,0)) = \prob_0(Y_n = (0,0)) = \prob_0(U_n = 0) \prob_0(V_n = 0)
\]
so
\[
  p_{0,0}^{d=2}(2n) = \left( p_{0,0}^{d=1} \right)^2.
\]

The moral of this calculation is, two dimensional random walk is indeed the product of two one dimensional cases, but in a not-entirely-straightforward way.

\section{Hitting Time and Probability}

\begin{definition}[Hitting Time]
  Given a subset \(A \subseteq S\), the \emph{hitting time} of \(A\) is
  \[
    H^A = \inf\{n\geq 0: X_n \in A \}
  \]
\end{definition}
Note that \(\inf \emptyset = \infty\) so \(H^A: \Omega \to \{0, 1, \dots\} \cup \{\infty\}\).

\begin{definition}[Hitting probability]
  The \emph{hitting probability} is \(h_i^A = \prob_i(H^A < \infty)\).
\end{definition}

By Markov property, hitting probability satisfies the equation
\begin{equation}
  \label{eqn:hitting prob}
  h_i^A =
  \begin{cases}
    1 & i \in A \\
    \sum_{j \in S} p_{ij}h_j^A & i \notin A
  \end{cases}
  \tag{\(\ast\)}
\end{equation}

\begin{theorem}
  The vector \(h^A = (h_i^A: i \in S)\) is the minimal non-negative solution to~\eqref{eqn:hitting prob} in that for any \(x = (x_i: i \in S)\) satisfy
  \[
    x_i =
    \begin{cases}
      1 & i \in A \\
      \sum_j p_{ij}x_j & i \notin A
    \end{cases}
  \]
  and \(x_i \geq 0\) for \(i \in S\), \(h_i^A \leq x_i\) for \(i \in S\).
\end{theorem}

\begin{proof}
  By MP, \(h^A\) satisfies~\eqref{eqn:hitting prob}. Suppose \(x = (x_i: i \in S)\) satisfies the hypothesis in the theorem. If \(i \in A\), \(x_i = 1 = h_i^A\) so \(h_i^A \leq x_i\). Let \(i \notin A\), then
  \begin{align*}
    x_i &= \sum_j p_{ij}x_j \\
        &= \sum_{j \in A}p_{ij}\cdot 1 + \sum_{j \notin A} p_{ij}x_j \\
        &\geq \sum_{j \in A} p_{ij} \\
        &= \prob_i(H^A = 1)
  \end{align*}
  Similarly
  \begin{align*}
    x_i &= \prob_i(H^A = 1)  + \sum_{j \notin A}^{ } p_{ij} \left( \sum_{k \in A}^{ } p_{jk}x_k + \sum_{k \notin A}^{ } p_{jk}x_k \right) \\
        &\geq \prob_i(H^A = 1) + \prob_i(H^A = 2)
  \end{align*}
  so by induction
  \[
    x_i \geq \sum_{m = 1}^{n} \prob_i(H^A = m) = \prob_i(H^A \leq n) \to \prob_i(H^A < \infty) = h_i^A
  \]
  as \(n \to \infty\).
\end{proof}

\begin{definition}[Mean hitting time]
  The \emph{mean hitting time} is
  \[
    k_i^A = \E_i(H^A).
  \]
\end{definition}

\begin{note}
  \(k_i^A = \infty\) if \(h_i^A < 1\).
\end{note}

\begin{theorem}
  The vector \(k^A = (k_i^A: i \in S)\) is the minimal non-negative solution to the equation
  \begin{equation}
    \label{eqn:hitting time}
    y_i =
    \begin{cases}
      0 & i \in A \\
      1 + \sum_{j}^{ } p_{ij}y_j & i \notin A
    \end{cases}
    \tag{\(\dag\)}
  \end{equation}
\end{theorem}

\begin{proof}
  By MP, \(k^A\) satisfies~\eqref{eqn:hitting time}. Let \(y = (y_i: i \in S)\) be a non-negative solution to~\eqref{eqn:hitting time}. Let \(i \in A\) then \(y_i = 0 = k_1\). Let \(i \notin A\).
  \begin{align*}
    y_i &= 1 + \sum_{j \in S}^{ } p_{ij}y_j \\
        &= 1 + \sum_{j \notin A}^{ } p_{ij}y_j \\
        &= 1 + \sum_{j \notin A}^{ } p_{ij} \left( 1+ \sum_{k \notin A}^{ } p_{jk}y_k \right) \\
        &\geq \prob_i(H^A \geq 1) + \prob_i(H^A \geq 2)
  \end{align*}
  By induction
  \[
    y_i \geq \sum_{m = 1}^{n} \prob_i(H^A \geq m) \to \sum_{m = 1}^{\infty} \prob_i(H^A \geq m) = \E_i(H^A)
  \]
  as \(n \to \infty\).
\end{proof}

\begin{eg}[Gambler's ruin]
  Let \(S = \{0, 1, 2, \dots \}\) and \(0 < p < 1\). Take a random walk on \(S\) which moves one step right with probability \(p\) and left with probability \(q = 1 - p\). \(0\) is an \emph{absorbing barrier} as as soon as the random walk hits \(0\) it ends.

  \begin{question}
    What is the probability of absorption at \(0\) starting at \(i\)?
  \end{question}

  \begin{answer}
    Let \(h_i = h_i^{\{0\}}\), then
    \begin{align*}
      h_0 &= 1 \\
      h_i &= p h_{i+1} + q h_{i-1}
    \end{align*}
    which can be regarded as a second order difference equation with initial conditions.

    First suppose \(p \neq q\). The general solution is
    \[
      h_i = A + B \left( \frac{q}{p} \right)^i \text{ for } i \geq 0.
    \]
    Suppose \(p < q\). Since \(h^i \leq 1\), \(B = 0\) or otherwise for large \(i\), \(h^i\) will blow up. Thus \(h_i = h_0 = 1\) for \(i \geq 0\). Now suppose \(p > q\). Eliminate \(B\) to get
    \[
      h_i = \left( \frac{q}{p} \right)^i + A \left( 1 - \left( \frac{q}{p} \right)^i \right)
    \]
    The minimality of \(h^i\) requires that \(A = 0\) so
    \[
      h_i = \left( \frac{q}{p} \right)^i
    \]

    Finally, consider the case \(p = q = \frac{1}{2}\). As above \(B = 0, A = 1\) so \(h_i = 1\) for \(i \geq 0\).
  \end{answer}
\end{eg}

\begin{eg}[Birth-death chain]
  This is similar to gambler's ruin but with inhomogeneuous transition probabilites: \(p_i + q_i = 1, p_i \in (0, 1)\). The governing equations are
  \begin{align*}
    h_0 &= 1 \\
    h_i &= p_i h_{i+1} + q_i h_{i-1}
  \end{align*}
  At first glance, it seems nothing like a second order differential equation. However, rearrange to get
  \[
    p_i(h_i - h_{i + 1}) = q_i(h_{i - 1} -h_i)
  \]
  Define \(u_i = h_{i - 1} - h_i\) then
  \[
    p_i u_{i + 1} = q_i u_i
  \]
  so
  \begin{align*}
    u_i &= \frac{q_i q_{i - 1} \dots q_1}{p_i p_{i - 1} \dots p_1} u_1 \\
        &= \gamma_i u_1
  \end{align*}
  where
  \[
    \gamma_i = \frac{\prod_{j = 1}^{i} q_j}{\prod_{j = 1}^{i} p_j}.
  \]
  As
  \begin{align*}
    h_i &= 1 - (u_1 + u_2 + \dots + u_i) \\
        &= 1 - u_1(\gamma_0 + \dots + \gamma_{i - 1}) \text{ for } i \geq 1
  \end{align*}
  where \(\gamma_0 = 1\). Let \(S = \sum_{i = 0}^{\infty} \gamma_i \). If \(S = \infty\) then \(u_1 = 0\) and \(h_i = 1\). If \(S < \infty\), \(1 - u_1 S = 0\) so
  \[
    h_i = 1 - \frac{\sum_{j = 0}^{i - 1}\gamma_j}{\sum_{j = 0}^{\infty}\gamma_j}
  \]
\end{eg}

\section{Stopping Times \& Strong Markov Property}

\begin{definition}[Stopping time]
  Let \(X\) be a Markov chain. A \emph{stopping time (or Markov time)} is a random variable \(T: \Omega \to \{0, 1, \dots\}\cup \{\infty\}\) such that for \(n \geq 0\), the event \(\{T = n\}\) is given in terms of \(X_0, \dots, X_n\).
\end{definition}

\begin{note}
  The definition can be equivalently formulated using languages of meausure theory. Recall that in a probability space \(\Omega, \mathcal F, \prob\), a random variable \(X: \Omega \to \R\) is measurable if
  \[
    X^{-1}((-\infty, n]) \in \mathcal F \, \forall n \in \R.
  \]
  Define the \(\sigma\)-field generated by \(X\) to be
  \[
    \sigma(X) = \sigma(\{X^{-1}((-\infty, n]): n \in \R \}) \subseteq \mathcal F
  \]
  and the definition basically says \(\{T = n \} \in \sigma(\{X_0, \dots X_n\})\).
\end{note}

\begin{theorem}[Strong Markov property]
  Let \(X\) be a Markov chain with transition matrix \(P\) and let \(T\) be a stopping time. Given \(T < \infty\) and \(X_T = i\),
  \[
    Y := (X_T, X_{T + 1}, X_{T + 2}, \dots),
  \]
  the future process of \(X\), is a Markov chain with transition matrix \(P\) and \(Y\) is independent of \(X_0, \dots, X_{T - 1}\).
\end{theorem}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Hitting time \(H^A\) is a stopping time:
    \[
      \{H^A = n\} = \{X_n \in A\} \cap \left( \bigcap_{m = 0}^{n - 1} \{X_m \notin A \} \right)
    \]
  \item \(H^A + 1\) is a stopping time.
  \item \(H^A - 1\) is \emph{not} a stopping time: \(\{H^A - 1 = n\}\) obviously depends on \(X_{n + 1}\).
  \end{enumerate}
\end{eg}

\begin{eg}[Gambler's ruin]
  Let \(H = H^{\{0\}}\). We have proved that
  \[
    \prob_i(H < \infty) =
    \begin{cases}
      = 1 & q \geq p \\
      < 1 & q < p
    \end{cases}
  \]
  if \(i \geq 1\). However, we want to find the probability mass function of \(H\) given \(X_0 = 1\). Use probability generating function:
  \begin{align*}
    G(s) &= \E_1(s^H) \\
    \intertext{where \(|s| < 1\) so that \(s^\infty\) can be interpreted as \(0\)}
         &= \sum_{n = 1}^{\infty} s^n \prob_1(H = n) \\
  \end{align*}
  where the limit \(s \to 1^-\) is studied via Abel's Lemma.

  \begin{align*}
    G(s) &= \E_1(s^H | X_1 = 0)q + \E_1(s^H | X_1 = 2)p \\
         &= qs + p \E_1(s^{1 + H' + H''}) \\
           \intertext{where \(H'\) is the hitting time of \(1\) starting at \(2\) and \(H''\) is the subsequent time needed to reach \(0\). By strong Markov property, \(H'\) and \(H''\) are independent and distributed as \(H\) so}
         &= qs + ps G(s)^2
  \end{align*}
  Hence
  \[
    G(s) = \frac{1 \pm \sqrt{1 - 4pqs^2}}{2ps}
  \]
  Note that \(G\) is continuous on \((-1, 1)\) since it is the a power series. Since \(\sqrt{1 - 4pqs^2} \neq 0\) for \(|s| < 1\), we must choose a sign and stick with it on \((-1, 1)\). Since \(G\) convergens \((-1, 1)\), \(+\) is impossible as otherwise \(G\) does not converge at \(s = 0\). Thus
  \[
    G(s) = \frac{1 - \sqrt{1 - 4pqs^2}}{2ps} = \sum_{n = 1}^{\infty} s^n \prob_1(H = n)
  \]
  hence \(\prob_1(H = n) \) can be found by expanding \(G\).

  In addition
  \begin{align*}
    \lim_{s \to 1^-} G(s) &= \sum_{n = 1 }^{\infty} \prob_1(H = n) \\
                          &= \prob_1(H < \infty) \\
                          &= \frac{1 - \sqrt{1- 4pq}}{2p} \\
                          &= \frac{1 - |p - 1|}{2p} \\
                          &=
                            \begin{cases}
                              1 & q \geq p \\
                              \frac{q}{p} & q < p
                            \end{cases}
  \end{align*}

  To find \(\E_1(H)\) when \(q \geq p\), differentiate to get
  \[
    G' = q + p G^2 + 2ps G G'
  \]
  so
  \[
    G'(s) = \frac{q + pG^2}{1 - 2psG}
  \]
  By \Cref{tbf} Abel's theorem
  \[
    E_1(H) = \lim_{s \to 1^-} G'(s) = \frac{q + p}{1 - 2p} = \frac{1}{q - p}.
  \]
\end{eg}

\section{Classification of States}

Depending on ht mean time to return to the recurrent state: finite or infinite

\begin{theorem}
  Let \(X_0 = i\) and \(V_i = |\{n \geq 1: X_n = i\}|\). Then \(V_i\) has a geometric distribution
  \[
    \prob_i(V_i = r) = f^r (1 - f), r \geq 1
  \]
  where \(f = f_{i, i} \in [0, 1]\).
\end{theorem}

\begin{proof}
  \begin{align*}
    \prob_i(V_i \geq r) &= \prob_i(T^r < \infty) \\
                        &= \prob_i(T^4 < \infty | T^{r - 1} < \infty) \prob_i(T^{r - 1} < \infty) \\
                        &= f \cdot \prob_i(V_i \geq r - 1) SMP \\
                        &= f^r
  \end{align*}
  where
  \[
    T^r =
    \begin{cases}
      time of rth return & \\
      \infty & \text{ if } V_i < r
    \end{cases}
  \]
  Then
  \[
    \prob_i(V_i = r) = \prob_i(V_i \geq r) - \prob_i(V_i \geq r + 1) = f^r(1 - r)
  \]
\end{proof}

\begin{note}
  If \(f < 1\), \(\prob_i(V_i < \infty) = 1\) and if \(f = 1\), \(\prob_i(V_i = \infty) = 1\).
\end{note}

\begin{definition}[Mean recurrence time]
  The \emph{mean recurrence time} of \(i \in S\) is
  \[
    \mu_i = \E_i(T_i) =
    \begin{cases}
      \infty & \text{if \(i\) is transient} \\
      \sum_{n = 1}^{\infty} n f_{i,i}(n) &
    \end{cases}
  \]

  Let \(i\) be recurrnet. Then \(i\) is \emph{null} if \(\mu_i = \infty\) and \emph{positive} if \(\mu_i < \infty\).

  The \emph{period} of \(i \in S\) is
  \[
    d_i = \gcd\{n: p_{i,i}(n) > 0 \}.
  \]

  \(i\) is \emph{aperiodic} if \(d_i = 1\).

  \(i\) is \emph{ergodic} if it is recurrent, positive and aperiodic.
\end{definition}

\begin{theorem}
  Let \(i \tot j\). Then
  \begin{enumerate}
  \item \(d_i = d_j\), i.e.\ period is a class property.
  \item \(i\) is recurrent if and only if \(j\) is recurrent.
  \item \(i\) is positive recurrent if and only if \(j\) is positive recurrent.
  \item \(i\) is ergodic if and only if \(j\) is ergodic.
  \end{enumerate}
\end{theorem}

\begin{proof}
  2 has already been proved and 4 follows from 1, 2 and 3. 3 will be proved later. To prove 1:

  Let \(i \tot j\) and \(i \neq j\).
  \begin{align*}
    D_k = \{n \geq 1: p_{k,k}(n) > 0 \} \\
    d-k = \gcd\{D_k\}
  \end{align*}
  Since \(i \to j\), there exists \(m, n \geq 1\) such that \(\alpha = p_{i,j}(m)p_{i,j}(n) > 0\). By CK,
  \[
    p_{i,i}(m + r + n) \geq \alpha p_{j,j}(r).
  \]
  Thus if \(r \in D_j \cup \{0\}\), \(p_{i,i} (m + r + n) > 0\) and hence \(d_i \divides m + r + n\) and hence \(d_i \divides r\) since if \(r = 0\), \(d_i \divides m + n\).

  Thus \(d_i \divides d_j\). Similarly \(d_j \divides d_i\) and hence \(d_i = d_j\).
\end{proof}

\begin{proposition}
  If a chain is irreducible and let \(j \in S\) be recurrent. Then
  \[
    \prob(T_j < \infty) = \prob(X_n = j \text{ for some } n \geq 1) = 1.
  \]
\end{proposition}
Compare to the definition of recurrence, \(\prob_j\)

\begin{proof}
  \(f_{i,j} = \prob_i(T_j < \infty)\). Let \(i \neq j\). Claim that \(p_{j, i}(m)(1 - f_{i,j}) \leq 1 - f_{j,j}\) where \(m= \inf\{r: p_{j,i}(r) > 0\}\). Then \(p_{j, i}(m) = \prob_j(X_m = i, X_r \neq j \text{ for } 1 \leq r < m)\). We have \(f_{j,j} = 1\) and hence \(f_{i,j} = 1\).

  Thus \(\prob_i(T_j < \infty) = 1\). Let \(\lambda_i = \prob(X_0 = i)\),
  \[
    \prob(T_j < \infty) = \sum_{i \in S} \prob_i(T_j < \infty) \lambda_i = 1.
  \]
\end{proof}

\section{Invariant Distributions}

What happens to \(X_n\) as \(n \to \infty\)? Random variables are functions so we are talking about convergence of a sequence of functions. There are lots of modes of convergence on function space. When studying Markov chains, it turns out there is a unique convergence that we need: does \(\prob(X_n = i)\) converge as \(n \to \infty\).

\[
  \prob(X_{n + 1} = j) = \sum_i \prob(X_{n + 1} = j | X_n = 0) \prob(X_n = i)
\]

\[
  \pi_j = \sum_i p_{i,j} \pi_i
\]
\(\pi = \pi P\), eigenvalue problem.

\begin{definition}[Invariant distribution]
  \(X\) is aMarkov chain with transition matrix \(P\). The vector \(\pi = (\pi_i: i \in S)\) is an \emph{invariant distribution} if
  \begin{enumerate}
  \item \(\pi_i \geq 0, \sum_i \pi_i = 1\),
  \item \(\pi = \pi P\)
  \end{enumerate}
\end{definition}

If \(X_0\) has distribution \(\pi\), \(X_n\) has distribution
\[
  \pi P^n = (\pi P) P^{n - 1} = \pi P^{n - 1} = \cdots \pi
\]

\begin{theorem}
  Let \(X\) be an irreducible Markov chain. Then
  \begin{enumerate}
  \item There exists an invariant distribution if and only if some state of the chain is positive recurrent.
  \item If there exists an invariant distribution \(\pi\) then every state is positive recurrent and
    \[
      \pi_i = \frac{1}{\mu_i}
    \]
    for \(i \in S\) where \(\mu_i\) is the mean recurrence time. In particular \(\pi\) is unique.
  \end{enumerate}
\end{theorem}

Fix \(k \in S\), start at \(k\). Let \(W_i\) be the number of visits to \(i\) up to the first return time to \(k\), i.e.
\[
  W_i = \sum_{m = 1}^{\infty} \V 1(X_m = i, T_k \geq m) = \sum_{m = 1}^{T_k} \V 1(X_m = i)
\]
where \(\V 1(\cdot)\) is the indicator function.

Let \(\rho(i) = \E_k(W_i)\).

\begin{proposition}
  Suppose the chain is irreducible and recurrent, \(k \in S\). \(\rho = (\rho_i: i \in S)\) satisfies
  \begin{enumerate}
  \item \(\rho_k = 1\).
  \item \(\sum_i \rho(i) = \mu_k\) whether or not \(\mu_k < \infty\).
  \item \(\rho = \rho P\).
  \item \(0 < \rho_i < \infty\) for \(i \in S\).
  \end{enumerate}
\end{proposition}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item Immediate from the definition.
  \item
    \[
      \E_k \sum_{i \in S}^{ } W_i = E_k T_k
    \]
    Assuming we can interchange summations (since \(\E\) is the limit of a series), since all summands are non-negative,
    \[
      \sum_{i \in S}^{ } E_k(W_i) = \mu_k
    \]
    but the summand on LHS is precisely \(\rho_i\).
  \item
    \begin{align*}
      \rho_j &= \E_k(W_j) \\
             &= \sum_{m \geq 1}^{ } \prob_k(X_m = j, T_k \geq m) \text{ again we interchange the summations} \\
             &= \sum_{m \geq 1}^{ } \sum_{i \in S}^{ } \prob_k(X_m = j, X_{m - 1} = i, T_k \geq m) \\
             &= \sum_{m \geq 1}^{ } \sum_{i \in S}^{ } \prob_k(X_m = j| X_{m - 1} = i, T_k \geq m)\prob_k(X_{m - 1} = i, T_k \geq m) \\
             &= \sum_{m \geq 1}^{ } \sum_{i \in S}^{ } \prob_k(X_m = j| X_{m - 1} = i, T_k \geq m)\prob_k(X_{m - 1} = i) \text{ Markov property} \\
             &= \sum_{m \geq 1}^{ } \sum_{i \in S}^{ } p_{i,j} \prob_k(X_{m - 1} = i, T_k \geq m) \\
             &= \sum_{i \in S}^{ } p_{i,j} \sum_{m \geq 1}^{ } \prob_k(X_{m - 1} = i, T_k \geq m) \\
             &= \sum_{i \in S}^{ } p_{i,j} \sum_{r \geq 1}^{ } \prob_k(X_{r} = i, T_k \geq r + 1) \\
      \intertext{consider two cases: if \(i \neq k\), the term when \(r = 0\) is \(0\) and \(T_k \geq r + 1\) if and only if \(T_k \geq r\). If \(i = k\), the term when \(r = 0\) is \(1\) and all the other terms are zero. Thus}
             &= \sum_{i \in S}^{ }p_{i,j} \rho_i
    \end{align*}
  \item Pivot off the fact that \(\rho_k = 1\). Since \(\rho = \rho P\), we have \(\rho = \rho P^r\) for \(r \geq 1\). Thus
    \[
      \rho_i \geq \rho_k p_{k, i}(m), \rho_k \geq \rho_i p_{i, k}(n)
    \]
    By irreducibility, there exists \(m, n \geq 1\) with \(p_{k, i}(m), p_{i, k}(m) > 0\) so
    \[
      0 < p_{k, i}(m) \leq \rho_i \leq \frac{1}{p_{i, k}(n)} < \infty
    \]
  \end{enumerate}
\end{proof}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item Let \(k\) be positive recurrent, hence \(\mu_k < \infty\). Then \(\pi_i := \rho_i/\mu_ik\) is an invariant distribution.
  \item Let \(\pi\) be an invariant distribution. Claim \(\pi_i > 0\) for all \(i \in S\):
    \begin{proof}
      Since \(\pi = \pi p\), we have \(\pi = \pi p^n\) for \(n \geq 0\) and hence
      \[
        \pi_i = \sum_j \pi_j p_{j,i}(n) \geq \pi_k p_{k,i}(n)
      \]
      for \(k \in S\). Since \(\sum_i \pi_i = 1\) we may pick \(k \in S\) with \(\pi_k > 0\). By irreducibility there exists \(n \geq 0\) such tat \(p_{k, i}(n) > 0\). Hence \(\pi_i > 0\).

      Suppose every state is transient, since \(\pi = \pi p^n\),
      \[
        \pi_j = \sum_i \pi_i p_{i, j}(n)
      \]
      taking limit as \(n \to \infty\),
      \[
        \sum_i \pi_i 0 = 0
      \]
      absurd.
      \begin{proof}[Proof of the limiting process]
        \begin{align*}
          0 \leq \sum_i \pi_i p_{i, j}(n) &= \sum_{i \in F} \cdots + \sum_i \notin F \cdots \text{ where } F \subseteq S, |F| < \infty \\
          &\leq \sum_{i \in F} p_{i, j}(n) + \sum_{i \notin F} \pi_i \text{ by boundedness} \\
                                          &\to 0 + \sum_{i \notin F} \pi_i \text{ as } n \to \infty \\
          &\to 0 \text{ as } F \to S^-
        \end{align*}
      \end{proof}
      Thus every state is recurrent.
    \end{proof}
    \begin{align*}
      \pi_i \mu_i &= \sum_{n = 1}^{\infty} \prob_i(T_i \geq n) \prob(X_0 = i) \\
                  &= \sum_{n = 1}^{\infty} \prob(X_0 = i, T_i \geq n) 
                    %TBF
    \end{align*}
    Using stationarity,
    \begin{align*}
      \pi_i \mu_i &= \pi_i + \sum_{n = 2}^{\infty} (a_{n - 2} - a_{n - 1}) \text{ where } a_r = \prob(X_0 \neq i, \dots, X_r \neq i) \\
                  &= \pi_i + a_0 - \lim_{m \to \infty} a_m \\
                  &= \pi_i + (1 - \pi_i) - \underbrace{\prob(T_i = \infty)}_{= 0 \text{ by recurrence}} \\
                  &= 1
    \end{align*}
    Since \(\pi_i\mu_i = 1\),
    \[
      \mu_i = \frac{1}{\pi_i} < \infty
    \]
    since \(\pi_i > 0\). Thus \(i\) is positive recurrent.
  \end{enumerate}
\end{proof}

\section{Convergence to Equilibrium}

\begin{theorem}
  Consider an irreducible aperiodic, positive recurrent Markov chain. For \(i, j \in S\), \(p_{i, j}(n) \to \pi_j\) as \(n \to \infty\) where \(\pi\) is the unique invariant distribution.
\end{theorem}

Ergodic theorem

\begin{proof}
  ``coupling'' is the main idea.

  Let \(X = (X_n), Y = (Y_n)\) be independent Markov chains with the appropriate common invariant distribution. Let \(Z = (Z_n = (X_n, Y_n))_{n \geq 0}\). Then \(Z\) is a Markov chain with state space \(S \times S\) and transition matrix
  \[
    p_{ij, kl} = p_{i, k}p_{j, l}
  \]
  Fix \(s \in S\), let
  \[
    T = \inf \{n \geq 1: Z_n = (s, s)\}.
  \]
  Since \(X\) and \(Y\) have invariant distribution \(\pi\), \(Z\) has invariant distribution \(v_{ij} = \pi_i\pi_j\) since
  \[
    \sum_{i,j } v_{ij}p_{ij, kl} = \sum_i \pi_i p_{i, k} \sum_j \pi_j p_{j, l} = \pi_k \pi_l = v_{kl}
  \]
  Hence \(Z\) is positive recurrent and \(\prob(T < \infty) = 1\).

  We are still lacking one thing: \(Z\) is irreducible. This has something to do with aperiodicity.

  A digression about number theory: if \(D\) is a finite subset of non-negative integers with \(\gcd(D) = 1\), there exists \(N\) such that for \(n > N\) and expression
  \[
    n = \sum_{d \in D} \alpha_d d
  \]
  with \(\alpha_d \in \{0, 1, 2, \dots\}\).

  Since \(X\) is aperiodic, we deduce that \(p_{i,i}(n) > 0\) for all large \(n\). Thus
  \[
    p_{ij, ij}(n) = p_{i, i}(n)p_{j, j}(n) > 0
  \]
  for all large \(n\). Therefore \(Z\) is aperiodic.

  Similarly \(Z\) is irreducible (oops, check the book!)
\end{proof}

\begin{align*}
  p_{i, k}(n) &= \prob_i(X_n = k) \\
              &= \prob_{ij}(X_n = k) \\
              &= \sum_{t = 1}^{\infty} \prob_{ij}(X_n = k | T = t)\prob_{ij}(T = t) \\
              &= \sum_{t \leq n}^{ } \prob_{ij}(X_n = k | T = t) \prob_{ij}(T = k) + \sum_{t > n}^{ } \prob_{ij}(T = t) \\
              %&= \prob_S(X_{n - t} = k) = \prob_s(Y_{n - t} = k)
              &= \sum_{t \leq n}^{ } \prob_{ij}(Y_n = k, T = k) + \prob_{ij}(T > n) \\
              &\leq p_{j, k}(n) + \prob_{ij}(T > n)
\end{align*}
so
\[
  |p_{i, k}(n) - p_{j, k}(n)| \leq \prob_{ij}(T > n) \to 0
\]
as \(n \to \infty\). This says that if it converges they converge the the same value. Now we are just one line from the final result:
\[
  \pi_k - p_{ij}(n) = \sum_{i}^{ } \pi(p_{i,k}(n) - p_{j, k}(n)) \to 0
\]
by bounded convergence theorem.

This is an extremely elegant proof and it took a long time before this proof was found.

We are left with one final bit 

\begin{theorem}
  Let \(V_i(n) = \sum_{k = 1}^n \V 1(x_k = i)\) be the total number of visits to \(i\) up to time \(n\). If the chain is irreducible and positive recurrent then
  \[
    \frac{V_i(n)}{n} \Rightarrow \frac{1}{\mu_i}
  \]
  as \(n \to \infty\), where \(\Rightarrow\) means weak convergence, i.e.
  \[
    \prob \left( \frac{V_i(n)}{n} \leq a\frac{1}{\mu_i} \right) \to
    \begin{cases}
      0 & a < 1 \\
      1 & a > 1
    \end{cases}
  \]
\end{theorem}

We are not going prove this. Renewal theorem.

\begin{remark}
  Let \(u_i\) be a typical interval length between successive visits to \(i\). \(V_i(n) \geq x\) if and only if \(\sum_{m = 1}^x u_i(m) \leq n\) where the \(u_i(m)\) are iid copies of \(u_i\).
\end{remark}

\section{Time Reversal}

some ruminations bout physics: in real life time reversal is possible but extremely unlikely. The typical explanation is entropy.

Let \(X = (X_n: n = 0, 1, \cdots, N)\) be an irreducible and positive recurrent Markov chain with transition matrix \(P\) and invariant distribution \(\pi\). Let \(Y_n = X_{N - n}\), so \(Y = (Y_0, \cdots, Y_N) = (X_n, \cdots, X_0)\), the reverse of \(X\).

We have to make some assumptions for our reversed chain to make sense: assume that \(X_0\) has distribution \(\pi\).

\begin{theorem}
  \(Y\) is an irreducible Markov chain with transition matrix
  \[
    \hat p_{i,j} = \frac{\pi_j}{\pi_i} p_{j, i}
  \]
  and invariant distribution \(\pi\).
\end{theorem}

\begin{proof}
  First check that \(\hat P = (\hat p_{i,j})\) is a stochastic matrix: the entries are non-negative and
  \[
    \sum_j \hat p_{i,j} = \sum_j \frac{\pi_j}{\pi_i} p_{j, i} = \frac{1}{\pi_i} \sum_j \pi_j p_{j,i} = 1.
  \]

  Claim \(\pi = \pi \hat P\):
  \begin{align*}
    \sum_i \pi_i \hat p_{i,j} &= \sum_i \pi_j p_{j, i} = \pi_j
  \end{align*}

  Now to prove it is a Markov chain,
  \begin{align*}
    \prob(Y_0 = i_0, \dots, Y_n = i_n) &= \prob(X_{N -n} = i_n, \dots, X_N = i_0) \\
                                       &= \pi_{i_n} p_{i_n, i_{n - 1}} \cdots p_{i_1, i_0} \\
                                       &= \pi_{i_{n - 1}} \hat p_{i_{n + 1}, i_n} p_{i_{n - 1}, i_{n - 1}} \cdots, p_{i_1, i_n} \\
                                       &= \pi_{i_0} \hat p_{i_0, i_1} \cdots \hat p_{i_{n - 1}, i_n}
  \end{align*}
  Hence \(Y\) has the stated properties.
\end{proof}

We call mY the \emph{time-reversal} of \(X\) and we say \(X\) is \emph{reversible} if \(Y\) and \(X\) have the same transition probabilities. By (*) (the equation is the statement of the theorem), \(X\) is recursive if and only if
\[
  \pi_i p_{i,j} = \pi_j p_{j, i}
\]
for all \(i, j \in S\). This is the detailed balance equation.

More generally, we say a transition matrix \(P\) and a distribution \(\lambda\) are in \emph{detailed balance} if
\[
  \lambda_i p_{i, j} = \lambda_j p_{j, i}
\]
for all \(i, j \in S\). An irreducible chain \(X\) with invariant distribution \(\pi\) is called \emph{reversible in equilibrium} if its \(P\) is in detailed balance with \(\pi\).

Equation such as \(\pi = \pi P\) can be difficult and may depend on some special structure on \(P\). On the other hand the detailed balance equation is almost trivial.

\begin{proposition}
  If \(\pi\) is a distribution satisfying
  \[
    \pi_i p_{i, j} = \pi_j p_{j, i}
  \]
  for all \(i, j \in S\) and \(S\) is irreducible, then \(\pi\) is the only invariant distribution of the chain and the chain is reversible in equilibrium
\end{proposition}

\begin{proof}
  Let \(\pi\) be a distribution satisfying the hypothesis. Then
  \[
    \sum_i \pi_j p_{j, i} = \sum_j \pi_i p_{i, j} = \pi_i
  \]
  as \((\pi P)_i = \pi_i\). Therefore \(\pi = \pi P\).
\end{proof}

\begin{eg}[Birth-death with retaining barrier]
  Try the detailed balance equation
  \[
    \pi_{i - 1} p_{i - 1} = \pi_i q_i
  \]
  so
  \begin{equation}
    \pi_i = \frac{p_{i - 1}}{q_i} \frac{p_{i - 2}}{q_{i - 1}} \cdots \frac{p_0}{q_1} \pi_0 = \rho_i \pi_0
  \end{equation}

  \[
    \sum_i \pi_i = \pi_0 \sum_i \rho_i
  \]
  If \(S = \sum_i \rho_i\) satisfies \(S < \infty\) then \(\pi_i = \rho_i/S\) is an invariant distribution and if \(S = \infty\) there is no invariant distribution.
\end{eg}

\section{Random Walk on a Graph}

A finite graph consists of vertices and edges and is denoted \(G = (V, E)\). We discuss graphs are simple (in which there are no parallel edges and loops) and connected. If \((u, v) \in E\) then \(v\) is called an \emph{neighbour} of \(u\). The \emph{degree} of \(u\), \(d(u)\), is the number of its neighbours.

A random walk on \(G\) is a Markov chain with state space \(V\) and transition probability
\[
  p_{u, n} =
  \begin{cases}
    0 & \text{ if \(v\) is not a neighbour of \(u\)} \\
    \frac{1}{d(u)} & \text{if \(v\) is a neighbour of \(u\)}
  \end{cases}
\]
This is irreducible if and only if \(G\) is connected, which we assume henceforth.

As always, the natural question to ask is if there is an invariant distribution. Try to solve
\[
  \cdot p_{u, v} = \cdot p_{v, u}
\]
for \((u, v) \in E\). We try to find ``things'' to multiply for the above relation to hold. The obvious choice is \(\pi_u = d(u)\). But we have to normalise it since
\[
  \sum_u d(u) = 2|E|
\]
Then \(\pi_u = \frac{d(u)}{2|E|}\) satsifies the above detailed balance equation, and hence is the unique invariant distribution.

\begin{eg}[Erratic Knight]
  A knight performs independent legal knight moves about a \(8 \times 8\) chessboard. This is a Markov chain on the state space \(S\), the smallest square on hte board. (Exercise: show this is irreducible). The question is: what is its invariant distribution?

  The answer is simple:
  \[
    \pi_i = \frac{\text{No. of legal moves from square}}{336}
  \]
\end{eg}

\begin{ex}[Erratic Bishop]
  There are two types of bishops, depending on the colour of the intial checkerboard. Consequently there are two commutative classes.
\end{ex}

\iffalse
\appendix

\section{Resources}


Reading list: Probability, an introduction Grimmett, Welsh, 2nd edition, Chapter 12

%webpage: www.statslab.cam.ac.uk/~grg/
\fi
\end{document}