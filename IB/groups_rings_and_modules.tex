\documentclass[a4paper]{article}

\def\npart{IB}

\def\ntitle{Groups, Rings and Modules}
\def\nlecturer{O.\ Randal-Williams}

\def\nterm{Lent}
\def\nyear{2017}

\input{header}

\DeclareMathOperator{\Ann}{Ann}
\DeclareMathOperator{\fit}{Fit}

\makeindex

\begin{document}

\input{titlepage}

\tableofcontents

\section{Groups}

\subsection{Definitions}

\begin{definition}[Group]\index{group}
  A \emph{group} is a triple \((G, \cdot, e)\) of a set \(G\), a function \(- \cdot -: G \times G \to G\) and \(e \in G\) such that
  \begin{itemize}
  \item associativity: for all \(a, b, c \in G\), \((a \cdot b) \cdot c = a \cdot (b \cdot c)\),
  \item identity: for all \(a \in G\), \(a \cdot e = a = e \cdot a\),
  \item inverse: for all \(a \in G\), there exists \(\alpha^{-1} \in G\) such that \(a \cdot a^{-1} = e = a^{-1} \cdot a\).
  \end{itemize}
\end{definition}

\begin{definition}[Subgroup]\index{group!subgroup}
  If \((G, \cdot, e)\) is a group, \(H \subseteq G\) is a \emph{subgroup} if
  \begin{itemize}
  \item \(e \in H\),
  \item for all \(a, b \in H\), \(a \cdot b \in H\).
  \end{itemize}
  This makes \((H, \cdot, e)\) into a group. Write \(H \leq G\).
\end{definition}

\begin{lemma}
  If \(H \subseteq G\) is non-emphy and for all \(a, b \in H\), \(a \cdot b^{-1} \in H\) then \(H \leq G\).
\end{lemma}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Additive groups: \((\N, +, 0)\), \((\R, +, 0)\), \((\C, +, 0)\).
  \item Groups of symmetries: \(S_n\), \(D_{2n}\), \(\GL_n(\R)\). They have subgroups \(A_n \leq S_n\), \(C_n \leq D_{2n}\), \(\SL_n(\R) \leq \GL_n(\R)\).
  \item An group \(G\) is \emph{abelian}\index{group!abelian} is a group such that \(a \cdot b = b \cdot a\) for all \(a, b \in G\).
  \end{enumerate}
\end{eg}

If \(H \leq G\), \(g \in G\), we define the \emph{left \(H\)-coset} of \(G\) to be
\[
  gH = \{gh: h \in H\}.
\]
As we have seen in IA Groups, the \(H\)-cosets form a patition of \(G\) and are in bijection with each other via
\begin{align*}
  H &\leftrightarrow gH \\
  h &\mapsto gh \\
  g^{-1}h &\mapsfrom h
\end{align*}
We write \(G/H\) for the set of left cosets.

\begin{theorem}[Lagrange]\index{Lagrange's Theorem}
  \label{thm:lagrange}
  If \(G\) is a finite group and \(H \leq G \) then
  \[
    |G| = |H| \cdot |G/H|.
  \]
\end{theorem}

We call \(|G/H|\) the \emph{index}\index{group!index} of \(H\) in \(G\).

\begin{definition}[Order]\index{order}
  Given \(g \in G\), the \emph{order} of \(g\) is the smallest \(n\) such that \(g^n = e\). We write \(n = o(g) = |g|\). If no such \(n\) exists then \(g\) has infinite order.
\end{definition}

Recall that if \(g^m = e\) then \(|g| \divides m\).

\begin{lemma}
  If \(G\) is finite and \(g \in G\) then \(|g| \divides |G|\).
\end{lemma}

\begin{proof}
  The set
  \[
    \generation g = \{e, g, \dots, g^{|g| - 1}\}
  \]
  is a subgroup of \(G\). The result follows from \nameref{thm:lagrange}.
\end{proof}

\subsection{Normal subgroups, Quotients and Homomorphisms}

Recall that \(gH = g'H\) if and only if \(g^{-1}g' \in H\). In particular, if \(h \in H\) then \(ghH = gH\).

Given a subgroup \(H \leq G\), we want to define a group structure on its cosets. Argurably the most natural candidate for the group operation would be
\begin{align*}
  - \cdot -: G/H \times G &\to G/H \\
  (gH, g'H) &\mapsto gg'H
\end{align*}
But is this well-defined? Suppose \(g'H = g'hH\), then
\[
  (gH) \cdot (g'hH) = gg'hH = gg'H
\]
so it is well-defined in the second coordinate. Suppose \(gH = ghH\), then
\[
  (ghH) \cdot (g'H) = ghg' H \stackrel{?}{=} gg'H
\]
where the last step holds if and only if \((g')^{-1}hg' \in H\) for all \(h \in H\), \(g' \in G\). Thus we need this true to define a group structure on the cosets. This motivates us to define

\begin{definition}[Normal subgroup]\index{group!subgroup!normal}
  A subgroup \(H \leq G\) is \emph{normal} if for all \(h \in H\), \(g \in G\), \(g^{-1}hg \in H\). Write \(H \normal G\).
\end{definition}

\begin{definition}[Quotient group]\index{group!quotient}
  If \(H \normal G\), then \(G/H\) equipped with the product
  \begin{align*}
    G/H \times G/H &\to G/H \\
    (gH, g'H) &\mapsto gg'H
  \end{align*}
  and identity \(eH\) is a group. This is the \emph{quotient group} of \(G\) by \(H\).
\end{definition}

Now we have defined and seen quite a few groups. We are interested not in the internal structure of groups but how they relate to each other. This motivates to define morphisms between groups:

\begin{definition}[Homomorphism]\index{group!homomorphism}
  If \((G, \cdot, e_G)\) and \((H, *, e_H)\) are groups, a function \(\varphi: G \to H\) is a \emph{homomorphism} if for all \(g, h \in G\),
  \[
    \varphi(g \cdot g') = \varphi(g) * \varphi(g').
  \]
\end{definition}

This implies that \(\varphi(e_G) = e_H\) and \(\varphi(g^{-1}) = \varphi(g)^{-1}\). We define
\begin{align*}
  \ker \varphi &= \{g \in G: \varphi(g) = e_H\}, \\
  \im \varphi &= \{\varphi(g): g \in G\}.
\end{align*}

\begin{lemma}\leavevmode
  \begin{itemize}
  \item \(\ker \varphi \normal G\),
  \item \(\im \varphi \leq H\).
  \end{itemize}
\end{lemma}

\begin{proof}
  Easy.
\end{proof}

\begin{definition}[Isomorphism]\index{group!isomorphism}
  A homomorphism \(\varphi\) is an \emph{isomorphism} if it is a bijection. Say \(G\) and \(H\) are \emph{isomorphic} if there exists some isomorphism \(\varphi: G \to H\). Write \(G \cong H\).
\end{definition}

\begin{ex}
  If \(\varphi\) is an isomorphism then the inverse \(\varphi^{-1}: H \to G\) is also an isomorphism.
\end{ex}

\begin{theorem}[1st Isomorphism Theorem]\index{tbd}
  Let \(\varphi: G \to H\) be a homomorphism . Then \(\ker \varphi \leq G\), \(\im \varphi \leq G\) and
  \[
    G/\ker \varphi \cong \im \varphi.
  \]
\end{theorem}

\begin{proof}
  We have done the first part. For the second part, define
  \begin{align*}
    \theta: G/\ker \varphi &\to \im \varphi \\
    g\ker \varphi &\mapsto \varphi(g)
  \end{align*}
  \[
    \begin{tikzcd}
      G \ar[r, "\varphi"] \ar[d, two heads] & H \\
      G/\ker \varphi \ar[ur, dashed, "\phi"']
    \end{tikzcd}
  \]
  Check this is well-defined: if \(g\ker \varphi = g'\ker \varphi\) then \(g^{-1}g' \in \ker \varphi\) so \(e_H = \varphi(g^{-1}g') = \varphi(g)^{-1}\varphi(g')\), \(\varphi(g) = \varphi(g')\) and \(\theta(g\ker \varphi) = \theta(g'\ker \varphi)\).

  \(\theta\) is a homomorphism:
  \[
    \theta(g\ker \varphi \cdot g'\ker \varphi) = \theta(gg' \ker \varphi) = \varphi(gg') = \varphi(g)\varphi(g') = \theta(g\ker \varphi) \theta(g\ker \varphi).
  \]
  \(\theta\) is surjective and finally to show it is injective, suppose \(\theta(g\ker \varphi) = e_H\). Then \(g \in \ker \varphi\) so \(g\ker \varphi = e\ker \varphi\).
\end{proof}

\begin{eg}
  Consider
  \begin{align*}
    \varphi: \C &\to \C \setminus \{0\} \\
    z &\mapsto e^z
  \end{align*}
  \(e^{z + w} = e^z \cdot e^w\) so \(\varphi: (\C, +, 0) \to (\C \setminus \{0\}, \times, 1)\) is a homomorphism. \(\varphi\) is surjective (as \(\log\) is a left inverse).
  \[
    \ker \varphi = \{z \in \C: e^z = 1\} = \{2\pi ik: k \in \Z\} = 2\pi i\Z
  \]
  so by 1st Isomorphism Theorem
  \[
    \C/2\pi i\Z \cong \C \setminus \{0\}.
  \]
\end{eg}

\begin{theorem}[2nd Isomorphism Theorem]
  Let \(H \leq G\) and \(K \normal G\). Then
  \begin{align*}
    HK &\leq G \\
    H &\cap K \normal H \\
    HK/K &\cong H/(H \cap K)
  \end{align*}
\end{theorem}

\begin{proof}
  Let \(h, h' \in H\), \(k, k' \in K\). Then
  \[
    (h'k')(hk)^{-1} = h'k'k^{-1}h^{-1} = (h'h^{-1})(hk'k^{-1}h^{-1}) \in HK.
  \]

  Consider
  \begin{align*}
    \varphi: H &\to G/K \\
    h &\mapsto hK
  \end{align*}
  This is the composition \(H \stackrel{\iota}{\to} G \stackrel{\pi}{\to} G/K\) so a homomorphism. Since
  \begin{align*}
    \ker \varphi &= \{hK: hK = eK\} = H \cap K \normal H \\
    \im \varphi &= \{gK: gK = hK \text{ for some } h \in H\} = HK/K
  \end{align*}
  so by 1st Isomorphism Theorem
  \[
    H/(H \cap K) \cong HK/K.
  \]
\end{proof}

As a corollary we have

\begin{theorem}[Subgroup correspondence]
  Let \(K \normal G\). There is a bijection between
  \begin{align*}
    \{\text{subgroups of } G/K\} &\leftrightarrow \{\text{subgroups of \(G\) containing \(K\)}\} \\
    H &\mapsto \{g \in G: gK \in H\} \\
    L/K &\mapsfrom K \normal L \leq G
  \end{align*}
  Moreover, the same map gives a bijection between
  \[
    \{\text{normal subgroups of } G/K\} \leftrightarrow \{\text{normal subgroups of \(G\) containing \(K\)}\}.
  \]
\end{theorem}

\begin{theorem}[3rd Isomorphism Theorem]
  Let \(K \leq L \leq G\) be normal subgroups. Then
  \[
    \frac{G/K}{L/K} \cong G/L.
  \]
\end{theorem}

\begin{proof}
  Consider
  \begin{align*}
    \varphi: G/K &\to G/L \\
    gK \mapsto gL
  \end{align*}
  Check it is well-defined: if \(gK = g'K\), \(g^{-1}g' \in K \subseteq L\) so \(gL = g'L\). \(\varphi\) is clearly surjective and has kernel
  \[
    ker \varphi = \{gK \in G/K: gL = eL\} = L/K
  \]
  so by 1st Isomorphism Theorem
  \[
    \frac{G/K}{L/K} \cong G/L.
  \]
\end{proof}

\begin{definition}[Simple group]\index{simple group}
  A group \(G\) is \emph{simple} if its only normal subgroups are \(\{e\}\) and \(G\).
\end{definition}

\begin{lemma}
  An abelian group is simple if and only if it is isomorphic to \(C_p\) for some prime \(p\).
\end{lemma}

\begin{proof}
  In an abelian group every subgroup is normal. Let \(g \in G\) be non-trivial. Then
  \[
    \generation g = \{\dots, g^{-2}, g^{-1}, e, g, g^2, \dots\} \normal G.
  \]
  If \(G\) is simple, this must be the whole group so \(G\) is cyclic. If \(G\) is infinite, it is isomorphic to \(\Z\) which is not simple as \(2\Z \normal G\). Therefore \(G \cong C_n\) for some \(n\). If \(n = ab\), \(a, b \in \N\), \(a, b \neq 1\) then \(\generation{g^a} \normal G\). Absurd. Thus \(n\) is a prime.

  For the other directions, note that \(C_p\) is simple for prime \(p\) by \nameref{thm:lagrange}.
\end{proof}

\begin{theorem}
  Let \(G\) be a finite group. Then there is a chain of subgroups
  \[
    G = H_0 \geq H_1 \geq H_2 \geq \dots \geq H_s = \{e\}
  \]
  such that \(H_{n + 1} \normal H_n\) and \(H_n/H_{n + 1}\) is simple for all \(n\).
\end{theorem}

\begin{proof}
  Let \(H_1\) be a normal subgroup of \(H_0 = G\) of maximal order. If \(H_0/H_1\) is not simple, there would be a proper normal subgroup \(X \normal H_1/H_2\). This corresponds to a normal subgroup of \(H_0\), \(Y\), which strictly contains \(H_1\). Absurd. Thus \(H_0/H_1\) is simple.

  Choose \(H_2\) to be the maximal normal subgroup of \(H_1\) and continue. As \(H_{n + 1}\) is a proper subgroup of \(H_n\), \(|H_{n + 1}| < |H_n|\) so this process terminates after finitely many steps.
\end{proof}

\subsection{Actions \& Permutations}

The \emph{symmetric group}\index{symmetric group} \(S_n\) is the set of permutations of \(\{1, \dots, n\}\). Every permutation is a product of transpositions. A permutation is \emph{even} if it is a product of evenly-many transpositions and \emph{odd} otherwise.

The \emph{sign}\index{sign} of a permutation is a homomorphism
\begin{align*}
  \sgn: S_n &\to \{\pm 1\} \\
  \sigma &\mapsto
           \begin{cases}
             1 & \sigma \text{ is even} \\
             -1 & \sigma \text{ is odd}
           \end{cases}
\end{align*}
The kernel of \(\sgn\) is the \emph{alternating group} \(A_n \normal S_n\) of index \(2\) for \(n \geq 2\).

For any set \(X\), we let \(\sym(X)\) denote the set of all permutations of \(X\), with composition as the group operation.

Here is a definition that is included in the syllabus but seems to be never used anywhere:

\begin{definition}
  A group \(G\) is a \emph{permutation group of degree \(n\)} if
  \[
    G \leq \sym(X)
  \]
  with \(|X| = n\).
\end{definition}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item \(S_n\) is a permutation group of order \(n\), so is \(A_n\).
  \item \(D_{2n}\) acts on the \(n\) vertices of a regular \(n\)-gon, so
    \[
      D_{2n} \leq S(\{n \text{ vertices}\}).
    \]
  \end{enumerate}
\end{eg}

\begin{definition}[Group action]\index{group action}
  An \emph{action} of a group \((G, \cdot, e)\) on a set \(X\) is a function \(- * - : G \times X \to X\) such that
  \begin{enumerate}
  \item For all \(g, h \in G\), \(x \in X\),
    \[
      g * (h * x) = (gh) * x.
    \]
  \item For all \(x \in X\),
    \[
      e * x = x.
    \]
  \end{enumerate}
\end{definition}

\begin{lemma}
  Giving an action of \(G\) on \(X\) is the same as giving a homomorphism \(\varphi: G \to \sym(X)\).
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(\Rightarrow\): Let \(- * -\) be an action. For all \(g \in G\), let
    \begin{align*}
      \varphi: X &\to X \\
      x &\mapsto g * x
    \end{align*}
    This satisfies
    \[
      \varphi(gh)(x) = (gh) * x = g * (h * x) = \varphi(g)(\varphi(h)(x)) = (\varphi(g) \compose \varphi(h))(x)
    \]
    so \(\varphi(gh) = \varphi(g) \compose \varphi(h)\).

    In addition \(\varphi(e)(x) = e * x = x = \id_X(x)\) so \(\varphi(e) = \id X\). Now we note that
    \[
      \id_X = \varphi(e) = \varphi(gg^{-1}) = \varphi(g) \compose \varphi(g^{-1})
    \]
    so \(\varphi(g^{-1})\) is inverse to \(\varphi(g)\). In particular \(\varphi(g)\) is a bijection.
  \item \(\Leftarrow\): Let \(\varphi: G \to \sym(X)\) be a homomorphism. Define
    \begin{align*}
      - * -: G \times X &\to X \\
      (g, x) &\mapsto \varphi(g)(x)
    \end{align*}
    Verify that
    \begin{align*}
      g * (h * x) &= \varphi(g)(\varphi(h)(x)) = (\varphi(g) \compose \varphi(h))(x) = \varphi(gh)(x) = (gh) * x \\
      e * x &= \varphi(e)(x) = \id_X(x) = x
    \end{align*}
  \end{itemize}
\end{proof}

Given a homomorphism \(\varphi: G \to \sym(X)\) induced by an action, define \(G^X = \im \varphi, G_X = \ker \varphi\). Then by 1st Isomorphism Theorem \(G_X \normal G, G/G_X \cong G^X\).

If \(G_X = \{e\}\), i.e.\ \(\varphi\) is injective then we say \(\varphi\) is a \emph{permutation representation of \(G\)}. It follows that \(G \cong G^X \leq \sym(X)\).

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Let \(G\) be the symmetries of a cube. Then \(G\) acts on the set \(X\) of diagonals. \(|X| = 4\) and \(\varphi: G \to \sym(X)\) is surjective so \(G^X = \sym(X) \cong S_4\). \(G_X = \{\id, \text{antipodal map}\}\) so by \nameref{thm:lagrange}
    \[
      |G| = |G_X| \cdot |G^X| = 48.
    \]
  \item For any group \(G\), left multiplication is a homomorphism:
    \begin{align*}
      \varphi: G &\to \sym G \\
      g &\mapsto g \cdot -
    \end{align*}
    \(G_X = \{g \in G: gh = h \text{ for all } G\} = \{e\}\) so \(\varphi\) is a permutation representation. This is

    \begin{theorem}[Cayley's]
      Every group is isomorphic to a subgroup of a symmetric group.
    \end{theorem}
  \item If \(G\) is a group and \(H \leq G\), we have
    \begin{align*}
      \varphi: G &\to \sym(G/H) \\
      g &\mapsto g \cdot -
    \end{align*}
    \(G_X = \{g \in G: gaH = aH \text{ for all } aH\} = \bigcap_{a \in G} aHa^{-1}\). This is the largest subgroup of \(H\) which is normal in \(G\).
  \end{enumerate}
\end{eg}

\begin{theorem}
  Let \(G\) be a finite group and \(H \leq G\) with index \(n\). Then there is a \(K \normal G, K \leq H\) such that \(G/K\) is isomorphic to a subgroup of \(S_n\). In particular
  \[
    |G/K| \divides n!, n \divides |G/K|.
  \]
\end{theorem}

\begin{proof}
  Let \(K = G_X\) for the action of \(G\) on \(X = G/H\). Then
  \[
    G/G_X \cong G^X \leq \sym(X) \cong S_n.
  \]
\end{proof}

\begin{theorem}
  Let \(G\) be a non-abelian simple group and \(H \leq G\) is a subgroup of index \(n > 1\). Then \(G\) is isomorphic to a subgroup of \(A_n\) for some \(n \geq 5\).
\end{theorem}

\begin{proof}
  
\end{proof}
\blindtext

\section{Rings}

\blindtext

\section{Modules}

\subsection{Definitions}

\begin{definition}[Module]\index{module}
  Let \(R\) be a commutative ring. A quadruple \((M, +, , 0_M, \cdot)\) is an \emph{\(R\)-module} if \((M, +, 0_M)\) is an abelian group and the operation \(- \cdot -: R \times M \to M\) satisfies
  \begin{itemize}
  \item \((r_1 + r_2) \cdot m = r_1 \cdot m + r_2 \cdot m\),
  \item \(r \cdot (m_1 + m_2) = r \cdot m_1 + r \cdot m_2\),
  \item \(r_2 \cdot (r_1 \cdot m) = (r_2r_2) \cdot m\),
  \item \(1_R \cdot m = m\).
  \end{itemize}
\end{definition}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item If \(R = \F\) is a field then an \(\R\)-module is precisely an \(\F\)-vector space.
  \item For any ring \(R\), \(R^n = \underbrace{R \times \dots \times R}_{n \text{ times}}\) is an \(\R\)-module via
    \[
      r \cdot (r_1, \dots, r_n) = (rr_1, \dots, rr_n).
    \]
    In particular for \(n = 1\), \(R\) is an \(R\)-module.
  \item If \(I \ideal R\) then \(I\) is an \(R\)-module via
    \[
      r \cdot a = ra \in I.
    \]
    Also \(R/I\) is an \(\R\)-module via
    \[
      r \cdot (r_1 + I) = rr_1 + I \in R/I.
    \]
  \item For \(R = \Z\), an \(\R\)-module is precisely an abelian group. This is because the axiom for \(\cdot\) says that
    \begin{align*}
      - \cdot -: \Z \times M &\to M \\
      (n, m) &\mapsto
               \begin{cases}
                 \underbrace{m + \dots + m}_{n \text{ times}} & n \geq 0 \\
                 -(\underbrace{m + \dots + m}_{n \text{ times}}) & n < 0 \\
               \end{cases}
    \end{align*}
    so \(\cdot\) is uniquely determined by \(M\).
  \item Let \(\F\) be a field and \(V\) be a vector space over \(\F\). Let \(\alpha: V \to V\) be a linear map. Then we can make \(V\) into an \(\F[X]\)-module via
    \begin{align*}
      \F[X] \times V &\to V \\
      (f, v) &\mapsto f(\alpha)(v)
    \end{align*}
    Different \(\alpha\)'s make \(V\) into different \(\F[x]\)-modules.
  \item Restriction of scalars: if \(\varphi: R \to S\) is a ring homomorphism and \(M\) is an \(S\)-module, then \(M\) becomes an \(R\)-modules via
    \[
      r \cdot_R m = \varphi(r) \cdot_s m.
    \]
  \end{enumerate}
\end{eg}

\begin{definition}[Submodule]\index{module!submodule}
  If \(M\) is an \(\R\)-module, \(N \subseteq M\) is a \emph{submodule} if \(N\) is a subgroup of \((M, +, 0_M)\) and for any \(n \in N, r \in R\), \(r \cdot n \in N\). Write \(N \leq M\).
\end{definition}

\begin{eg}
  A subset of \(R\) is a submodule if and only if it is an ideal.
\end{eg}

\begin{definition}[Quotient module]\index{module!quotient}
  If \(N \leq M\) is a submodule, the \emph{quotient module} \(M/N\) is the set of \(N\)-cosets in \((M, +, 0_M)\), i.e. the quotient abelian group with
  \[
    r \cdot (m + N) = r \cdot m + N.
  \]
\end{definition}

\begin{definition}[Homomorphism]\index{module!homomorphism}
  A function \(f: M \to N\) is an \emph{\(\R\)-module homomorphism} if it is a homomorphism of abelian groups and \(f(r \cdot m) = r \cdot f(m)\).
\end{definition}

\begin{eg}
  If \(R = \F\) is a field and \(V\) and \(W\) are \(\F\)-modules (i.e. \(\F\)-vector spaces), then a map is an \(\F\)-module homomorphism if and only if it is an \(\F\)-linear map.
\end{eg}

\begin{theorem}[First Isomorphism Theorem]\index{tbd}
  If \(f: M \to N\) is an \(R\)-module homomorphism then
  \begin{align*}
    \ker f &= \{m \in M: f(m) = 0\} \leq M \\
    \im f &= \{n \in N: n = f(m)\} \leq N
  \end{align*}
  and
  \[
    M/\ker f \cong \im f.
  \]
\end{theorem}

\begin{theorem}[Second Isomorphism Theorem]\index{tbd}
  Let \(A, B \leq M\) be submodules. Then
  \begin{align*}
    A + B &= \{m \in M: m = a + b, a \in A, b \in B\} \leq M \\
    A \cap B &\leq M
  \end{align*}
  and
  \[
    (A + B)/A \cong B/(A \cap B).
  \]
\end{theorem}

\begin{theorem}[Third Isomorphism Theorem]\index{tbd}
  Let \(N \leq L \leq M\) be a chain of submodules. Then
  \[
    \frac{M/N}{L/N} \cong M/L.
  \]
\end{theorem}

\begin{definition}[Annihilator]\index{annihilator}
  If \(M\) is an \(R\)-module and \(m \in M\), the \emph{annihilator} of \(m\) is
  \[
    \Ann(m) = \{r \in R: r \cdot m = 0_M\} \ideal R.
  \]

  The \emph{annihilator} of \(M\) is
  \[
    \Ann(M) = \bigcap_{m \in M} \Ann(m) \ideal R.
  \]
\end{definition}

\begin{definition}[Generated submodule]\index{module!submodule!generated}
  If \(M\) is an \(R\)-module and \(m \in M\), the \emph{submodule generated by \(m\)} is
  \[
    Rm = \{r \cdot m \in M: r \in R\}.
  \]
\end{definition}

\begin{note}
  Intuitively, the annihilator of an element is the stabiliser of a ring action and that of a module is the kernel. We also have
  \[
    Rm \cong R/\Ann(m).
  \]
\end{note}

\begin{definition}[Finitely generated]\index{finitely generated}
  \(M\) is \emph{finitely generated} if there are \(m_1, \dots, m_n \in M\) such that
  \[
    M = Rm_1 + \dots Rm_n = \{r_1m_1 + \dots + r_nm_r: r_i \in R\}.
  \]
\end{definition}

\begin{lemma}
  An \(R\)-module \(M\) is finitely generated if and only if there is a surjetion \(\varphi: R^n \surj M\) for some \(n\).
\end{lemma}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(\Rightarrow\): Suppose \(M = Rm_1 + \dots + Rm_n\). Define
    \begin{align*}
      \varphi: R^n &\to M \\
      (r_1, \dots, r_n) &\mapsto r_1m_1 + \dots + r_nm_m
    \end{align*}
    This is an \(R\)-module homomorphism and is surjective.
  \item \(\Leftarrow\): Let \(m_i = \varphi((0, \dots, 0, 1, 0, \dots, 0))\) with \(1\) in the \(i\)th position. Then
    \begin{align*}
      \varphi((r_1, \dots, r_n)) &= \varphi((r_1, 0, \dots, 0) + \dots + (0, \dots, 0, r_n)) \\
                                 &= \varphi((r_1, 0, \dots, 0)) + \dots + \varphi((0, \dots, 0, r_n)) \\
                                 &= r_1 \varphi((1, 0, \dots, 0)) + \dots + r_n \varphi((0, \dots, 0, 1)) \\
      &= r_1m_1 + \dots + r_nm_n
    \end{align*}
    As \(\varphi\) is surjective, \(M = Rm_1 + Rm_n\).
  \end{itemize}
\end{proof}

\begin{corollary}
  Let \(M\) be an \(R\)-module and \(N \leq M\). If \(M\) is finitely generated the so is \(M/N\).
\end{corollary}

\begin{proof}
  \[
    R^n \overset{f}{\surj} M \overset{\pi}{\surj} M/N.
  \]
\end{proof}

\begin{note}
  A module of a finitely generated \(R\)-module need \emph{not} to be finitely generated. For example,
  \[
    (X_1, X_2, \dots) \ideal \Z[X_1, X_2, \dots] = R
  \]
  is an \(R\)-module but note finitely generated, as otherwise it would be a finitely generated ideal.
\end{note}

\begin{eg}
  For \(\alpha \in \C\), \(\alpha\) is an algebraic integer if and only if \(\Z[\alpha]\) is a finitely generated \(\Z\)-module.
\end{eg}

\subsection{Direct Sums and Free Modules}

\begin{definition}[Direct sum]\index{direct sum}
  If \(M_1, \dots, M_k\) are \(R\)-modules, the \emph{direct sum} \(M_1 \oplus \dots \oplus M_k\) is the set \(M_1 \times \dots \times M_k\) with addition
  \[
    (m_1, \dots, m_k) + (m_1', \dots, m_k') = (m_1 + m_1', \dots, m_k + m_k')
  \]
  and \(R\)-module structure
  \[
    r \cdot (m_1, \dots, m_k) = (rm_1, \dots, rm_k).
  \]
\end{definition}

\begin{eg}
  \[
    R^n = \underbrace{R \oplus \dots \oplus R}_{n \text{ times}}.
  \]
\end{eg}

\begin{definition}[Independence]\index{independenc}
  Let \(m_1, \dots m_k \in M\). They are \emph{independent} if
  \[
    \sum_i r_i \cdot m_i = 0
  \]
  implies that \(r_i = 0\) for all \(1 \leq i \leq k\).
\end{definition}

\begin{definition}[Free generation]\index{tbd}
  A subset \(S \subseteq M\) \emph{generates \(M\) freely} if
    \begin{enumerate}
    \item \(S\) generates \(M\).
    \item Any function \(\psi: S \to N\) to an \(R\)-module \(N\) extends to an \(R\)-module homomorphism \(\theta: M \to N\).
    \end{enumerate}
    \[
      \begin{tikzcd}
        S \ar[r, hook] \ar[dr, "\psi"'] & R^S \ar[d, dashed, "\theta"] \\
        & N
      \end{tikzcd}
    \]
\end{definition}

\begin{note}
  We can show this extension is unique: given \(\theta_1, \theta_2: M \to N\) two extensions of \(\psi\), \(\theta_1 - \theta_2:M \to N\) is an \(R\)-module homomorphism so \(\ker(\theta_1 - \theta_2) \leq M\). But \(\theta_1, \theta_2\) both extend \(\psi\) so \(S \subseteq \ker(\theta_1 - \theta_2)\). As \(S\) generates \(M\), \(M \leq \ker(\theta_1 - \theta_2)\) so \(\theta_1 = \theta_2\).
\end{note}

An \(R\)-module which is freely generated by \(S \subseteq M\) is said to be \emph{free} and \(S\) is called a \emph{basis}

\begin{proposition}
  For a finite subset \(S = \{m_1, \dots, m_k\} \subseteq M\), TFAE:
  \begin{enumerate}
  \item \(M\) is freely generated by \(S\).
  \item \(M\) is generated by \(S\) and \(S\) is independent.
  \item Every \(m \in M\) can be written as \(r_1m_1 + \dots + r_km_k\) for some unique \(r_i \in R\).
  \end{enumerate}
\end{proposition}

\begin{proof}\leavevmode
  \begin{itemize}
  \item \(1 \Rightarrow 2\): Let \(S\) generate \(M\) freely. If \(S\) is not independent, then there is a non-trivial relation
    \[
      \sum_{i = 1}^k r_im_i = 0
    \]
    with \(r_j \neq 0\). Let
    \begin{align*}
      \psi: S &\to R \\
      m_i &\mapsto
            \begin{cases}
              0_R & i \neq j \\
              1_R & i = j
            \end{cases}
    \end{align*}
    This extends to an \(R\)-module homomorphism \(\theta: M \to R\). Then
    \[
      0 = \theta(0) = \theta \left(\sum r_im_i \right) = \sum r_i\theta(m_i) = r_j.
    \]
    Absurd. Thus \(S\) is independent.
  \item The other steps follow similarly from those in IB Linear Algebra.
  \end{itemize}
\end{proof}

\begin{eg}
  Unlike vector spaces, a minimal generating set need not to be independent. For example \(\{2, 3\} \subseteq \Z\) generates \(\Z\) but is not linear independent as \((-3) \cdot 2 + (2) \cdot 3 = 0\).
\end{eg}

However, like vector spaces, in case a module is freely generated, it is isomorphic to direct sums of copies of the ring:

\begin{lemma}
  If \(S = \{m_1, \dots, m_k\} \subseteq M\) freely generates \(M\) then
  \[
    M \cong R^k
  \]
  as an \(R\)-module.
\end{lemma}

\begin{proof}
  This is entirely analogous to vector spaces. Let
  \begin{align*}
    f: R^k &\to M \\
    (r_1, \dots, r_k) &\mapsto \sum_i r_im_i
  \end{align*}
  It is surjective as \(S\) generates \(M\) and injective as \(m_i\)'s are independent.
\end{proof}

If an \(R\)-module is generated by \(m_1, \dots, m_k\), we have seen before that there is a surjection \(f: R^k \surj M\). We define

\begin{definition}[Relation module]\index{relation module}
  The \emph{relation module} for the generators is
  \[
    \ker f \leq \R^k.
  \]
\end{definition}

As \(M \cong R^k/\ker f\), knowing \(M\) is equivalent to knowing the relation module.

\begin{definition}[Finitely presented]\index{finitely presented}
  \(M\) is \emph{finitely presented} if there is a finitely generating set \(m_1, \dots, m_k\) for which the associated relation module is finitely generated.
\end{definition}

Let \(\{n_1, \dots, n_r\} \subseteq \ker f \leq R^k\) be a set of generators. Then
\[
  n_i =
  \begin{pmatrix}
    r_{1i} \\
    r_{ri} \\
    \vdots \\
    r_{ki}
  \end{pmatrix}
\]
and \(M\) is generated by \(m_1, \dots, m_k\) subject to relations
\[
  \sum_{j = 1}^k r_{ij} m_j = 0
\]
for \(1 \leq i\leq r\).

\begin{proposition}[Invariance of Dimension]
  If \(R^n \cong R^m\) then \(n = m\).
\end{proposition}

\begin{note}
  This does not hold in general for modules over non-commutative rings.
\end{note}

\begin{proof}
  As a general strategy, let \(I \ideal R\). Then
  \[
    IM = \left\{\sum a_im_i: a_i \in I, m_i \in M \right\} \leq M
  \]
  is a submodule as
  \[
    r \cdot \sum a_im_i = \sum (ra_i)m_i \in IM.
  \]
  Thus we have a quotient \(R\)-module \(M/IM\). We can make this into an \((R/I)\)-module via
  \[
    (r + I) \cdot (m + IM) = rm + IM.
  \]

  Let \(I \ideal R\) be a maximal proper ideal (this requires Zorn's Lemma). Then \(R/I\) is a field and therefore \(R^n \cong R^m\) implies
  \begin{align*}
    R^n/IR^n &\cong R^m/IR^m \\
    (R/I)^n &\cong (R/I)^m
  \end{align*}
  This is a vector space isomorphism so \(n = m\).
\end{proof}

We have classified all finite abelian groups (well, at least we declared so), i.e.\ \(\Z\)-modules. What if we want to classify all \(R\)-modules? That is going to be the final goal we will build towards.

Recall that \(M\) is finitely generated by \(m_1, \dots, m_k\) if and only if there is a surjection \(f: R^k \surj M\). \(M\) is finitely presentely if and only \(\ker f\) is finitely generated, say \(n_1, \dots, n_\ell\). Let
\[
  n_i =
  \begin{pmatrix}
    r_{1i} \\
    r_{ri} \\
    \vdots \\
    r_{ki}
  \end{pmatrix}
\]
then such an \(R\)-module \(M\) is determined by the matrix
\[
  \begin{pmatrix}
    r_{11} & r_{12} & \cdots & r_{1\ell} \\
    r_{r1} & & & \\
    \vdots & & \ddots & \\
    r_{k1} & & & r_{k\ell}
  \end{pmatrix}
  \in \matrixring_{k, \ell}(R).
\]

\subsection{Matrices over Euclidean Domains}

For this section assume \(R\) to be a Euclidean domain and let \(\varphi: R \setminus \{0\} \to \Z_{\geq 0}\) be the Euclidean function. For \(a, b \in R\), we have shown that \(\gcd(a, b)\) exists and is unique up to associates. In addition, the Euclidean algorithm shows that \(\gcd(a, b) = ax + by\) for some \(x, y \in R\).

What follows would be very similar to what we have learned in IB Linear Algebra --- in fact identical except a single modification:

\begin{definition}[Elementary row operation]\index{tbd}
  \emph{Elementary row operation} on an \(m \times n\) matrix \(m\) with entries in \(R\) are
  \begin{enumerate}
  \item Add \(c \in R\) times the \(i\)th row to the \(j\)th row where \(i \neq j\). This can be realised by left multiplication by \(mI + C\) where \(C\) is \(c\) in \((i,j)\)th position and \(0\) elsewhere.
  \item Swapping the \(i\)th and \(j\)th row where \(i \neq j\). Realised by left multiplication by
  \[
    \begin{pmatrix}
      1 & 0 & \cdots & & & 0 \\
      \vdots & \ddots & & & & \vdots \\
      & & 0 & & 1 & 0 \\
      0 & \cdots & 0 & \ddots & 0 & 0 \\
      & & 1 & 0 & 0 & & \\
      \vdots & &&  \ddots & \\
      0 & & \cdots & & \cdots & 0
    \end{pmatrix}
  \]
\item Multiply the \(i\)th row by a \emph{unit} \(c \in R\). Realised by left multiplication by
  \[
    \begin{pmatrix}
      1 & 0 & \cdots & & 0 \\
       & \ddots & & & \\
      \vdots & & c & & \vdots \\
       & & & \ddots & \\
      0 & \cdots & & 0 & 1
    \end{pmatrix}
  \]
  \end{enumerate}
\end{definition}

\begin{definition}[Elementary column operation]
  Defined analogously by replacing ``row'' with ``column''.
\end{definition}

Similary to IB Linear Algebra, we define an equivalence relation

\begin{definition}[Equivalence]\index{equivalence}
  \(A, B \in \matrixring_{m, n}(R)\) are \emph{equivalent} if there is a sequence of elementary row and column operations taking \(A\) to \(B\).
\end{definition}

If \(A\) and \(B\) are equivalent then there are invertible square matrices \(P\) and \(Q\) such that
\[
  B = QAP^{-1}.
\]

\begin{theorem}[Smith Normal Form]\index{Simith Normal Form}
  An \(n \times m\) matrix over a Euclidean domain \(R\) is equivalent to
  \[
    \begin{pmatrix}
      d_1 \\
      & d_2 \\
      & & \ddots \\
      & & & d_r \\
      & & & & 0 \\
      & & & & & \ddots \\
      & & & & & & 0
    \end{pmatrix}
  \]
  where the \(d_i\)'s are non-zero and
  \[
    d_1 \divides d_2 \divides \cdots \divides d_r.
  \]
\end{theorem}

\begin{proof}
  This proof is going to be algorithmic. If \(A = 0\) we are done. Otherwise there is a \(A_{ij} \neq 0\). By swapping \(1\)st and \(i\)th row, and \(1\)st and \(i\)th column we may suppose \(A_{11} \neq 0\). We want to reduct \(\varphi(A_{11})\) as much as possible. Split into three cases:
  \begin{itemize}
  \item Case 1: if there is a \(A_{1j}\) not divisible by \(A_{11}\) then have
    \[
      A_{1j} = q A_{11} + r
    \]
    with \(\varphi(r) < \varphi(A_{11})\). Add \(-q\) times the \(1\)st column to the \(j\)th. This makes the \((1,j)\)th entry \(r\). Swap \(i\)th and \(j\)th column to get \(A_{11} = r\). Thus we have \emph{stictly} decreased the \(\varphi\) value of the \((1, 1)\) entry.
  \item Case 2: if \(A_{11}\) does not divide some \(A_{i1}\), do the analogous to entries in the first column to strictly reduce \(\varphi(A_{11})\).

    As \(\varphi(A_{11})\) can only strictly decrease finitely many times, after some applications of Case 1 and 2 we can assumes \(A_{11}\) divides all the entries in the \(1\)st row and \(1\)st column. If \(A_{1j} = q A_{11}\) then we can add \(-q\) times the \(1\)st column to the \(j\)th row to make the \((i, j)\)th entry \(0\). Thus we obtain
    \[
      A =
      \begin{pmatrix}
        d & 0 \\
        0 & C
      \end{pmatrix}
    \]
  \item Case 3: if there is an entry \(c_{ij}\) of \(C\) not divisible by \(d\), write
    \[
      c_{ij} = qd + r
    \]
    where \(\varphi(r) < \varphi(d)\). Conduct the following series of elementary operations
    \begin{align*}
      &
        \begin{pmatrix}
          d & 0 & \cdots & 0 & \cdots & 0 \\
          0 \\
          \vdots \\
          0 & & & c_{ij} \\
          \vdots \\
          0
        \end{pmatrix}
      \stackrel{\text{EC } 1}{\to}
      \begin{pmatrix}
        d & 0 & \cdots & d & \cdots & 0 \\
        0 \\
        \vdots \\
        0 & & & c_{ij} \\
        \vdots \\
        0
      \end{pmatrix}
      \\
      \stackrel{\text{ER } 1}{\to}&
      \begin{pmatrix}
        d & 0 & \cdots & d & \cdots & 0 \\
        0 \\
        \vdots \\
        -qd & & & r \\
        \vdots \\
        0
      \end{pmatrix}
      \stackrel{\text{ER } 2, \text{EC } 2}{\to}
      \begin{pmatrix}
        r & * & \cdots & * \\
        * \\
        \vdots & & * \\
        *
      \end{pmatrix}
    \end{align*}
    Repeat Case 1 and 2, we finally get
    \[
      \begin{pmatrix}
        d' \\
        & C'
      \end{pmatrix}
    \]
    where \(\varphi(d') < \varphi(d)\).
  \end{itemize}

  Eventually we can suppose that \(d'\) divides every entry of \(C'\). By induction \(C'\) is equivalent to
  \[
    \begin{pmatrix}
      d_2 \\
      & d_3 \\
      & & \ddots \\
      & & & d_r \\
      & & & & 0 \\
      & & & & & \ddots \\
      & & & & & & 0
    \end{pmatrix}
  \]
  with
  \[
    d_2 \divides d_3 \divides \cdots \divides d_r
  \]
  and we must have \(d' \divides d_i\) for \(i > 1\).
\end{proof}

\begin{remark}
  The \(d_i\)'s in Smith Normal Form are unique up to associates.
\end{remark}

Certainly Smith Normal Form is nice form and the algorithm guarantees that it exists and is unique. However, the computation is too cumbersome to be useful. However, if we could prove it is invariant under matrix conjugation, we may apply some clever tricks to extract the \(d_i\)'s in Smith Normal Form without explicitly computing them.

\begin{definition}[Minor]\index{minor}
  A \(k \times k\) \emph{minor} of a matrix \(A\) is the determinant of a matrix formed by forgetting all but \(k\) rows and \(k\) columns of \(A\).
\end{definition}

\begin{definition}[Fitting ideal]\index{Fitting ideal}
  The \(k\)th \emph{Fitting ideal} of \(A\) \(\fit_k(A) \ideal R\) is the ideal generated by all \(k \times k\) minors of \(A\).
\end{definition}

Given a matrix \(A\) in Smith Normal Form as above with \(d_1 \divides \cdots \divides d_r\), the only \(k \times k\) submatrices which do not have a whole row or column \(0\) are those which keep both \(i_1\)th row and \(i_1\)th column, both \(i_2\)th row and \(i_2\)th column, etc. Therefore
\begin{align*}
  \fit_k(A) &= \left(\det
    \begin{pmatrix}
      d_{i_1} \\
      & d_{i_2} \\
      & & \ddots \\
      & & & d_{i_k}
    \end{pmatrix}
  \right) \\
            &= (d_{i_1} \cdots d_{i_k}: \text{ sequences } i_1, \dots, i_k) \\
            &= (d_1d_2 \cdots d_k)
\end{align*}
as \(d_m \divides d_{i_m}\) for all \(m\).

Therefore from the above computation \(\fit_k(A)\) and \(\fit_{k - 1}(A)\) determine \(d_k\) up to associates.

\begin{lemma}
  If \(A\) and \(B\) are equivalent matrices then \(\fit_k(A) = \fit_k(B)\) for all \(k\).
\end{lemma}

\begin{proof}
  It amounts to show that elementary operations does not change \(\fit_k(A) \ideal R\). We do the first type of row operation. Fix a \(k \times k\) submatrix \(C\) in \(A\). Recall the this row operation adds \(\lambda\) times the \(i\)th row to the \(j\)th row. Depending on \(i\) and \(j\), split into three cases:
  \begin{itemize}
  \item Case 1: if the \(j\)th row is not in \(C\) then \(C\) is unchanged, so is its determinant.
  \item Case 2: if the \(i\)th and \(j\)th rows are both in \(C\), the operation changes \(C\) by a row operation so its determinant is unchanged.
  \item Cases 3: if the \(j\)th row is in \(C\) but the \(i\)th is not, suppose wlog the \(i\)th row of \(A\) corresponding to columns of \(C\) has entries \((f_1, f_2, \dots, f_k)\). After the row operation, \(C\) is changed to \(C'\) whose \(j\)th row is
    \[
      (c_{j, 1} + \lambda f_1, c_{j, 2} + \lambda f_2, \dots, c_{j, k} + \lambda f_k).
    \]
    By expansion along the \(j\)th row,
    \[
      \det C' = \det C \pm \lambda \det D
    \]
    where \(D\) is the matrix obtained by replacing the \(j\)th row of \(C\) with \((f_1, \dots, f_k)\), which is a \(k \times k\) submatrix of \(A\) up to reordering (which is accounted for by the \(\pm\) sign), by multilinearity of \(\det\). So \(\det C' \in \fit_k(A)\) as it is a linear combination of minors. Therefore \(\fit_k(A') \subseteq \fit_k(A)\) where \(A'\) is obtained from \(A\) by this operation. As row operations are inertible, we must have equality.
  \end{itemize}

  The other two types of row operations are similar but easier. Column operations follow analogously.
\end{proof}

\begin{eg}
  Let
  \[
    A =
    \begin{pmatrix}
      2 & -1 \\
      1 & 2
    \end{pmatrix}
    \in \matrixring_{2, 2}(\Z).
  \]
  Algorithmically, we can carry out the following sequence of operations to obtain Smith Normal Form:
  \[
    \begin{pmatrix}
      2 & -1 \\
      1 & 2
    \end{pmatrix}
    \stackrel{\text{ER } 2}{\to}
    \begin{pmatrix}
      1 & 2 \\
      2 & -1
    \end{pmatrix}
    \stackrel{\text{ER } 1}{\to}
    \begin{pmatrix}
      1 & 2 \\
      0 & -5
    \end{pmatrix}
    \stackrel{\text{ER } 1}{\to}
    \begin{pmatrix}
      1 & 0 \\
      0 & -5
    \end{pmatrix}
    \stackrel{\text{ER } 3}{\to}
    \begin{pmatrix}
      1 & 0 \\
      0 & 6
    \end{pmatrix}
  \]

  Alternatively, using what we have just proved,
  \begin{align*}
    \fit_1(A) &= (2, -1, 2, 1) = (1) \\
    \fit_2(A) &= (\det A) = (5)
  \end{align*}
  so \(d_1 = 1, d_1d_2 = 5\) so \(d_2 = 5\).
\end{eg}

Recall that we have remarked that a submodule of a finitely genereated module may not be finitely generated. However the following lemma tells us that submodules of finitely generated free modules over some particular rings are so:

\begin{lemma}
  Let \(R\) be a PID. Any submodule of \(R^n\) is generated by at most \(n\) elements.
\end{lemma}

\begin{proof}
  Let \(N \leq R^n\) and consider the ideal
  \[
    I = \{r \in R: \exists r_2, \dots, r_n \text{ such that } (r, r_2, \dots, r_n) \in N\},
  \]
  which is the image of \(N \stackrel{\iota}{\to} R^n \stackrel{\pi_1}{\to} R\), a submodule of \(R\).

    As \(R\) is a PID, \(I = (a) \ideal R\) for some \(a \in R\). Thus there is some
    \[
      n_1 = (a, a_2, a_2, \dots, a_n) \in N.
    \]
    Suppose \((r_1, r_2, \dots, r_n) \in N\). Then there exists some \(x \in R\) such that \(r_1 = ax\). Then
    \[
      (r_1, \dots, r_n) - x \cdot n_1 = (0, r_2 - xa_2, \dots, r_n - xa_n) \in N \cap (0 \oplus R^{n-1}).
    \]

    By induction \(N \cap (0 \oplus R^{n - 1}) \cong N' \leq R^{n - 1}\) is generated by \(n_2, \dots, n_n\) so \(n_1, \dots, n_n\) generate \(N\).
\end{proof}

\begin{theorem}
  Let \(R\) be a Euclidean domain and \(N \leq R^n\). Then there is a basis \(v_1, \dots, v_n\) of \(R^n\) such that \(N\) is generated by \(d_1v_1, \dots, d_rv_r\) for some \(0 \leq r \leq n\) and some \(d_1 \divides \dots \divides d_r\).
\end{theorem}

\begin{proof}
  By the previous lemma there are \(x_1, \dots, x_m \in N\) which generate \(N\) and \(0 \leq m \leq n\). Each \(x_i\) is an element of \(R^n\) so we can form an \(n \times m\) matrix whose first \(m\) columns are \(x_i\), i.e.
  \[
    A =
    \begin{pmatrix}
      \uparrow & \uparrow & & \uparrow & \uparrow & & \uparrow \\
      x_1 & x_2 & \cdots & x_m & 0 & \cdots & 0 \\
      \downarrow & \downarrow & & \downarrow & \downarrow & & \downarrow \\
    \end{pmatrix}
    \in \matrixring_{n, m}(R)
  \]

  We can put \(A\) into Smith Normal Form with diagonal entries \(d_1 \divides \cdots \divides d_r\) by elementary operations. Each row operation is given by a change of basis of \(R^n\) and each column operation is given by rechoosing the generating set \(x_1, \dots, x_m\). Thus after a change of basis of \(R^n\) to \(v_1, \dots, v_n\), \(N\) is generated by \(d_1v_1, \dots, d_rv_r\).
\end{proof}

\begin{corollary}
  A submodule \(N \leq R^n\) is isomorphic to \(R^m\) for some \(m \leq n\).
\end{corollary}

\begin{proof}
  By the theorem above, we can find a basis \(v_1, \dots, v_n\) for \(R^n\) such that \(N\) is generated by \(d_1v_1, \dots, d_mv_m\). These are linearly independent as a dependence between them would give a dependence between \(v_1, \dots, v_n\).
\end{proof}

Now we are ready for the big theorem in this course:

\begin{theorem}[Classification Theorem for Finitely Generated Modules over Euclidean Domain]\index{tbd}
  \label{thm:module over ED}
  Let \(R\) be a Euclidean domain and \(M\) a finitely generated \(R\)-modules. Then
  \[
    M \cong \frac{R}{(d_1)} \oplus \frac{R}{(d_2)} \oplus \dots \oplus \frac{R}{(d_r)} \oplus R \oplus \dots \oplus R
  \]
  for some \(d_i \neq 0\) with \(d_1 \divides d_2 \divides \cdots \divides d_r\).
\end{theorem}

\begin{proof}
  Let \(M\) be generateed by \(m_1, \dots, m_n \in M\), giving a surjection \(\varphi: R^n \surj M\) so \(M \cong R^n/\ker \varphi\). By the previous theorem there is a basis \(v_1, \dots, v_n\) of \(R^n\) such that \(\ker \varphi\) is generated by \(d_1v_1, \dots, d_rv_r\) with \(d_1 \divides \cdots \divides d_r\). Thus by changing the basis of \(R^n\) to \(v_i\)'s, \(\ker \varphi\) is generated by columns of
  \[
    \begin{pmatrix}
      d_1 \\
      & d_2 \\
      & & \ddots \\
      & & & d_r \\
      & & & & 0 \\
      & & & & & \ddots \\
      & & & & & & 0
    \end{pmatrix}
  \]
  so
  \[
    M \cong \frac{R^n}{\ker \varphi} \cong \left( \bigoplus_{i = 1}^r \frac{R}{(d_i)} \right) \oplus R \oplus \dots \oplus R
  \]
  as required.
\end{proof}

\begin{eg}
  Let \(R = \Z\), a Euclidean domain, and \(A\) be the abelian group (i.e.\ \(\Z\)-module) generated by \(a, b, c\), subject to
  \[
    \begin{cases}
      2a + 3b + c = 0 \\
      a + 2b = 0 \\
      5a + 6b + 7c = 0
    \end{cases}
  \]
  Thus \(A = \Z^3/N\) where \(N \leq \Z^3\) is generated by \((2, 3, 1)^T, (1, 2, 0)^T, (5, 6, 7)^T\). The matrix \(A\) whose columns are these vectors
  \[
    A =
    \begin{pmatrix}
      2 & 1 & 5 \\
      3 & 2 & 6 \\
      1 & 0 & 7
    \end{pmatrix}
  \]
  has Smith Normal Form with diagonal entries \(1, 1, 3\):

  \begin{proof}
    \begin{align*}
      \fit_1(A) &= (1) \\
      \fit_2(A) &\supseteq \left( \det
                  \begin{pmatrix}
                    2 & 1 \\
                    3 & 2
                  \end{pmatrix}
                        \right)
                        = (1) \\
      \fit_3(A) &= (\det A) = 3
    \end{align*}
    so \(d_1 = 1, d_1d_2 = 1, d_1d_2d_3 = 3\).
  \end{proof}

  After change of basis, \(N\) is generated by \((1, 0, 0)^T, (0, 1, 0)^T, (0, 0, 3)^T\) so
  \[
    A \cong \Z/1\Z \oplus \Z/1\Z \oplus \Z/3\Z \cong \Z/3\Z.
  \]
\end{eg}

We can derive, as a corollary actually, what we stated earlier without proof

\begin{theorem}[Structure Theorem for Finitely Generated Abelian Groups]
  Any finitely generated abelian group is isomorphis to
  \[
    C_{d_1} \times C_{d_2} \times \dots \times C_{d_r} \times C_\infty \times \dots \times C_\infty
  \]
  with \(d_1 \divides \cdots \divides d_r\).
\end{theorem}

\begin{proof}
  ``Trivial'' should suffice here but let us spell it out: apply \nameref{thm:module over ED} to \(\Z\), and note that
  \[
    \Z/(d) = C_d, \, \Z = C_\infty.
  \]
\end{proof}

The above classification theorem decompose into modules whose relation modules' principal ideals form a descending chain by divisibility. It turns out it is also possible to decompose by the coprime factors of the relation modules. Before that let us prove something we have known for a (very) long time, but at a high level:

\begin{lemma}[Chinese Remainder Theorem]
  Let \(R\) be a Euclidean domain and \(a, b \in R\) with \(\gcd(a, b) = 1\). Then
  \[
    R/(ab) \cong R(a) \oplus R/(b).
  \]
\end{lemma}

\begin{proof}
  Consider the \(R\)-module homomorphism
  \begin{align*}
    \varphi: R/(a) \oplus R/(b) &\to R/(ab) \\
    (r_1 + (a), r_2 + (b)) &\mapsto br_1 + ar_2 + (ab)
  \end{align*}
  As \(\gcd(a, b) = 1\), \((a, b) = (1)\) so \(1 = xa + yb\) for some \(x, y \in R\). Therefore for \(r \in R\), \(r = rxa + ryb\) so
  \[
    r + (ab) = rxa + ryb + (ab) = \varphi((ry + (a), rx + (b)))
  \]
  and so \(\varphi\) is surjective.

  If \(\varphi((r_1 + (a), r_2 + (b))) = 0\) then \(br_1 + ar_2 \in (ab)\). Thus \(a \divides br_1 + ar_2, a \divides br_1\). As \(\gcd(a, b) = 1\), \(a \divides r_1\) so \(r_1 + (a) = 0 + (a)\). Similarly \(r_2 + (b) = 0 + (b)\) so \(\varphi\) is injective.
\end{proof}

We thus have

\begin{theorem}[Primary Decomposition Theorem]\index{primary decomposition}
  \label{thm:primary decomposition}
  Let \(R\) be a Euclidean domain and \(M\) be a finitely generated \(R\)-module. Then
  \[
    M \cong \bigoplus_{i = 1}^n N_i
  \]
  with each \(N_i\) either equal to \(R\) or \(R/(p^m)\) for some prime \(p \in R\) and \(n \geq 1\).
\end{theorem}

\begin{proof}
  Note that if \(d = p_1^{m_1} \cdots p_k^{m_k}\) with \(p_i \in R\) distinct primes, by the previous lemma
  \[
    \frac{R}{(d)} \cong \frac{R}{(p_1^{n_1})} \oplus \dots \oplus \frac{R}{(p_k^{m_k})}.
  \]
  Plug this into \nameref{thm:module over ED} to get the required result.
\end{proof}

\subsection{\texorpdfstring{\(\F[X]\)}{𝔽}-modules and Normal Form}

For any field \(\F\), \(\F[X]\) is a Euclidean domain and so results of the last section apply. If \(V\) is an \(\F\)-vector space and \(\alpha: V \to V\) is an endomorphism, then we have
\begin{align*}
  \F[X] \times V &\to V \\
  (f, v) &\mapsto f(\alpha)(v)
\end{align*}
which makes \(V\) into an \(\F[X]\)-module, call it \(V_\alpha\). It turns out that \(\F[X]\)-module is the correct tool to study endomorphisms and many results in IB Linear Algebra, as well as many further results in algebra, can be obtained by looking into the \(\F[X]\)-module structure.

\begin{lemma}
  If \(V\) is finite-dimensional then \(V_\alpha\) is finitely generated as an \(\F[X]\)-module.
\end{lemma}

\begin{proof}
  \(V\) is a finitely generated \(\F\)-module and \(\F \leq \F[X]\) so \(V\) is also a finitely generated \(\F[X]\)-module.
\end{proof}

\begin{eg}\leavevmode
  \begin{enumerate}
  \item Suppose \(V_\alpha \cong \F[X]/(X^r)\) as an \(\F[X]\)-module. This has \(\F\)-basis \(\{X^i\}_{i = 0}^{r - 1}\) and the action of \(\alpha\) corresponds to multiplication by \(X\). Thus in this basis \(\alpha\) has matrix representation
    \[
      \begin{pmatrix}
        0 \\
        1 & 0 \\
        & 1 & 0 \\
        & & & \ddots \\
        & & & 1 & 0
      \end{pmatrix}
    \]
  \item Suppose \(V_\alpha \cong \F[X]/((X - \lambda)^r)\). Consider \(\beta = \alpha - \lambda \cdot \id\). Then \(V_\beta \cong \F[Y]/(Y^r)\) as an \(\F[Y]\)-module. By the previous example \(V\) has a basis so that \(\beta\) is given by the matrix above and \(\alpha\) is given by
    \[
      \begin{pmatrix}
        \lambda \\
        1 & \lambda \\
        & 1 & \lambda \\
        & & & \ddots \\
        & & & 1 & \lambda
      \end{pmatrix}
    \]
  \item Suppose \(V_\alpha \cong \F[X]/(f)\) where
    \[
      f = X^r + a_{r - 1}X^{r - 1} + \dots + a_1X + a_0.
    \]
    Then \(\{X^i\}_{i = 0}^{r - 1}\) is an \(\F\)-basis and in this basis \(\alpha\) is given by
    \[
      \begin{pmatrix}
        0 & & & & & & -a_0 \\
        1 & 0 & & & & & -a_1 \\
        & 1 & 0 & & & & -a_2 \\
        & & 1 & 0 & & & -a_3 \\
        & & & & \ddots & & \vdots \\
        & & & & & 1 & -a_{r - 1}
      \end{pmatrix}
    \]
    This matrix is called the \emph{companion matrix} for \(f\), written \(C(f)\).
  \end{enumerate}
\end{eg}

\begin{theorem}[Rational Canonical Form]\index{Rational Canonical Form}
  Let \(V\) be a finite-dimensional \(\F\)-vector space and \(\alpha: V \to V\) be linear. Regard \(V\) as an \(\F[X]\)-module \(V_\alpha\), we have
  \[
    V_\alpha \cong \frac{\F[X]}{(d_1)} \oplus \dots \oplus \frac{\F[X]}{(d_r)}
  \]
  with \(d_1 \divides d_2 \divides \cdots \divides d_r\). There is a basis of \(V\) with respect to which \(\alpha\) is given by
  \[
    \begin{pmatrix}
      C(d_1) \\
      & C(d_2) \\
      & & \ddots \\
      & & & C(d_r)
    \end{pmatrix}
  \]
\end{theorem}

\begin{proof}
  Apply \nameref{thm:module over ED} to \(\F[X]\), a Euclidean domain. Note that no copies of \(\F[X]\) appear as it has infinite dimension over \(V\).
\end{proof}

Some observations:
\begin{enumerate}
\item If \(\alpha\) is represented by a matrix \(A\) in some basis, then \(A\) is conjugate to the above matrix.
\item The minimal polynomial of \(\alpha\) is \(d_r \in \F[X]\).
\item The characteristic polynomial of \(\alpha\) is \(d_1d_2\cdots d_r\).
\end{enumerate}

Recall that we have two classification theorems for modules over Euclidean domain. The above theorem corresponds to invariant decomposition. One might naturally ask what result follows from primary decomposition. Before that let's convince ourselves that primes in \(\C[X]\) are as simple as they can be:

\begin{lemma}
  The primes in \(\C[X]\) are \(X - \lambda\) for \(\lambda \in \C\) up to associates.
\end{lemma}

\begin{proof}
  If \(f \in \C[X]\) is irreducible then Fundamental Theorem of Algebra says that \(f\) has a root \(\lambda\), i.e.\ \(f\) is a constant. Thus it is \(0\) or a unit, absurd. Thus \((X - \lambda) \divides f\), write \(f = (X - \lambda) g\). But \(f\) is irreducible so \(g\) is a unit. Thus \(f\) is an associate of \(X - \lambda\).
\end{proof}

\begin{remark}
  The lemma is equivalent to the statement that \(\C\) is algebraically closed, which says that every polynomial with coefficients in \(\C\) factorises into linear factors in \(\C\). In fact, every field can be extended to an algebraically closed one. This will be discussed in detail in IID Galois Theory.
\end{remark}

\begin{theorem}[Jordan Normal Form]\index{Jordan normal form}
  Let \(V\) be a finite-dimensional \(\C\)-vector space and \(\alpha:V \to V\) linear. Consider \(V_\alpha\) as an \(\C[X]\)-module, then
  \[
    V_\alpha \cong \frac{\C[X]}{((X - \lambda_1)^{a_1})} \oplus \frac{\C[X]}{((X - \lambda_2)^{a_2})} \oplus \dots \oplus \frac{\C[X]}{((X - \lambda_r)^{a_r})}
    \]
    where the \(\lambda_i\)'s are not necessarily distinct. There is a basis of \(V\) with respect to which \(\alpha\) is given by
    \[
      \begin{pmatrix}
        J_{a_1}(\lambda_1) \\
        & J_{a_2}(\lambda_2) \\
        & & \ddots \\
        & & & J_{a_r}(\lambda_r)
      \end{pmatrix}
    \]
    where
    \[
      J_m(\lambda) =
      \begin{pmatrix}
        \lambda \\
        1 & \lambda \\
        & 1 & \lambda \\
        & & & \ddots \\
        & & & 1 & \lambda
      \end{pmatrix}
    \]
    has size \(m\).
\end{theorem}

\begin{proof}
  Immediate from \nameref{thm:primary decomposition} and knowing all the primes in \(\C[X]\).
\end{proof}

\begin{remark}\leavevmode
  \begin{enumerate}
  \item The \(J_m(\lambda)\) are called \emph{Jordan \(\lambda\)-blocks}.
  \item The minimal polynomial of \(\alpha\) is
    \[
      m_\alpha(t) = \prod_\lambda (X - \lambda)^{a_\lambda}
    \]
    where \(a_\lambda\) is the largest \(\lambda\)-block.
  \item The characteristic polynomial of \(\alpha\) is
    \[
      \chi_\alpha(t) = \prod_\lambda (X - \lambda)^{b_\lambda}
    \]
    where \(b_\lambda\) is the sum of the sizes of the \(\lambda\)-blocks.
  \item Consider \(\ker (X \cdot -: V_\alpha \to V_\alpha)\). What is its dimension?

    On \(\C[X]/(X - \lambda)^a\), if \(\lambda \neq 0\) then the map \(X \cdot -\) is an isomorphism since
    \[
      \ker (X \cdot -) = \{f + ((X - \lambda)^a): Xf \in ((X - \lambda)^a)\}
    \]
    so if \(Xf = (X - \lambda)^a \cdot g\), as \(X\) and \(X - \lambda\) are coprime, \(X \divides g\), \((X - \lambda)^a \divides f\) so \(\ker (X \cdot -) = 0\).

    If \(\lambda = 0\), \(X \cdot -: \C[X]/(X^a) \to \C[X]/(X^a)\) has matrix
    \[
      \begin{pmatrix}
        0 \\
        1 & 0 \\
        & 1 & 0 \\
        & & & \ddots \\
        & & & 1 & 0
      \end{pmatrix}
    \]
    so \(1\)-dimensional kernel. Thus
    \[
      \dim \ker (X \cdot -: V_\alpha \to V_\alpha) = \# \text{Jordan \(0\)-blocks}.
    \]
  \item Similarly, \(X^2 \cdot -: \C[X]/((X - \lambda)^a) \to \C[X]/((X - \lambda)^a)\) is an isomorphism for \(\lambda \neq 0\) and for \(\lambda = 0\) is given by the matrix
    \[
      \begin{pmatrix}
        0 \\
        0 & 0 \\
        1 & 0 & 0 \\
        & 1 & 0 & \\
        & & & \ddots \\
        & & & & 0
      \end{pmatrix}
    \]
    which has \(2\)-dimensional kernel if \(a > 1\) and \(1\)-dimensional kernel if \(a = 1\). Therefore
    \begin{align*}
      \dim \ker (X^2 \cdot -: V_\alpha \to V_\alpha) &= \# \text{Jordan \(0\)-blocks} \\
                                                     &+ \# \text{Jordan \(0\)-blocks of size \(> 1\)}.
    \end{align*}
    so
    \[
      \# \text{Jordan \(0\)-blocks of size \(1\)} = 2 \dim \ker (X \cdot -) - \dim \ker (X^2 \cdot -).
    \]
    Using the same method we can find Jordan \(0\)-blocks of other sizes.
    \iffalse
    \begin{align*}
      \dim (X^r \cdot -: V_\alpha \to V_\alpha) &= \# \text{Jordan \(0\)-blocks} \\
                                                &+ \sum_{i = 1}^{r - 1} \# \text{Jordan \(0\)-blocks of size \(> i\)} \\
      \sum_{i = 1}^{r - 1} \# \text{blocks of size \(\leq i\)} &= r \cdot \dim \ker (X \cdot -) - \dim \ker (X^r \cdot -) \\
      \# \text{blocks of size \(\leq r - 1\)} &= r \cdot \dim \ker (X \cdot -) - \dim \ker (X^r \cdot -) \\
                                                &- (r - 1) \cdot \dim \ker (X \cdot -) + \dim \ker (X^{r - 1} \cdot -) \\
                                                &= \dim \ker (X \cdot -) - \dim \ker (X^r \cdot -) + \dim \ker (X^{r - 1} \cdot -) \\
      \# \text{blocks of size \(r\)} &= 2 \cdot \dim \ker (X^r \cdot -) - \dim \ker (X^{r + 1} \cdot -) - \dim \ker(X^{r - 1} \cdot -)
    \end{align*}
    \fi
  \end{enumerate}
\end{remark}

\subsection{Conjugacy}

This section is non-examinable.

\begin{lemma}
  If \(\alpha: V \to V\) and \(\beta: W \to W\) are endomorphism of \(\F\)-vector spaces, then \(V_\alpha \cong W_\beta\) as \(\F[X]\)-modules if and only if there is an isomorphism \(\gamma: V \to W\) such that
  \[
    \gamma^{-1} \beta \gamma = \alpha,
  \]
  i.e.\ \(\alpha\) and \(\beta\) are conjugates.
\end{lemma}

\begin{proof}
  Let \(\hat \gamma: V_\alpha \to W_\beta\) be an \(\F[X]\)-module isomorphism. In particular \(\hat \gamma\) gives an \(\F\)-vector space isomorphism \(\gamma: V \to W\). Then
  \begin{align*}
    \beta \compose \gamma: W_\beta &\to W_\beta \\
    v &\mapsto X \cdot \gamma(v)
  \end{align*}

  Now
  \begin{align*}
    X \cdot \gamma(v) &= X \cdot \hat \gamma(v) \text{ \(\hat \gamma\) as an \(\F[X]\)-module map} \\
                      &= \hat \gamma(X \cdot v) \text{ in \(\F[X]\)-module \(V_\alpha\)} \\
                      &= \hat \gamma(\alpha(v)) \\
                      &= \gamma(\alpha(v))
  \end{align*}
  so \(\beta \compose \gamma = \gamma \compose \alpha\), \(\gamma^{-1} \compose \beta \compose \gamma = \alpha\). Therefore if \(W = V\) then \(V_\alpha \cong V_\beta\) if and only if \(\alpha\) and \(\beta\) are conjugates.
  \[
    \begin{tikzcd}
      V \ar[r, "\alpha"] \ar[d, dashed, "\gamma"'] & V \ar[d, dashed, "\gamma"] & V_\alpha \ar[r, dashed, "\alpha = X \cdot -"] \ar[d, "\hat \gamma"'] & V_\alpha \ar[d, "\hat \gamma"] \\
      W \ar[r, "\beta"] & W & W_\beta \ar[r, dashed, "\beta = X \cdot -"] & W_\beta
    \end{tikzcd}
  \]
\end{proof}

Applying \nameref{thm:module over ED}, we get

\begin{corollary}
  There is a bijection
  \[
    \{\text{conjugacy class of } \matrixring_n(\F)\} \leftrightarrow
    \left\{
    \begin{array}[h]{c}
      \text{sequence of monic polynomials } d_1, \dots, d_r \\
      \text{ where } d_1 \divides \cdots \divides d_r \text{ and } \deg (d_1 \cdots d_r) = n
    \end{array}
  \right\}
  \]
\end{corollary}

\begin{eg}
  Consider \(\GL_2(\F)\). The conjugacy classes are described by \(d_1 \divides \cdots \divides d_r\) where \(\deg (d_1 \cdots d_r) = 2\). Therefore we have one of the followings:
  \begin{enumerate}
  \item \(\deg d_1 = 2\),
  \item \(\deg d_1 = \deg d_2 = 1\). As \(d_1 \divides d_2\), \(d_1 = d_2\).
  \end{enumerate}
  These give us respecively
  \begin{enumerate}
  \item \(\F[X]/(X^2 + a_1X + a_0)\),
  \item \(\F[X]/(X - \lambda) \oplus \F[X]/(X - \lambda)\).
  \end{enumerate}

  Therefore any \(A \in \GL_2(\F)\) is conjugate to one of
  \[
    \begin{pmatrix}
      0 & -a_0 \\
      1 & -a_1
    \end{pmatrix}
    ,\quad
    \begin{pmatrix}
      \lambda & 0 \\
      0 & \lambda
    \end{pmatrix}
  \]
  They are not conjugates.

  The first case be futher split into two cases. If \(X^2 + a_1X + a_0\) is reducible they it factorises as either \((X - \lambda)^2\) or \((X - \lambda)(X - \mu)\) where \(\lambda \neq \mu\). Thus we get one of
  \[
    \begin{pmatrix}
      \lambda & 0 \\
      1 & \lambda
    \end{pmatrix}
    ,\quad
    \begin{pmatrix}
      \lambda & 0 \\
      0 & \mu
    \end{pmatrix}
  \]
\end{eg}

\begin{eg}
  Let \(\F = \Z/3\Z\). For what \(a_1, a_0\) is \(X^2 + a_1X + a_0 \in \F[X]\) irreducible? There are \(3 \times 3 = 9\) polynomials in total, of which \(\binom{3}{1} + \binom{3}{2} = 6\) are reducible. Guess (any verify!) that the irreducibles are \(X^2 + 1, X^2 + 2X + 2, X^2 + 2X + 2\). Therefore the conjugacy classes in \(\GL_2(\Z/3\Z)\) are
  \[
    \begin{array}[h]{ccc}
      \begin{psmallmatrix}
        0 & -1 \\
        1 & 0
      \end{psmallmatrix}
          &
            \begin{psmallmatrix}
              0 & -2 \\
              1 & -1
            \end{psmallmatrix}
          &
            \begin{psmallmatrix}
              0 & -2 \\
              1 & -2
            \end{psmallmatrix}
      \\ \hline
      \begin{psmallmatrix}
        \lambda & 0 \\
        1 & \lambda
      \end{psmallmatrix}
          &
            \lambda \neq 0
      \\ \hline
      \begin{psmallmatrix}
        \lambda & 0 \\
        0 & \mu
      \end{psmallmatrix}
          &
            \lambda, \mu \neq 0
    \end{array}
  \]
  so there are in total \(8\) conjugacy classes. They have order
  \[
    \begin{array}[h]{c|c|c|c|c|c}
      \begin{psmallmatrix}
        0 & 2 \\
        1 & 0
      \end{psmallmatrix}
          &
            \begin{psmallmatrix}
              0 & 1 \\
              1 & 2
            \end{psmallmatrix}
          &
            \begin{psmallmatrix}
              0 & 1 \\
              1 & 1
            \end{psmallmatrix}
          &
            \begin{psmallmatrix}
              1 & 0 \\
              1 & 1
            \end{psmallmatrix}
          &
            \begin{psmallmatrix}
              2 & 0 \\
              1 & 2
            \end{psmallmatrix}
          &
            \begin{psmallmatrix}
              \lambda & 0 \\
              0 & \mu
            \end{psmallmatrix}
      \\ \hline
      4 & 8 & 8 & 3 & 6 & 2
    \end{array}
  \]
\end{eg}

Just for fun, let's use what we deduced above and knowledge about Sylow \(p\)-subgroups way back in the beginning of the course to determine the group structure of \(\GL_2(\Z/3/Z)\).

Recall that
\[
  |GL_2(\Z/3\Z)| = (3^2 - 1)(3^2 - 3) = 2^4 \cdot 3
\]
so the Sylow \(2\)-subgroup has order \(2^4 = 16\). There are no elements of order \(16\) so it cannot be cyclic. Let
\[
  A =
  \begin{pmatrix}
    0 & 2 \\
    1 & 0
  \end{pmatrix}
  ,\quad
  B =
  \begin{pmatrix}
    0 & 1 \\
    1 & 2
  \end{pmatrix}
\]
so
\[
  A^{-1}BA =
  \begin{pmatrix}
    2 & 2 \\
    2 & 0
  \end{pmatrix}
  = B^3.
\]
Therefore \(\generation{B} \normal \generation{A, B} \leq \GL_2(\Z/3\Z)\). The 2nd Isomorphism Theorem says that
\[
  \generation{A, B}/\generation{B} \cong \generation{A}/(\generation{A} \cap \generation{B}).
\]
Now \(\generation A \cap \generation B = \generation*{\begin{psmallmatrix} 2 & 0 \\ 0 & 2 \end{psmallmatrix}}\), a group of order \(2\). Therefore
\[
  |\generation{A, B}| = \frac{|\generation A| \cdot |\generation B|}{|\generation A \cap \generation B|} = \frac{8 \cdot 4}{2} = 16
\]
which is a Sylow \(2\)-subgroup of \(\GL_2(\Z/3\Z)\). It has presentation
\[
  \generation{A, B| A^4 = B^8 = 1, A^{-1}BA = B^3},
\]
the semidihedral group of order \(16\).

Since we still have time left, we can do one more fun example.

\begin{eg}
  Let \(R = \Z[X]/(X^2 + 5) \cong \Z[\sqrt{-5}] \leq \C\). Then
  \[
    (1 + X)(1 - X) = 1 - X^2 = 1 + 5 = 6 = 2 \cdot 3.
  \]
  As \(1 \pm X, 2\) and \(3\) are irreducibles \(R\) is \emph{not} a UFD. Let
  \[
    I_1 = (3, 1 + X), I_2 = (3, 1 - x)
  \]
  be submodules of \(R\). Consider
  \begin{align*}
    \varphi: I_1 \oplus I_2 &\to R \\
    (a, b) &\to a + b
  \end{align*}
  Then \(\im \varphi = (3, 1 + X, 1 - X)\). Since \(3 - (1 + X) - (1 - X) = 1\), \(\im \varphi = R\). Also
  \[
    \ker \varphi = \{(a, b) \in I_1 \oplus I_2 : a + b = 0\} \cong I_1 \cap I_2
  \]
  where the last isomorphism can be deduced from the map \((x, -x) \mapsfrom x\). Note that \((3) \subseteq I_1 \cap I_2\). Let
  \[
    s \cdot 3 + t \cdot (1 + X) \in (3, 1 - X) \subseteq R = \Z[X]/(X^2 - 5).
  \]
  Reduce mod \(3\), we get
  \[
    t \cdot (1 + X) = (1 - X)p \mod (3, X^2 + 5) = (3, X^2 - 1) = (2, (X + 1)(X - 1))
  \]
  so \(1 - X \divides t\), \((1 + X)(1 - X) \divides t(1 + X)\) so
  \[
    t(1 + X) = q(X^2 - 1) = q(X^2 + 5 -6) = 3(-2q).
  \]
  Then \(s \cdot 3 + t \cdot (1 + X)\) is divisible by \(3\) so \(I_1 \cap I_2 \subseteq (3)\). Equality follows.

  From example sheet 4 we know that if \(N \leq M\) and \(M/N \cong \R^n\) then \(M \cong N \oplus R^n\). Here
  \[
    I_1 \oplus I_2/\ker \varphi \cong \im \varphi = R
  \]
  so
  \[
    I_1 \oplus I_2 \cong R \oplus \ker \varphi = R \oplus (3).
  \]
  Consider
  \begin{align*}
    \psi: R &\to (3) \\
    x &\mapsto 3x
  \end{align*}
  a surjective \(R\)-module map. \(\ker \varphi = 0\) as \(R\) is an integral domain so \(\varphi\) is an isomorphism. Thus
  \[
    I_1 \oplus I_2 \cong R \oplus R = R^2.
  \]
  In particular this shows that sums of non-free modules can be free.

  Next we claim that \(I_1\) is not principal. If \(I_1 = (a + bX)\) then \(I_2 = (a + bX)\). This is because \(I_1 = (3, 1 + X)\) and \(I_2 = (3, 1 - X)\) and \(R\) has automorphism \(X \mapsto -X\) which interchanging \(I_1\) and \(I_2\).\footnote{This technique will play a central role in IID Galois Theory.} But then
  \[
    (3) = I_1 \cap I_2 = ((a + bX)(a - bX)) = (a^2 - b^2X^2) = (a^2 + 5b^2)
  \]
  so \(a^2 + 5b^2 \divides 3\), absurd.

  In summary, we have shown that
  \begin{enumerate}
  \item \(I_1\) needs \(2\) elements to generate (as it is not principal), but it is not the free module \(R^2\).
  \item \(I_1\) is a direct summand of \(R^2\).
  \end{enumerate}
\end{eg}

\printindex
\end{document}
