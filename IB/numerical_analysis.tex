\documentclass[a4paper]{article}

\def\npart{IB}

\def\ntitle{Numerical Analysis}
\def\nlecturer{H.\ Fawzi}

\def\nterm{Lent}
\def\nyear{2018}

\input{header}

\makeindex

\begin{document}

\input{titlepage}

\tableofcontents

\setcounter{section}{-1}

\section{Introduction}

Numerical analysis is the study of \emph{algorithms} for continuous mathematics. Examples of problems in continuous mathematics are:
\begin{itemize}
\item solve \(f(x) = 0\) where \(f: \R^n \to \R\),
\item solve \(\frac{dx}{dt} = f(x)\) where \(f: \R^n \to \R^n\)
\item optimisation: find \(\min f(x)\) where \(x \in \R^n, f: \R^n \to \R\).
\end{itemize}

A note on complexity: we measure the complexity of an algorithm by the number of \emph{elementary operations} (\(+, \times, -, /\)) it needs.

Big \(O\) notation: for example \(O(n), O(n^2)\), where \(n\) is input size. We also have complexity \(O(f(n))\) if the number of operations is at most \(cf(n)\) where \(c > 0\).

\section{Polynomial Interpolations}

Denote a degree \(n\) polynomial by
\[
  p(x) = p_0 + p_1x + \dots + p_nx^n
\]
and let \(P_n[x]\) be the vector space of polynomials of degree at most \(n\). The interpolation problem is, given \(x_0, x_1, \dots, x_n \in \R\) and \(f_0, f_1, \dots, f_n \in \R\), find \(p \in P_n[x]\) such that \(p(x_i) = f_i\) for \(i = 0, \dots, n\).

\subsection{Lagrange formula}

Claim that
\[
  p(x) = \sum_{k = 0}^{n} f_k \underbrace{\prod_{\ell \neq k}^{ } \frac{x - x_\ell}{x_k - x_\ell}}_{L_k(x)}
\]
solves the problem.

Note that \(L_k(x_j) = \begin{cases} 1 & j = k \\ 0 &j \neq k \end{cases}\) and the result easily follows.

Now we prove the uniqueness of the solution. Assume \(q \in P_n[x]\) is another polynomial that interpolates the data. Then \(p - q\) has \(n + 1\) zeros. But a non-zero polynomial in \(P_n[x]\) has at most \(n\) zeros. Thus \(p - q\) must be zero.

This is an easy solution but what is its complexity? For each \(k\), the complexity of evaluating \(L_k(x)\) is \(O(n)\) so the total complexity of evaluating \(p(x)\) is \(O(n^2)\).

\subsubsection{Error of polynomial interpolation}

Let \(C^s[a, b]\) be the space of functions \([a, b] \to \R\) that are \(s\) times continuously differentiable.

\begin{theorem}
  Let \(f \in C^{n + 1}[a, b]\) and let \(p \in P_n[x]\) interpolate \(f\) at distinct \(x_0, \dots x_n\) , i.e.\ \(p(x_i) = f(x_i)\) for \(i = 0, \dots, n\). Then for all \(x \in [a, b]\), there exists \(\xi \in [a, b]\) such that
  \[
    f(x) - p(x) = \frac{1}{(n + 1)!} f^{(n + 1)}(\xi) \prod_{i = 0}^n (x - x_i).
  \]
\end{theorem}

The last term \(\prod_{i = 0}^n (x - x_i)\) is called the \emph{nodal polynomial}.

\begin{proof}
  If \(x = x_i\) for some \(i\) then the result is trivially true. Assume \(x\) is distinct from \(x_i\)'s for all \(i\). Define
  \[
    \varphi(t) = f(t) - \left(p(t) + (f(x) - p(x)) \frac{\prod_{i = 0}^n(t - x_i)}{\prod_{i = 0}^n(x - x_i)} \right).
  \]
  Note that the second term is by construction the interpolating polynomial of \(f\) at \(x_0, \dots, x_n\) and \(x\). Thus
  \[
    \varphi(x_0) = \varphi(x_1) = \dots = \varphi(x_n) = \varphi(x) = 0
  \]
  so \(\varphi\) has \(n + 2\) zeros. Recall from IA Analysis I Rolle's Theorem: if \(g \in C^1[a, b]\) such that \(g(a) = g(b)\) then there exists \(\alpha \in (a, b)\) such that \(g'(\alpha) = 0\). Apply it to \(\varphi\), we deduce that \(\varphi'\) has \(n + 1\) zeros. Inductively we find that \(\varphi^{(n + 1)}\) has \(1\) zero, i.e.\ there exists \(\xi \in [a, b]\) such that \(\varphi^{(n + 1)}(\xi) = 0\). Thus
  \[
    0 = \varphi^{(n + 1)}(\xi) = f^{(n + 1)}(\xi) - \left( \underbrace{p^{(n + 1)}(\xi)}_{= 0} + (f(x) - p(x)) \frac{(n + 1)!}{\prod_{i = 0}^n(x - x_i)} \right).
  \]
  Rearrange,
  \[
    f(x) - p(x) = \frac{1}{(n + 1)!}f^{(n + 1)}(\xi) \prod_{i = 0}^n(x - x_i).
  \]
\end{proof}

\begin{eg}
  \([a, b] = [-5, 5]\), \(x_j = -5 + 10 \frac{j}{n}\) for \(j = 0, \dots, n\). Plot \(\prod_{i = 0}^n(x - x_i)\), we note that it vanishes at \(x_i\)'s but blows up near the endpoints.
\end{eg}

This is called \emph{Runge's phenomenon}. If one attempts to interpolate \(f(x) = \frac{1}{1 + x^2}\) using equispaced points on \([-5, 5]\) and plots the error \(f(x) - p(x)|\).

Thus equispaced points may not be the most suitable to minimise the error. The remedy is to look for points \(x_0, \dots, x_n\) such that \(|\prod_{i = 0}^n(x - x_i)|\) is small. This leads us to \emph{Chebyshev points}. For example in the previous case we should choose
\[
  x_j = 5 \cos \frac{(n - j)\pi}{n}.
\]
This choice of points is the one that minimises
\[
  \max_{x \in [a, b]} \left| \prod_{i =0}^n(x - x_i) \right|.
\]

\subsection{Divided difference and Newton's interpolation formula}

\begin{definition}[Divided difference]\index{divided difference}
  Given pairwise distinct points \(x_0, \dots, x_n\), let \(p \in P_n[x]\) that interpolates \(f \in C[a, b]\) at these points. The coefficient of \(x^n\) in \(p\) is called the \emph{divided difference} of \(f\) at \((x_0, \dots, x_n)\) and is denoted \(f[x_0, \dots, x_n]\).
\end{definition}

We know from the last lecture that the interpolent is
\[
  p(x) = \sum_{k = 0}^n f(x_k) \prod_{\ell \neq k} \frac{x - x_\ell}{x_k - x_\ell}.
\]
Thus we get
\[
  f[x_0, \dots, x_n] = \sum_{k = 0}^n f(x_k) \prod_{\ell \neq k} \frac{1}{x_k - x_\ell}.
\]

The next example illustrates the reason behind the name ``divided difference''.

\begin{eg}\leavevmode
  \begin{itemize}
  \item \(f[x_0] = f(x_0)\).
  \item \(f[x_0, x_1] = f(x_0)\frac{1}{x_0 - x_1} + f(x_1)\frac{1}{x_1 - x_0} = \frac{f(x_1) - f(x_0)}{x_1 - x_0}\).
  \end{itemize}
\end{eg}

This observation is generalised by

\begin{theorem}
  \[
    f[x_0, x_1, \dots, x_{n + 1}] = \frac{f[x_1, \dots, x_{n + 1}] - f[x_0, \dots, x_n]}{x_{n + 1} - x_0}.
  \]
\end{theorem}

\begin{proof}
  Let \(p \in P_n[x]\) interpolate \(f\) at \(x_0, \dots, x_n\) and \(q \in P_n[x]\) interpolate \(f\) at \(x_1, \dots, x_{n + 1}\) and let
  \[
    r(x) = \frac{x - x_{n + 1}}{x_0 - x_{n + 1}}p(x) + \frac{x - x_0}{x_{n + 1} - x_0}q(x) \in P_{n + 1}[x].
  \]
  Observe that \(r\) interpolates \(f\) at \(x_0, \dots, x_{n + 1}\). So the divided difference, i.e.\ coefficient of \(x^{n + 1}\) in \(r(x)\) is
  \[
    \frac{1}{x_0 - x_{n + 1}}f[x_0, \dots, x_n] + \frac{1}{x_{n + 1} - x_0}f[x_1, \dots, x_{n + 1}].
  \]
\end{proof}

\begin{theorem}
  Assume \(x_0, \dots, x_n\) are pairwise distinct in \([a, b]\) and \(f \in C^n[a, b]\), then there exists \(\xi \in [a, b]\) such that
  \[
    f[x_0, \dots, x_n] = \frac{1}{n!}f^{(n)}(\xi).
  \]
\end{theorem}

\begin{proof}
  Let \(p \in P_n[x]\) interpolate \(f\) at \(x_0, \dots, x_n\). Then function \(f - p\) has \(n + 1\) zeros in \([a, b]\). Applying Rolle's Theorem \(n\) times, we get that \((f - p)^{(n)}\) has at least one zero in \([a, b]\). Let \(\xi \in [a, b]\) be such that \(f^{(n)}(\xi) - p^{(n)}(\xi) = 0\). As \(p\) has degree \(n\), \(p^{(n)}(x) = n! \cdot \text{ leading coefficient}\). The result thus follows.
\end{proof}

\begin{theorem}[Newton's interpolation formula]
  Let \(x_0, \dots, x_n\) be pairwise distinct and let
  \[
    p(x) = f[x_0] + f[x_0, x_1](x - x_0) + \dots + f[x_0, \dots, x_n]\prod_{i = 0}^{n - 1}(x - x_i).
  \]
  Then \(p(x_i) = f(x_i)\) for \(i = 0, \dots, n\).
\end{theorem}

\begin{proof}
  Induction on \(n\). If \(n = 0\) then \(p(x) = f[x_0]\) which trivially satisfies \(p(x_0) = f[x_0] = f(x_0)\).

  Suppose it holds for \(n - 1\). Let \(p_n \in P_{n - 1}[x]\) interpolate \(f\) at \(x_0, \dots, x_{n - 1}\) and \(p \in P_n[x]\) interpolate \(f\) at \(x_0, \dots, x_{n - 1}, x_n\). \(p - p_n\) is a polynomial of degree \(n\) that vanishes at \(x_0, \dots, x_{n - 1}\), implying that
  \[
    p - p_n = \alpha \prod_{i = 0}^{n - 1}(x - x_i).
  \]
  But \(\alpha\) is the coefficient of \(x^n\) in \(p\) and so we must have \(\alpha = f[x_0, \dots, x_n]\). Thus we deduce that
  \[
    p = p_n + f[x_0, \dots, x_n] \prod_{i = 0}^{n - 1}(x - x_i).
  \]
  By using induction hypothesis, the result follows.
\end{proof}

\subsubsection{Evaluating Newton's interpolating formula}

Recall that the divided difference is defined recursively by
\[
  f[x_0, x_1, \dots, x_{n + 1}] = \frac{f[x_1, \dots, x_{n + 1}] - f[x_0, \dots, x_n]}{x_{n + 1} - x_0}.
\]
We can draw a table to evaluate them:
\[
  \begin{tikzcd}[column sep=tiny, row sep=tiny]
    f[x_0] \ar[dr] \\
    & f[x_0, x_1] \ar[dr] \\
    f[x_1] \ar[ur] & & \cdots \ar[dr] \\
    \vdots & & & f[x_0, \dots, x_n] \\
    f[x_{n - 1}] \ar[dr] & & \cdots \ar[ur] \\
    & f[x_{n - 1}, x_n] \ar[ur] \\
    f[x_n] \ar[ur]
  \end{tikzcd}
\]

The output of this procedure is \(f[x_j, x_{j + 1}, \dots, x_\ell]\) for all \(0 \leq j \leq \ell \leq n\). To evaluate a new divided difference requires 3 operations (2 substractions and 1 division) so the complexity is
\[
  3(n - 1) + 3(n - 2) + \dots + 3 \approx 3 \cdot \frac{n^2}{2} \sim O(n^2).
\]

Having obtained the divided differences, we can use Horner's scheme to evaluate Newton's formula in \(O(n)\). Note that the term \((x - x_0)\), for example, appears in every term except the first one in Newton's interpolating formula so we may group them together
\begin{align*}
  p(x) &= f[x_0] + (x - x_0) (f[x_0, x_1] + (x - x_1)(f[x_0, x_1, x_2] + \cdots \\
       &+ (x - x_{n - 1})f[x_0, x_1, \dots, x_n])).
\end{align*}
This way the evaluation requires \(O(n)\) operations.




\printindex

\iffalse
http://damtp.cam.ac.uk/user/hf323/L18-IB-NA/
\fi

\end{document}
