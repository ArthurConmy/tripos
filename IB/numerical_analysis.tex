\documentclass[a4paper]{article}

\def\npart{IB}

\def\ntitle{Numerical Analysis}
\def\nlecturer{H.\ Fawzi}

\def\nterm{Lent}
\def\nyear{2018}

\input{header}

\newcommand*{\inner}{\innerproduct}

\makeindex

\begin{document}

\input{titlepage}

\tableofcontents

\setcounter{section}{-1}

\section{Introduction}

Numerical analysis is the study of \emph{algorithms} for continuous mathematics. Examples of problems in continuous mathematics are:
\begin{itemize}
\item solve \(f(x) = 0\) where \(f: \R^n \to \R\),
\item solve \(\frac{dx}{dt} = f(x)\) where \(f: \R^n \to \R^n\)
\item optimisation: find \(\min f(x)\) where \(x \in \R^n, f: \R^n \to \R\).
\end{itemize}

A note on complexity: we measure the complexity of an algorithm by the number of \emph{elementary operations} (\(+, \times, -, /\)) it needs.

Big \(O\) notation: for example \(O(n), O(n^2)\), where \(n\) is input size. We also have complexity \(O(f(n))\) if the number of operations is at most \(cf(n)\) where \(c > 0\).

\section{Polynomial Interpolations}

Denote a degree \(n\) polynomial by
\[
  p(x) = p_0 + p_1x + \dots + p_nx^n
\]
and let \(P_n[x]\) be the vector space of polynomials of degree at most \(n\). The interpolation problem is, given \(x_0, x_1, \dots, x_n \in \R\) and \(f_0, f_1, \dots, f_n \in \R\), find \(p \in P_n[x]\) such that \(p(x_i) = f_i\) for \(i = 0, \dots, n\).

\subsection{Lagrange formula}\index{Langrange formula}

Claim that
\[
  p(x) = \sum_{k = 0}^{n} f_k \underbrace{\prod_{\ell \neq k}^{ } \frac{x - x_\ell}{x_k - x_\ell}}_{L_k(x)}
\]
solves the problem.

Note that \(L_k(x_j) = \begin{cases} 1 & j = k \\ 0 &j \neq k \end{cases}\) and the result easily follows.

Now we prove the uniqueness of the solution. Assume \(q \in P_n[x]\) is another polynomial that interpolates the data. Then \(p - q\) has \(n + 1\) zeros. But a non-zero polynomial in \(P_n[x]\) has at most \(n\) zeros. Thus \(p - q\) must be zero.

This is an easy solution but what is its complexity? For each \(k\), the complexity of evaluating \(L_k(x)\) is \(O(n)\) so the total complexity of evaluating \(p(x)\) is \(O(n^2)\).

\subsubsection{Error of polynomial interpolation}

Let \(C^s[a, b]\) be the space of functions \([a, b] \to \R\) that are \(s\) times continuously differentiable.

\begin{theorem}
  Let \(f \in C^{n + 1}[a, b]\) and let \(p \in P_n[x]\) interpolate \(f\) at distinct \(x_0, \dots x_n\) , i.e.\ \(p(x_i) = f(x_i)\) for \(i = 0, \dots, n\). Then for all \(x \in [a, b]\), there exists \(\xi \in [a, b]\) such that
  \[
    f(x) - p(x) = \frac{1}{(n + 1)!} f^{(n + 1)}(\xi) \prod_{i = 0}^n (x - x_i).
  \]
\end{theorem}

The last term \(\prod_{i = 0}^n (x - x_i)\) is called the \emph{nodal polynomial}.

\begin{proof}
  If \(x = x_i\) for some \(i\) then the result is trivially true. Assume \(x\) is distinct from \(x_i\)'s for all \(i\). Define
  \[
    \varphi(t) = f(t) - \left(p(t) + (f(x) - p(x)) \frac{\prod_{i = 0}^n(t - x_i)}{\prod_{i = 0}^n(x - x_i)} \right).
  \]
  Note that the second term is by construction the interpolating polynomial of \(f\) at \(x_0, \dots, x_n\) and \(x\). Thus
  \[
    \varphi(x_0) = \varphi(x_1) = \dots = \varphi(x_n) = \varphi(x) = 0
  \]
  so \(\varphi\) has \(n + 2\) zeros. Recall from IA Analysis I Rolle's Theorem: if \(g \in C^1[a, b]\) such that \(g(a) = g(b)\) then there exists \(\alpha \in (a, b)\) such that \(g'(\alpha) = 0\). Apply it to \(\varphi\), we deduce that \(\varphi'\) has \(n + 1\) zeros. Inductively we find that \(\varphi^{(n + 1)}\) has \(1\) zero, i.e.\ there exists \(\xi \in [a, b]\) such that \(\varphi^{(n + 1)}(\xi) = 0\). Thus
  \[
    0 = \varphi^{(n + 1)}(\xi) = f^{(n + 1)}(\xi) - \left( \underbrace{p^{(n + 1)}(\xi)}_{= 0} + (f(x) - p(x)) \frac{(n + 1)!}{\prod_{i = 0}^n(x - x_i)} \right).
  \]
  Rearrange,
  \[
    f(x) - p(x) = \frac{1}{(n + 1)!}f^{(n + 1)}(\xi) \prod_{i = 0}^n(x - x_i).
  \]
\end{proof}

\begin{eg}
  \([a, b] = [-5, 5]\), \(x_j = -5 + 10 \frac{j}{n}\) for \(j = 0, \dots, n\). Plot \(\prod_{i = 0}^n(x - x_i)\), we note that it vanishes at \(x_i\)'s but blows up near the endpoints.
\end{eg}

This is called \emph{Runge's phenomenon}. If one attempts to interpolate \(f(x) = \frac{1}{1 + x^2}\) using equispaced points on \([-5, 5]\) and plots the error \(f(x) - p(x)|\).

Thus equispaced points may not be the most suitable to minimise the error. The remedy is to look for points \(x_0, \dots, x_n\) such that \(|\prod_{i = 0}^n(x - x_i)|\) is small. This leads us to \emph{Chebyshev points}. For example in the previous case we should choose
\[
  x_j = 5 \cos \frac{(n - j)\pi}{n}.
\]
This choice of points is the one that minimises
\[
  \max_{x \in [a, b]} \left| \prod_{i =0}^n(x - x_i) \right|.
\]

\subsection{Divided difference and Newton's interpolation formula}

\begin{definition}[Divided difference]\index{divided difference}
  Given pairwise distinct points \(x_0, \dots, x_n\), let \(p \in P_n[x]\) that interpolates \(f \in C[a, b]\) at these points. The coefficient of \(x^n\) in \(p\) is called the \emph{divided difference} of \(f\) at \((x_0, \dots, x_n)\) and is denoted \(f[x_0, \dots, x_n]\).
\end{definition}

We know from the last lecture that the interpolent is
\[
  p(x) = \sum_{k = 0}^n f(x_k) \prod_{\ell \neq k} \frac{x - x_\ell}{x_k - x_\ell}.
\]
Thus we get
\[
  f[x_0, \dots, x_n] = \sum_{k = 0}^n f(x_k) \prod_{\ell \neq k} \frac{1}{x_k - x_\ell}.
\]

The next example illustrates the reason behind the name ``divided difference''.

\begin{eg}\leavevmode
  \begin{itemize}
  \item \(f[x_0] = f(x_0)\).
  \item \(f[x_0, x_1] = f(x_0)\frac{1}{x_0 - x_1} + f(x_1)\frac{1}{x_1 - x_0} = \frac{f(x_1) - f(x_0)}{x_1 - x_0}\).
  \end{itemize}
\end{eg}

This observation is generalised by

\begin{theorem}
  \[
    f[x_0, x_1, \dots, x_{n + 1}] = \frac{f[x_1, \dots, x_{n + 1}] - f[x_0, \dots, x_n]}{x_{n + 1} - x_0}.
  \]
\end{theorem}

\begin{proof}
  Let \(p \in P_n[x]\) interpolate \(f\) at \(x_0, \dots, x_n\) and \(q \in P_n[x]\) interpolate \(f\) at \(x_1, \dots, x_{n + 1}\) and let
  \[
    r(x) = \frac{x - x_{n + 1}}{x_0 - x_{n + 1}}p(x) + \frac{x - x_0}{x_{n + 1} - x_0}q(x) \in P_{n + 1}[x].
  \]
  Observe that \(r\) interpolates \(f\) at \(x_0, \dots, x_{n + 1}\). So the divided difference, i.e.\ coefficient of \(x^{n + 1}\) in \(r(x)\) is
  \[
    \frac{1}{x_0 - x_{n + 1}}f[x_0, \dots, x_n] + \frac{1}{x_{n + 1} - x_0}f[x_1, \dots, x_{n + 1}].
  \]
\end{proof}

\begin{theorem}
  Assume \(x_0, \dots, x_n\) are pairwise distinct in \([a, b]\) and \(f \in C^n[a, b]\), then there exists \(\xi \in [a, b]\) such that
  \[
    f[x_0, \dots, x_n] = \frac{1}{n!}f^{(n)}(\xi).
  \]
\end{theorem}

\begin{proof}
  Let \(p \in P_n[x]\) interpolate \(f\) at \(x_0, \dots, x_n\). Then function \(f - p\) has \(n + 1\) zeros in \([a, b]\). Applying Rolle's Theorem \(n\) times, we get that \((f - p)^{(n)}\) has at least one zero in \([a, b]\). Let \(\xi \in [a, b]\) be such that \(f^{(n)}(\xi) - p^{(n)}(\xi) = 0\). As \(p\) has degree \(n\), \(p^{(n)}(x) = n! \cdot \text{ leading coefficient}\). The result thus follows.
\end{proof}

\begin{theorem}[Newton's interpolation formula]\index{Newton's formula}
  Let \(x_0, \dots, x_n\) be pairwise distinct and let
  \[
    p(x) = f[x_0] + f[x_0, x_1](x - x_0) + \dots + f[x_0, \dots, x_n]\prod_{i = 0}^{n - 1}(x - x_i).
  \]
  Then \(p(x_i) = f(x_i)\) for \(i = 0, \dots, n\).
\end{theorem}

\begin{proof}
  Induction on \(n\). If \(n = 0\) then \(p(x) = f[x_0]\) which trivially satisfies \(p(x_0) = f[x_0] = f(x_0)\).

  Suppose it holds for \(n - 1\). Let \(p_n \in P_{n - 1}[x]\) interpolate \(f\) at \(x_0, \dots, x_{n - 1}\) and \(p \in P_n[x]\) interpolate \(f\) at \(x_0, \dots, x_{n - 1}, x_n\). \(p - p_n\) is a polynomial of degree \(n\) that vanishes at \(x_0, \dots, x_{n - 1}\), implying that
  \[
    p - p_n = \alpha \prod_{i = 0}^{n - 1}(x - x_i).
  \]
  But \(\alpha\) is the coefficient of \(x^n\) in \(p\) and so we must have \(\alpha = f[x_0, \dots, x_n]\). Thus we deduce that
  \[
    p = p_n + f[x_0, \dots, x_n] \prod_{i = 0}^{n - 1}(x - x_i).
  \]
  By using induction hypothesis, the result follows.
\end{proof}

\subsubsection{Evaluating Newton's interpolating formula}

Recall that the divided difference is defined recursively by
\[
  f[x_0, x_1, \dots, x_{n + 1}] = \frac{f[x_1, \dots, x_{n + 1}] - f[x_0, \dots, x_n]}{x_{n + 1} - x_0}.
\]
We can draw a table to evaluate them:
\[
  \begin{tikzcd}[column sep=tiny, row sep=tiny]
    f[x_0] \ar[dr] \\
    & f[x_0, x_1] \ar[dr] \\
    f[x_1] \ar[ur] & & \cdots \ar[dr] \\
    \vdots & & & f[x_0, \dots, x_n] \\
    f[x_{n - 1}] \ar[dr] & & \cdots \ar[ur] \\
    & f[x_{n - 1}, x_n] \ar[ur] \\
    f[x_n] \ar[ur]
  \end{tikzcd}
\]

The output of this procedure is \(f[x_j, x_{j + 1}, \dots, x_\ell]\) for all \(0 \leq j \leq \ell \leq n\). To evaluate a new divided difference requires 3 operations (2 substractions and 1 division) so the complexity is
\[
  3(n - 1) + 3(n - 2) + \dots + 3 \approx 3 \cdot \frac{n^2}{2} \sim O(n^2).
\]

Having obtained the divided differences, we can use Horner's scheme to evaluate Newton's formula in \(O(n)\). Note that the term \((x - x_0)\), for example, appears in every term except the first one in Newton's interpolating formula so we may group them together
\begin{align*}
  p(x) &= f[x_0] + (x - x_0) (f[x_0, x_1] + (x - x_1)(f[x_0, x_1, x_2] + \cdots \\
       &+ (x - x_{n - 1})f[x_0, x_1, \dots, x_n])).
\end{align*}
This way the evaluation requires \(O(n)\) operations.

\section{Orthogonal Polynomials}


Missed a lecture

\subsection{Least-squares Polynomial Fitting}

\begin{question}
Fix a scalar product on \([a, b]\) of the form
\[
  \inner{f, g} = \int_a^b w(x)f(x)g(x) dx
\]
where \(w: [a, b] \to \R\) is positive. Given \(f \in C[a, b]\), find \(p \in P_n[x]\) such that
  \[
    \norm{f - g}^2 = \inner{f - p, f - p}
  \]
  is minimised.
\end{question}

Let \(\hat p_n\) be the polynomial of degree \(n\) minimising \(\norm{f - g}^2\).

\begin{theorem}
  Let \(p_0, p_1, p_2\) be orthogonal polynomials with respect to \(\inner{\cdot, \cdot}\). Then
  \[
    \hat p_n = \sum_{k = 0}^n \frac{\inner{f, p_k}}{\inner{p_k, p_k}} p_k.
  \]
\end{theorem}

\begin{proof}
  We know that any \(p \in P_n[x]\) can be written as
  \[
    p = \sum_{i = 0}^n c_kp_k
  \]
  where \(c_0, \dots, c_n \in \R\). Then
  \begin{align*}
    \inner{f - p, f - p} &= \inner*{f - \sum_{k = 0}^n c_kp_k, f - \sum_{k = 0}^n c_kp_k} \\
                         &= \inner{f, f} - 2\inner*{f, \sum_{k = 0}^n c_kp_k} + \inner*{\sum_{k = 0}^n c_kp_k, \sum_{k = 0}^n c_kp_k} \\
                         &= \inner{f, f} - 2\inner*{f, \sum_{k = 0}^n c_kp_k} + \sum_{k = 0}^n \sum_{k' = 0}^n c_kc_{k'}\underbrace{\inner{p_k, p_k}}_{= 0 \text{ if } k \neq k'} \\
                         &= \inner{f, f} - 2\inner*{f, \sum_{k = 0}^n c_kp_k} + \sum_{k = 0}^n c_k^2 \inner{p_k, p_k}
  \end{align*}
  As usual, to minimise the quantity we take derivative with respect to \(c_i\)'s:
  \[
    \frac{\p}{\p c_k} \inner{f - p, f - p} = -2 \inner{f, p_k}  + 2c_k \inner{p_k, p_k}.
  \]
  Set the derivative to \(0\), we get
  \[
    c_k = \frac{\inner{f, p_k}}{\inner{p_k, p_k}}.
  \]
\end{proof}

In this case, the minimal error is
\[
  \norm{f - \hat p_n}^2 = \inner{f - \hat p_n, f - \hat p_n} = \inner{f, f} - \sum_{k = 0}^n \frac{\inner{f, p_k}^2}{\inner{p_k, p_k}} = \inner{f, f} - \inner{\hat p_n, \hat p_n}.
\]
The can be interpreted as the generalised Pythagoras Theorem for inner product spaces.

It is obvious that \(\norm{f - \hat p_n}^2\) is a non-increasing function of \(n\), but
\begin{question}
  Does \(\norm{f - \hat p_n}^2 \to 0\) as \(n \to \infty\)?
\end{question}

The answer is yes can can be proved using Weierstrass theorem, which we state without giving a proof:

\begin{theorem}[Weierstrass]
  Let \(f \in C[a, b]\) where \([a, b]\) is bounded. For all \(\varphi > 0\), there exists a polynomial \(p\) of high enough degree such that
  \[
    |f(x) - p(x)| < \varepsilon
  \]
  for all \(x \in [a, b]\).
\end{theorem}

Using Weierstass theorem, we can prove the claim above. For any \(p\) we have
\[
  norm{f - p}^2 = \int_a^b w(x)(f(x) - p(x))62 dx \leq \left(\max_{x \in [a, b]} |f(x) - p(x)| \right)^2 \int_a^b w(x) d(x).
\]
Give any \(\delta > 0\), by Weierstrass theorem applied with \(\varphi = \sqrt{\delta/\int_a^b w(x) dx}\), there is a polynomial \(p\) of degree \(N\) such that
\[
  |(x) - p(x)| < \varepsilon
\]
for all \(x \in [a, b]\). Then for any \(N \geq n\),
\[
  \norm{f - \hat p_N}^2 \leq \norm{f - p}^2 \leq \varepsilon^2 \int_a^b w(x) dx = \delta
\]
as required.

\begin{theorem}[Parseval identity]\index{Parseval identity}
  \[
    \sum_{k = 0}^\infty \frac{\inner{f, p_k}^2}{\inner{p_k, p_k}} = \inner{f, f}.
  \]
\end{theorem}

\begin{proof}
  Reformulation of the above claim.
\end{proof}

\subsection{Least-squares fitting to discrete data}

Give the value of a function at pairwise distinct points \(x_1, \dots, x_m\), the goal of this section is to find \(p \in P_n[x]\) that minimises
\[
  \sum_{k = 1}^m (f(x_k) - p(x_k))^2.
\]
This can be thought as a generalisation of the problem studied in chapter 1, where \(m = n + 1\) and the minimised error is always \(0\). In the generalised setting, \(n \leq m - 1\) (and usual much smaller).

This question can be reformulated as in terms of orthogonal polynomial, where the inner product is defined to be
\[
  \inner{g, h} = \sum_{k = 1}^m g(x_k)h(x_k)
\]
since
\[
  \inner{f - p, f - p} = \sum_{k = 0}^n (f(x_k) - p(x_k))^2.
\]

We can proceed as follow (\(n \leq m - 1\)):
\begin{enumerate}
\item use three-term recurrence for the inner product to compute orthogonal polynomials \(p_0, \dots, p_n\).
\item Form
  \[
    \hat p_n = \sum_{k = 0}^n \frac{\inner{f, p_k}}{\inner{p_k, p_k}}p_k.
  \]
\end{enumerate}
The reason we require \(n \leq m - 1\) is for positive definiteness.

\subsection{Gaussian Quadrature}

\begin{question}
  Let \(w: [a, b] \to \R\) be positive. We want to approximate \(\int_a^b w(x)f(x) dx\) where \(f \in C[a, b]\).
\end{question}
A \emph{quadrature formula} is an approximation of the above expression
\begin{equation}
  \label{eqn:quadrature}
  \int_a^b w(x)f(x) dx \approx \sum_{k = 1}^\nu b_kf(c_k)
  \tag{\(\ast\)}
\end{equation}
where \(b_k\) are \emph{weights} and \(c_k\) are \emph{nodes}.

We require the quadrature formula to be exact for \(f \in P_n[x]\).

Claim that if the quadrature formula \eqref{eqn:quadrature} is exact for any \(f \in P_n[x]\) then necessarily \(n \leq 2\nu - 1\).

\begin{proof}
  We will construct a polynomial of degree \(2\nu\) such that LHS and RHS are different. Let
  \[
    f(x) = \prod_{k = 1}^\nu (x - c_k)^2
  \]
  which has degree \(2\nu\). RHS is equal to \(0\) while LHS is \(\int_a^b w(x)f(x)dx > 0\).
\end{proof}

Can we find quadrature formula that is exact for all polynomials of degree \(\leq 2\nu - 1\)? Yes, and this is Gaussian quadrature.

Let
\[
  \inner{f, g} = \int_a^b w(x)f(x)g(x) dx
\]
and let \(p_0, p_1, p_2\) be orthogonal polynomials associated to this inner product.

\begin{theorem}
  For any \(n \geq 0\), \(p_n\) has \(n\) real distinct roots in \([a, b]\).
\end{theorem}

\begin{proof}
  Let \(\xi_1, \dots, \xi_m \in (a, b)\) be the points where \(p_n\) changes sign and let
  \[
    q(x) = \prod_{i = 1}^m (x - \xi_i).
  \]
  Note that \(p_n(x)q(x)\) has constant sign in \([a, b]\), i.e.\ in this step we make even root have even multiplicity. Since \(p_nq\) has constant sign on \([a, b]\),
  \[
    |\inner{p_n, q}| = \left| \int_a^b w(x)p_n(x)q(x) dx \right| = \int_a^b w(x) |p_n(x)q(x)| dx > 0
  \]
  This implies that \(\deg q \geq n\) so \(m \geq n\). But \(m \leq n\) because \(p_n\) has degree \(n\). Thus \(p_n\) has \(n\) distinct real roots in \((a, b)\).
\end{proof}

Given nodes \(c_1, \dots, c_\nu\), define the \emph{interpolatory weights}\index{interpolatory weights}
\begin{equation}
  \label{eqn:weight}
  b_k = \int_a^b w(x) \prod_{j = 1, j \neq k}^\nu \frac{x - c_j}{c_k - c_j} dx
  \tag{\(\ast\ast\)}
\end{equation}
for \(k = 1, \dots, \nu\).

\begin{theorem}\leavevmode
  \begin{enumerate}
  \item The quadrature formula with weights given by \eqref{eqn:weight} is exact for polynomials of degree up to \(\nu - 1\).
  \item If furthermore the \(c_k\)'s are the roots of \(p_\nu\), then the quadrature formula is exact for polynomials up to degree \(2\nu - 1\).
  \end{enumerate}
\end{theorem}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item Let \(f \in P_{n - 1}[x]\). Write \(f\) in terms of its interpolating formula at the \(c_k\)'s,
    \[
      f(x) = \sum_{k = 1}^\nu f(c_k) \prod_{j \neq k}^\nu \frac{x - c_j}{c_k - c_j}.
    \]
    Then
    \begin{align*}
      \int_a^b w(x)f(x) dx &= \int_a^b w(x) \sum_{k = 1}^\nu f(c_k) \prod_{j \neq k}^\nu \frac{x - c_j}{c_k - c_j} \\
      &= \sum_{k = 1}^\nu f(c_k) \int_a^b w(x) \prod_{j \neq k} \frac{x - c_j}{c_k - c_j} dx
    \end{align*}
  \item Assume now that \(c_1, \dots, c_\nu\) are the roots of \(p_\nu\). Let \(f \in P_{2\nu - 1}[x]\). We can write
    \[
      f = p_\nu q + r
    \]
    with \(\deg r \leq \nu - 1, \deg q \leq \nu - 1\). Then
    \begin{align*}
      \int_a^b w(x)f(x) dx &= \int_a^b w(x)p_\nu(x) q(x) dx + \int_a^b w(x)r(x) dx \\
                           &= \inner{p_\nu, q} + \sum_{k = 1}^\nu b_k r(c_k) \\
                           &= 0 + \sum_{k = 1}^\nu b_kf(c_k)
    \end{align*}
  \end{enumerate}
\end{proof}

\subsection{Peano Kernel Theorem}

How do the errors behave when we use the quadrature formula to approximate a function that is not a polynomial, or a polynomial of higher degree?

The error from a quadrature formula is
\[
  L(f) = \int_a^b w(x)f(x) dx - \sum_{k = 1}^\nu b_kf(c_k)
\]
Assume \(L(f) = 0\) for all \(f \in P_n[x]\). The goal is to bound error \(|L(f)|\) for \(f \notin P_n[x]\). Recall from IA Analysis I the Taylor expansion formula with integral remainder
\[
  f(x) = \sum_{i = 1}^n f^{(i)}(a) \frac{(x - a)^i}{i!} + \frac{1}{n!} \int_a^x (x - \theta)^n f^{(n + 1)}(\theta) d\theta.
\]
Call the first part \(g(x)\). Since \(g \in P_n[x]\), \(L(g) = 0\). Thus by linearity of \(L\) we get
\[
  L(f) = L\left( x \mapsto \frac{1}{n!} \int_a^x (x - \theta)^n f^{(n + 1)}(\theta) d\theta \right).
\]
Let
\[
  (x - \theta)_+^n =
  \begin{cases}
    (x - \theta)^n & \text{if } \theta \leq x \\
    0 & \text{otherwise}
  \end{cases}
\]
Then
\begin{align*}
  \int_a^x (x - \theta)^n f^{(n + 1)}(\theta) d\theta & = \int_a^b \mathbb{1}(\theta \leq x) (x - \theta)^n f^{(n + 1)}(\theta) d\theta \\
                                                      &= \int_a^b (x - \theta)_+^n f^{(n + 1)}(\theta) d\theta
\end{align*}
so
\[
  L(f) = \frac{1}{n!} \int_a^b K(\theta)f^{(n + 1)}(\theta)
\]
where
\[
  K(\theta) = L(x \mapsto (x - \theta)_+^n)
\]
is the \emph{Peano kernel}\index{Peano kernel}.

\begin{eg}
  Simpson's rule
  \[
    \int_{-1}^1 f(x)dx \approx \frac{1}{3} [f(-1) + 4f(0) + f(1)].
  \]
  Then
  \[
    L(f) = \int_{-1}^1 f(x)dx - \frac{1}{3}[f(-1) + 4f(0) + f(1)]
  \]
  Can check that \(L(f) = 0\) for all \(f \in P_2[x]\). The Peano kernel for \(L\) is
  \[
    K(\theta) = L(x \mapsto (x - \theta)_+^2) =
    \begin{cases}
      -\theta(1 + \theta)^2 & \text{if } \theta \leq 0 \\
      -\theta(1 - \theta)^2 & \text{if } \theta \geq 0
    \end{cases}
  \]
  For any \(f \in C^3[-1, 1]\),
  \begin{align*}
    |L(f)| &= \frac{1}{2}\left| \int_{-1}^1 K(\theta)f^{(3)}(\theta)d\theta \right| \\
           &\leq \frac{1}{2} \max_{\theta \in [-1, 1]} |f^{(3)}(\theta)| \int_{-1}^1|K(\theta)| d\theta \\
    &\leq \frac{1}{36} \max_{\theta \in [-1, 1]} |f^{(3)}(\theta)|
  \end{align*}
\end{eg}







\printindex

\iffalse
http://damtp.cam.ac.uk/user/hf323/L18-IB-NA/
\fi

\end{document}
