\documentclass[a4paper]{article}

\def\npart{IB}

\def\ntitle{Numerical Analysis}
\def\nlecturer{H.\ Fawzi}

\def\nterm{Lent}
\def\nyear{2018}

\input{header}

\newcommand*{\inner}{\innerproduct}

\begin{document}

\input{titlepage}

\tableofcontents

\setcounter{section}{-1}

\section{Introduction}

Numerical analysis is the study of \emph{algorithms} for continuous mathematics. Examples of problems in continuous mathematics are:
\begin{itemize}
\item solve \(f(x) = 0\) where \(f: \R^n \to \R\),
\item solve \(\frac{dx}{dt} = f(x)\) where \(f: \R^n \to \R^n\)
\item optimisation: find \(\min f(x)\) where \(x \in \R^n, f: \R^n \to \R\).
\end{itemize}

A note on complexity: we measure the complexity of an algorithm by the number of \emph{elementary operations} (\(+, \times, -, /\)) it needs.

Big \(O\) notation: for example \(O(n), O(n^2)\), where \(n\) is input size. We also have complexity \(O(f(n))\) if the number of operations is at most \(cf(n)\) where \(c > 0\).

\section{Polynomial Interpolations}

Denote a degree \(n\) polynomial by
\[
  p(x) = p_0 + p_1x + \dots + p_nx^n
\]
and let \(P_n[x]\) be the vector space of polynomials of degree at most \(n\). The interpolation problem is, given \(x_0, x_1, \dots, x_n \in \R\) and \(f_0, f_1, \dots, f_n \in \R\), find \(p \in P_n[x]\) such that \(p(x_i) = f_i\) for \(i = 0, \dots, n\).

\subsection{Lagrange formula}\index{Langrange formula}

Claim that
\[
  p(x) = \sum_{k = 0}^{n} f_k \underbrace{\prod_{\ell \neq k}^{ } \frac{x - x_\ell}{x_k - x_\ell}}_{L_k(x)}
\]
solves the problem.

Note that \(L_k(x_j) = \begin{cases} 1 & j = k \\ 0 &j \neq k \end{cases}\) and the result easily follows.

Now we prove the uniqueness of the solution. Assume \(q \in P_n[x]\) is another polynomial that interpolates the data. Then \(p - q\) has \(n + 1\) zeros. But a non-zero polynomial in \(P_n[x]\) has at most \(n\) zeros. Thus \(p - q\) must be zero.

This is an easy solution but what is its complexity? For each \(k\), the complexity of evaluating \(L_k(x)\) is \(O(n)\) so the total complexity of evaluating \(p(x)\) is \(O(n^2)\).

\subsubsection{Error of polynomial interpolation}

Let \(C^s[a, b]\) be the space of functions \([a, b] \to \R\) that are \(s\) times continuously differentiable.

\begin{theorem}
  Let \(f \in C^{n + 1}[a, b]\) and let \(p \in P_n[x]\) interpolate \(f\) at distinct \(x_0, \dots x_n\) , i.e.\ \(p(x_i) = f(x_i)\) for \(i = 0, \dots, n\). Then for all \(x \in [a, b]\), there exists \(\xi \in [a, b]\) such that
  \[
    f(x) - p(x) = \frac{1}{(n + 1)!} f^{(n + 1)}(\xi) \prod_{i = 0}^n (x - x_i).
  \]
\end{theorem}

The last term \(\prod_{i = 0}^n (x - x_i)\) is called the \emph{nodal polynomial}.

\begin{proof}
  If \(x = x_i\) for some \(i\) then the result is trivially true. Assume \(x\) is distinct from \(x_i\)'s for all \(i\). Define
  \[
    \varphi(t) = f(t) - \left(p(t) + (f(x) - p(x)) \prod_{i = 0}^n \frac{t - x_i}{x - x_i} \right).
  \]
  Note that the second term is by construction the interpolating polynomial of \(f\) at \(x_0, \dots, x_n\) and \(x\). Thus
  \[
    \varphi(x_0) = \varphi(x_1) = \dots = \varphi(x_n) = \varphi(x) = 0
  \]
  so \(\varphi\) has \(n + 2\) zeros. Recall from IA Analysis I Rolle's Theorem: if \(g \in C^1[a, b]\) such that \(g(a) = g(b)\) then there exists \(\alpha \in (a, b)\) such that \(g'(\alpha) = 0\). Apply it to \(\varphi\), we deduce that \(\varphi'\) has \(n + 1\) zeros. Inductively we find that \(\varphi^{(n + 1)}\) has \(1\) zero, i.e.\ there exists \(\xi \in [a, b]\) such that \(\varphi^{(n + 1)}(\xi) = 0\). Thus
  \[
    0 = \varphi^{(n + 1)}(\xi) = f^{(n + 1)}(\xi) - \left( \underbrace{p^{(n + 1)}(\xi)}_{= 0} + (f(x) - p(x)) \frac{(n + 1)!}{\prod_{i = 0}^n(x - x_i)} \right).
  \]
  Rearrange,
  \[
    f(x) - p(x) = \frac{1}{(n + 1)!}f^{(n + 1)}(\xi) \prod_{i = 0}^n(x - x_i).
  \]
\end{proof}

\begin{eg}
  \([a, b] = [-5, 5]\), \(x_j = -5 + 10 \frac{j}{n}\) for \(j = 0, \dots, n\). Plot \(\prod_{i = 0}^n(x - x_i)\), we note that it vanishes at \(x_i\)'s but blows up near the endpoints.
\end{eg}

This is called \emph{Runge's phenomenon}. If one attempts to interpolate \(f(x) = \frac{1}{1 + x^2}\) using equispaced points on \([-5, 5]\) and plots the error \(f(x) - p(x)|\).

Thus equispaced points may not be the most suitable to minimise the error. The remedy is to look for points \(x_0, \dots, x_n\) such that \(|\prod_{i = 0}^n(x - x_i)|\) is small. This leads us to \emph{Chebyshev points}. For example in the previous case we should choose
\[
  x_j = 5 \cos \frac{(n - j)\pi}{n}.
\]
This choice of points is the one that minimises
\[
  \max_{x \in [a, b]} \left| \prod_{i =0}^n(x - x_i) \right|.
\]

\subsection{Divided difference and Newton's interpolation formula}

\begin{definition}[Divided difference]\index{divided difference}
  Given pairwise distinct points \(x_0, \dots, x_n\), let \(p \in P_n[x]\) that interpolates \(f \in C[a, b]\) at these points. The coefficient of \(x^n\) in \(p\) is called the \emph{divided difference} of \(f\) at \((x_0, \dots, x_n)\) and is denoted \(f[x_0, \dots, x_n]\).
\end{definition}

We know from the last lecture that the interpolent is
\[
  p(x) = \sum_{k = 0}^n f(x_k) \prod_{\ell \neq k} \frac{x - x_\ell}{x_k - x_\ell}.
\]
Thus we get
\[
  f[x_0, \dots, x_n] = \sum_{k = 0}^n f(x_k) \prod_{\ell \neq k} \frac{1}{x_k - x_\ell}.
\]

The next example illustrates the reason behind the name ``divided difference''.

\begin{eg}\leavevmode
  \begin{itemize}
  \item \(f[x_0] = f(x_0)\).
  \item \(f[x_0, x_1] = f(x_0)\frac{1}{x_0 - x_1} + f(x_1)\frac{1}{x_1 - x_0} = \frac{f(x_1) - f(x_0)}{x_1 - x_0}\).
  \end{itemize}
\end{eg}

This observation is generalised by

\begin{theorem}
  \[
    f[x_0, x_1, \dots, x_{n + 1}] = \frac{f[x_1, \dots, x_{n + 1}] - f[x_0, \dots, x_n]}{x_{n + 1} - x_0}.
  \]
\end{theorem}

\begin{proof}
  Let \(p \in P_n[x]\) interpolate \(f\) at \(x_0, \dots, x_n\) and \(q \in P_n[x]\) interpolate \(f\) at \(x_1, \dots, x_{n + 1}\) and let
  \[
    r(x) = \frac{x - x_{n + 1}}{x_0 - x_{n + 1}}p(x) + \frac{x - x_0}{x_{n + 1} - x_0}q(x) \in P_{n + 1}[x].
  \]
  Observe that \(r\) interpolates \(f\) at \(x_0, \dots, x_{n + 1}\). So the divided difference, i.e.\ coefficient of \(x^{n + 1}\) in \(r(x)\) is
  \[
    \frac{1}{x_0 - x_{n + 1}}f[x_0, \dots, x_n] + \frac{1}{x_{n + 1} - x_0}f[x_1, \dots, x_{n + 1}].
  \]
\end{proof}

\begin{theorem}
  Assume \(x_0, \dots, x_n\) are pairwise distinct in \([a, b]\) and \(f \in C^n[a, b]\), then there exists \(\xi \in [a, b]\) such that
  \[
    f[x_0, \dots, x_n] = \frac{1}{n!}f^{(n)}(\xi).
  \]
\end{theorem}

\begin{proof}
  Let \(p \in P_n[x]\) interpolate \(f\) at \(x_0, \dots, x_n\). Then function \(f - p\) has \(n + 1\) zeros in \([a, b]\). Applying Rolle's Theorem \(n\) times, we get that \((f - p)^{(n)}\) has at least one zero in \([a, b]\). Let \(\xi \in [a, b]\) be such that \(f^{(n)}(\xi) - p^{(n)}(\xi) = 0\). As \(p\) has degree \(n\), \(p^{(n)}(x) = n! \cdot \text{ leading coefficient}\). The result thus follows.
\end{proof}

\begin{theorem}[Newton's interpolation formula]\index{Newton's formula}
  Let \(x_0, \dots, x_n\) be pairwise distinct and let
  \[
    p(x) = f[x_0] + f[x_0, x_1](x - x_0) + \dots + f[x_0, \dots, x_n]\prod_{i = 0}^{n - 1}(x - x_i).
  \]
  Then \(p(x_i) = f(x_i)\) for \(i = 0, \dots, n\).
\end{theorem}

\begin{proof}
  Induction on \(n\). If \(n = 0\) then \(p(x) = f[x_0]\) which trivially satisfies \(p(x_0) = f[x_0] = f(x_0)\).

  Suppose it holds for \(n - 1\). Let \(p_n \in P_{n - 1}[x]\) interpolate \(f\) at \(x_0, \dots, x_{n - 1}\) and \(p \in P_n[x]\) interpolate \(f\) at \(x_0, \dots, x_{n - 1}, x_n\). \(p - p_n\) is a polynomial of degree \(n\) that vanishes at \(x_0, \dots, x_{n - 1}\), implying that
  \[
    p - p_n = \alpha \prod_{i = 0}^{n - 1}(x - x_i).
  \]
  But \(\alpha\) is the coefficient of \(x^n\) in \(p\) and so we must have \(\alpha = f[x_0, \dots, x_n]\). Thus we deduce that
  \[
    p = p_n + f[x_0, \dots, x_n] \prod_{i = 0}^{n - 1}(x - x_i).
  \]
  By using induction hypothesis, the result follows.
\end{proof}

\subsubsection{Evaluating Newton's interpolating formula}

Recall that the divided difference is defined recursively by
\[
  f[x_0, x_1, \dots, x_{n + 1}] = \frac{f[x_1, \dots, x_{n + 1}] - f[x_0, \dots, x_n]}{x_{n + 1} - x_0}.
\]
We can draw a table to evaluate them:
\[
  \begin{tikzcd}[column sep=tiny, row sep=tiny]
    f[x_0] \ar[dr] \\
    & f[x_0, x_1] \ar[dr] \\
    f[x_1] \ar[ur] & & \cdots \ar[dr] \\
    \vdots & & & f[x_0, \dots, x_n] \\
    f[x_{n - 1}] \ar[dr] & & \cdots \ar[ur] \\
    & f[x_{n - 1}, x_n] \ar[ur] \\
    f[x_n] \ar[ur]
  \end{tikzcd}
\]

The output of this procedure is \(f[x_j, x_{j + 1}, \dots, x_\ell]\) for all \(0 \leq j \leq \ell \leq n\). To evaluate a new divided difference requires 3 operations (2 substractions and 1 division) so the complexity is
\[
  3(n - 1) + 3(n - 2) + \dots + 3 \approx 3 \cdot \frac{n^2}{2} \sim O(n^2).
\]

Having obtained the divided differences, we can use Horner's scheme to evaluate Newton's formula in \(O(n)\). Note that the term \((x - x_0)\), for example, appears in every term except the first one in Newton's interpolating formula so we may group them together
\begin{align*}
  p(x) &= f[x_0] + (x - x_0) (f[x_0, x_1] + (x - x_1)(f[x_0, x_1, x_2] + \cdots \\
       &+ (x - x_{n - 1})f[x_0, x_1, \dots, x_n])).
\end{align*}
This way the evaluation requires \(O(n)\) operations.

\section{Orthogonal Polynomials}

\subsection{Definitions}

\begin{definition}[Inner product]\index{inner product}
  An \emph{inner product} is a function \(\inner{\cdot, \cdot}: V \times V \to \R\) where \(V\) is a real vector space which is
  \begin{enumerate}
  \item symmetric: for all \(x, y \in V\), \(\inner{x, y} = \inner{y, x}\),
  \item positive-definite: for all \(x \in V\), \(\inner{x, x} \geq 0\) with equality if and only if \(x = 0\),
  \item linear: for all \(x, y, z \in V\), \(a, b \in \R\), \(\inner{ax + by, z} = a\inner{x, z} + b\inner{y, z}\).
  \end{enumerate}
\end{definition}

\begin{definition}[Orthogonality]
  Given an inner product, \(x, y \in V\) are \emph{orthogonal} if \(\inner{x, y} = 0\).
\end{definition}

In this section, let \(V = C[a, b]\) and let \(w \in V\) be a positive function and define an inner product
\[
  \inner{f, g} = \int_a^b w(x)f(x)g(x) dx.
\]
It is easy to verify the axioms of inner product.

\begin{definition}[Orthogonal polynomial]\index{orthogonal polynomial}
  Given an inner product in \(V = P[x]\), the \emph{orthogonal polynomials} are a sequence of polynomials \(p_0, p_1, \dots\) such that
  \begin{enumerate}
  \item \(\deg p_n = n\) for all \(n \geq 0\) and
  \item \(\inner{p_n, p_m} = 0\) for all \(n \neq m\).
  \end{enumerate}
\end{definition}

\begin{remark}
  For any \(n \geq 0\), \(\{p_0, \dots, p_n\}\) is an orthogonal basis of \(P_n[x]\).
\end{remark}

The next theorem shows that this object with the desired property does exist:

\begin{theorem}
  For every \(n \geq 0\) there exists a unique monic orthogonal polynomial \(p_n\) of degree \(n\).
\end{theorem}

\begin{proof}
  Let \(p_0(x) = 1\) and proceed by induction on \(n\). Suppose \(p_0, \dots, p_n\) satisfy the induction hypothesis. To define \(p_{n + 1}\) let \(q(x) = x^{n + 1} \in P_{n + 1}[x]\) and conduct Gram-Schmidt on \(q(x)\):
  \[
    p_{n + 1}(x) = q(x) - \sum_{k = 0}^n \frac{\inner{q, p_k}}{\inner{p_k, p_k}}p_k(x).
  \]
  Clearly \(p_{n + 1} \in P_{n + 1}[x]\) and is monic. The orthogonality is a consequence of Gram-Schmidt.

  To prove uniqueness, suppose there exists \(\tilde p_{n + 1} \in P_{n + 1}[x]\) also monic orthogonal. Then \(p = p_{n + 1} - \tilde p_{n + 1} \in P_n[x]\) and thus we have
  \[
    0 = \inner{p_{n + 1}, p} - \inner{\tilde p_{n + 1}, p} = \inner{p, p}
  \]
  so \(p = 0\).
\end{proof}

\begin{eg}[Legendre polynomials]\index{orthogonal polynomial!Legendre}
  Define an inner product
  \[
    \inner{f, g} = \int_{-1}^1 f(x)g(x) dx
  \]
  for \(f, g \in P[x]\). The orthogonal polynomials arising from the scalar product is called \emph{Legendre polynomials}. The first few terms in the sequence are
  \begin{align*}
    p_0(x) &= 1 \\
    p_1(x) &= x \\
    p_2(x) &= x^2 - \frac{1}{3} \\
    p_3(x) &= x^3 - \frac{3}{5}x \\
    f_4(x) &= x^4 - \frac{30}{35}x^2 + \frac{3}{35}
  \end{align*}
\end{eg}

Some well-known examples of orthogonal polynomials:
\begin{table}[h!]
  \centering
  \begin{tabular}{|c||c|c|c|}
    \hline
    Name & Notation & \([a, b]\) & \(w(x)\) \\ \hline
    Legendre & \(P_n\) & \([-1, 1]\) & \(1\) \\ \hline
    Chebyshev & \(T_n\) & \([-1, 1]\) & \((1 - x^2)^{-1/2}\) \\ \hline
    Laguerre & \(L_n\) & \([0, \infty)\) & \(e^{-x}\) \\ \hline
    Hermite & \(H_n\) & \((-\infty, \infty)\) & \(e^{-x^2}\) \\ \hline
  \end{tabular}
  \caption{Common orthogonal polynomials}
\end{table}

\subsection{Three-term recurrence relation}

Gram-Schmidt gives us a way to construct orthogonal polynomials, but it suffers from loss of accuracy due to imprecisions in the calculation of scalar products. A considerably better procedure follows from our next theorem, which relies on the mild assumption that the inner product satisfies
\[
  \inner{xp, q} = \inner{p, xq}
\]
for all \(p, q \in P[x]\).

\begin{theorem}
  Assume the scalar product on \(P[x]\) satisfies \(\inner{xp, q} = \inner{p, xq}\) for all \(p, q \in P[x]\). The orthogonal polynomials are given by
  \begin{align*}
    p_{-1}(x) &= 0 \\
    p_{0}(x) &= 1 \\
    p_{n + 1}(x) &= (x - \alpha_n)p_n(x) - \beta_np_{n - 1}(x),\, n \geq 0
  \end{align*}
  where
  \begin{align*}
    \alpha_n &= \frac{\inner{p_n, xp_n}}{\inner{p_n, p_n}} \\
    \beta_n &= \frac{\inner{p_n, p_n}}{\inner{p_{n - 1}, p_{n - 1}}}
  \end{align*}
\end{theorem}

\begin{proof}
  Let \(n \geq 0\) and \(\psi(x) = p_{n + 1}(x) - (x - \alpha_n)p_n(x) + \beta_np_{n - 1}(x)\). Since \(p_n\) and \(p_{n + 1}\) are monic, it follows that \(\psi \in P_n[x]\). Moreover, because of orthogonality of \(p_{n - 1}, p_n, p_{n + 1}\),
  \[
    \inner{\psi, p_\ell} = 0
  \]
  for \(\ell = 0, \dots, n - 2\) by linearity. Because of monicity, \(q = xp_{n - 1} - p_n \in P_{n - 1}[x]\). Thus from the definition of \(\alpha_n\) and \(\beta_n\),
  \begin{align*}
    \inner{\psi, p_{n - 1}} &= -\inner{p_n, xp_{n - 1}} + \beta_n\inner{p_{n - 1}, p_{n - 1}} = -\inner{p_n, p_n} + \beta_n\inner{p_{n - 1}, p_{n - 1}} = 0 \\
    \inner{\psi, p_n} &= -\inner{xp_n, p_n} + \alpha_n\inner{p_n, p_n} = 0
  \end{align*}
  Every \(p \in P_n[x]\) that obeys \(\inner{p, p_\ell} = 0\) for all \(0 \leq \ell \leq n\) must necessarily be the zero polynomials. Thus \(\psi = 0\) and the result follows.
\end{proof}

\begin{eg}[Chebyshev polynomials]\index{orthogonal polynomial!Chebyshev}
  Define an inner product on \(C[-1, 1]\)
  \[
    \inner{f, g} = \int_{-1}^1 \frac{f(x)g(x)}{\sqrt{1 - x^2}} dx
  \]
  and define
  \begin{align*}
    T_n &\in P_n[x] \\
    T_n(\cos\theta) &= \cos(n\theta)
  \end{align*}
  The first three terms are thus
  \begin{align*}
    T_0(x) &= 1 \\
    T_1(x) &= x \\
    T_2(x) &= 2x^2 - 1
  \end{align*}
  The inner product can be easily found by a change of vairable:
  \begin{align*}
    \inner{T_n, T_m}
    &= \int_{-1}^1 \frac{T_n(x)T_m(x)}{\sqrt{1 - x^2}} dx \\
    &= \int_0^\pi \cos n\theta \cos m\theta d\theta \\
    &= \frac{1}{2} \int_0^\pi (\cos (n + m)\theta + \cos (n - m)\theta) d\theta \\
    &= 0
  \end{align*}
  whenever \(n \neq m\).

  The recurrence relation for Chebyshev polynomials is particularly simple:
  \[
    T_{n + 1}(x) = 2xT_n(x) - T_{n - 1}(x)
  \]
  which can be verified at once from the identity
  \[
    \cos(n + 1)\theta + \cos(n - 1)\theta = 2\cos \theta \cos (n\theta).
  \]
  Note that \(T_n\)'s are not monic. To obtain monic polynomials take \(T_n/2^{n - 1}\) for \(n \geq 1\).
\end{eg}


\subsection{Least-squares polynomial fitting}

\begin{question}
Fix a scalar product on \(C[a, b]\) of the form
\[
  \inner{f, g} = \int_a^b w(x)f(x)g(x) dx
\]
where \(w: [a, b] \to \R\) is positive. Given \(f \in C[a, b]\), find \(p \in P_n[x]\) such that
  \[
    \norm{f - p}^2 = \inner{f - p, f - p}
  \]
  is minimised.
\end{question}

Let \(\hat p_n\) be the polynomial of degree \(n\) minimising \(\norm{f - p}^2\).

\begin{theorem}
  Let \(p_0, p_1, p_2\) be orthogonal polynomials with respect to \(\inner{\cdot, \cdot}\). Then
  \[
    \hat p_n = \sum_{k = 0}^n \frac{\inner{f, p_k}}{\inner{p_k, p_k}} p_k.
  \]
\end{theorem}

\begin{proof}
  We know that any \(p \in P_n[x]\) can be written as
  \[
    p = \sum_{i = 0}^n c_kp_k
  \]
  where \(c_0, \dots, c_n \in \R\). Then
  \begin{align*}
    \inner{f - p, f - p} &= \inner*{f - \sum_{k = 0}^n c_kp_k, f - \sum_{k = 0}^n c_kp_k} \\
                         &= \inner{f, f} - 2\inner*{f, \sum_{k = 0}^n c_kp_k} + \inner*{\sum_{k = 0}^n c_kp_k, \sum_{k = 0}^n c_kp_k} \\
                         &= \inner{f, f} - 2\inner*{f, \sum_{k = 0}^n c_kp_k} + \sum_{k = 0}^n \sum_{k' = 0}^n c_kc_{k'}\underbrace{\inner{p_k, p_k}}_{= 0 \text{ if } k \neq k'} \\
                         &= \inner{f, f} - 2\inner*{f, \sum_{k = 0}^n c_kp_k} + \sum_{k = 0}^n c_k^2 \inner{p_k, p_k}
  \end{align*}
  As usual, to minimise the quantity we take derivative with respect to \(c_i\)'s:
  \[
    \frac{\p}{\p c_k} \inner{f - p, f - p} = -2 \inner{f, p_k}  + 2c_k \inner{p_k, p_k}.
  \]
  Set the derivative to \(0\), we get
  \[
    c_k = \frac{\inner{f, p_k}}{\inner{p_k, p_k}}.
  \]
\end{proof}

In this case, the minimal error is
\[
  \norm{f - \hat p_n}^2 = \inner{f - \hat p_n, f - \hat p_n} = \inner{f, f} - \sum_{k = 0}^n \frac{\inner{f, p_k}^2}{\inner{p_k, p_k}} = \inner{f, f} - \inner{\hat p_n, \hat p_n}.
\]
The can be interpreted as the generalised Pythagoras Theorem for inner product spaces.

It is obvious that \(\norm{f - \hat p_n}^2\) is a non-increasing function of \(n\), but
\begin{question}
  Does \(\norm{f - \hat p_n}^2 \to 0\) as \(n \to \infty\)?
\end{question}

The answer is yes and can be proved using Weierstrass theorem, which we state without giving a proof:

\begin{theorem}[Weierstrass]
  Let \(f \in C[a, b]\) where \([a, b]\) is bounded. For all \(\varepsilon > 0\), there exists a polynomial \(p\) of high enough degree such that
  \[
    |f(x) - p(x)| < \varepsilon
  \]
  for all \(x \in [a, b]\).
\end{theorem}

Using Weierstrass theorem, we can prove the claim above. For any \(p\) we have
\[
  \norm{f - p}^2 = \int_a^b w(x)(f(x) - p(x))^2 dx \leq \left(\max_{x \in [a, b]} |f(x) - p(x)| \right)^2 \int_a^b w(x) d(x).
\]
Give any \(\delta > 0\), by Weierstrass theorem applied with \(\varphi = \sqrt{\delta/\int_a^b w(x) dx}\), there is a polynomial \(p\) of degree \(N\) such that
\[
  |f(x) - p(x)| < \varepsilon
\]
for all \(x \in [a, b]\). Then for any \(N \geq n\),
\[
  \norm{f - \hat p_N}^2 \leq \norm{f - p}^2 \leq \varepsilon^2 \int_a^b w(x) dx = \delta
\]
as required.

\begin{theorem}[Parseval identity]\index{Parseval identity}
  \[
    \sum_{k = 0}^\infty \frac{\inner{f, p_k}^2}{\inner{p_k, p_k}} = \inner{f, f}.
  \]
\end{theorem}

\begin{proof}
  Reformulation of the above claim.
\end{proof}

\subsection{Least-squares fitting to discrete data}

Given the value of a function at pairwise distinct points \(x_1, \dots, x_m\), the goal of this section is to find \(p \in P_n[x]\) that minimises
\[
  \sum_{k = 1}^m (f(x_k) - p(x_k))^2.
\]
This can be thought as a generalisation of the interpolation problem, where \(m = n + 1\) and the minimised error is always \(0\). In the generalised setting, \(n \leq m - 1\) (and usual much smaller).

This question can be reformulated as in terms of orthogonal polynomial, where the inner product is defined to be
\[
  \inner{g, h} = \sum_{k = 1}^m g(x_k)h(x_k)
\]
since then
\[
  \inner{f - p, f - p} = \sum_{k = 0}^n (f(x_k) - p(x_k))^2.
\]

We can proceed as follow, given \(n \leq m - 1\):
\begin{enumerate}
\item use three-term recurrence for the inner product to compute orthogonal polynomials \(p_0, \dots, p_n\).
\item Form
  \[
    \hat p_n = \sum_{k = 0}^n \frac{\inner{f, p_k}}{\inner{p_k, p_k}}p_k.
  \]
\end{enumerate}
The reason we require \(n \leq m - 1\) is for positive definiteness.

\subsection{Gaussian quadrature}

\begin{question}
  Let \(w: [a, b] \to \R\) be positive. We want to approximate the integral \(\int_a^b w(x)f(x) dx\) where \(f \in C[a, b]\).
\end{question}
A \emph{quadrature formula}\index{quadrature} is an approximation of the above expression
\begin{equation}
  \label{eqn:quadrature}
  \int_a^b w(x)f(x) dx \approx \sum_{k = 1}^\nu b_kf(c_k)
  \tag{\(\ast\)}
\end{equation}
where \(b_k\) are \emph{weights} and \(c_k\) are \emph{nodes}.

We require the quadrature formula to be exact for \(f \in P_n[x]\).

Claim that if the quadrature formula \eqref{eqn:quadrature} is exact for any \(f \in P_n[x]\) then necessarily \(n \leq 2\nu - 1\).

\begin{proof}
  We will construct a polynomial of degree \(2\nu\) such that LHS and RHS are different. Let
  \[
    f(x) = \prod_{k = 1}^\nu (x - c_k)^2
  \]
  which has degree \(2\nu\). RHS is equal to \(0\) while LHS is \(\int_a^b w(x)f(x)dx > 0\).
\end{proof}

Can we find a quadrature formula that is exact for all polynomials of degree \(\leq 2\nu - 1\)? Yes, and this is \emph{Gaussian quadrature}\index{quadrature!Gaussian}.

Let
\[
  \inner{f, g} = \int_a^b w(x)f(x)g(x) dx
\]
and let \(p_0, p_1, p_2\) be orthogonal polynomials associated to this inner product.

\begin{theorem}
  For any \(n \geq 0\), \(p_n\) has \(n\) real distinct roots in \([a, b]\).
\end{theorem}

\begin{proof}
  Let \(\xi_1, \dots, \xi_m \in (a, b)\) be the points where \(p_n\) changes sign and let
  \[
    q(x) = \prod_{i = 1}^m (x - \xi_i).
  \]
  Note that \(p_n(x)q(x)\) has constant sign in \([a, b]\), i.e.\ in this step we make every root have even multiplicity. Since \(p_nq\) has constant sign on \([a, b]\),
  \[
    |\inner{p_n, q}| = \left| \int_a^b w(x)p_n(x)q(x) dx \right| = \int_a^b w(x) |p_n(x)q(x)| dx > 0
  \]
  This implies that \(\deg q \geq n\) so \(m \geq n\). But \(m \leq n\) because \(p_n\) has degree \(n\). Thus \(p_n\) has \(n\) distinct real roots in \((a, b)\).
\end{proof}

Given nodes \(c_1, \dots, c_\nu\), define the \emph{interpolatory weights}\index{interpolatory weights}
\begin{equation}
  \label{eqn:weight}
  b_k = \int_a^b w(x) \prod_{j = 1, j \neq k}^\nu \frac{x - c_j}{c_k - c_j} dx
  \tag{\(\ast\ast\)}
\end{equation}
for \(k = 1, \dots, \nu\).

\begin{theorem}\leavevmode
  \begin{enumerate}
  \item The quadrature formula with weights given by \eqref{eqn:weight} is exact for polynomials of degree up to \(\nu - 1\).
  \item If furthermore the \(c_k\)'s are the roots of \(p_\nu\), then the quadrature formula is exact for polynomials up to degree \(2\nu - 1\).
  \end{enumerate}
\end{theorem}

\begin{proof}\leavevmode
  \begin{enumerate}
  \item Let \(f \in P_{n - 1}[x]\). Write \(f\) in terms of its interpolating formula at the \(c_k\)'s,
    \[
      f(x) = \sum_{k = 1}^\nu f(c_k) \prod_{j \neq k} \frac{x - c_j}{c_k - c_j}.
    \]
    Then
    \begin{align*}
      \int_a^b w(x)f(x) dx &= \int_a^b w(x) \sum_{k = 1}^\nu f(c_k) \prod_{j \neq k} \frac{x - c_j}{c_k - c_j} dx \\
                           &= \sum_{k = 1}^\nu f(c_k) \int_a^b w(x) \prod_{j \neq k} \frac{x - c_j}{c_k - c_j} dx
    \end{align*}
  \item Assume now that \(c_1, \dots, c_\nu\) are the roots of \(p_\nu\). Let \(f \in P_{2\nu - 1}[x]\). We can write
    \[
      f = p_\nu q + r
    \]
    with \(\deg r \leq \nu - 1, \deg q \leq \nu - 1\). Then
    \begin{align*}
      \int_a^b w(x)f(x) dx &= \int_a^b w(x)p_\nu(x) q(x) dx + \int_a^b w(x)r(x) dx \\
                           &= \inner{p_\nu, q} + \sum_{k = 1}^\nu b_k r(c_k) \\
                           &= 0 + \sum_{k = 1}^\nu b_kf(c_k)
    \end{align*}
  \end{enumerate}
\end{proof}

\subsection{Peano Kernel Theorem}

How do the errors behave when we use the quadrature formula to approximate a function that is not a polynomial, or a polynomial of higher degree?

The error from a quadrature formula is
\[
  L(f) = \int_a^b w(x)f(x) dx - \sum_{k = 1}^\nu b_kf(c_k)
\]
Assume \(L(f) = 0\) for all \(f \in P_n[x]\). The goal is to bound error \(|L(f)|\) for \(f \in C^{n + 1}[a, b]\). Recall from IA Analysis I the Taylor expansion formula with integral remainder
\[
  f(x) = \sum_{i = 0}^n f^{(i)}(a) \frac{(x - a)^i}{i!} + \frac{1}{n!} \int_a^x (x - \theta)^n f^{(n + 1)}(\theta) d\theta.
\]
Call the first part \(g(x)\). Since \(g \in P_n[x]\), \(L(g) = 0\). Thus by linearity of \(L\) we get
\[
  L(f) = L\left( x \mapsto \frac{1}{n!} \int_a^x (x - \theta)^n f^{(n + 1)}(\theta) d\theta \right).
\]
Let
\[
  (x - \theta)_+^n =
  \begin{cases}
    (x - \theta)^n & \text{if } \theta \leq x \\
    0 & \text{otherwise}
  \end{cases}
\]
Then
\begin{align*}
  \int_a^x (x - \theta)^n f^{(n + 1)}(\theta) d\theta & = \int_a^b 𝟙_{\theta \leq x} (x - \theta)^n f^{(n + 1)}(\theta) d\theta \\
                                                      &= \int_a^b (x - \theta)_+^n f^{(n + 1)}(\theta) d\theta
\end{align*}
Assuming we can exchange \(L\) and the integral, we get
\[
  L(f) = \frac{1}{n!} \int_a^b K(\theta)f^{(n + 1)}(\theta) dx
\]
where
\[
  K(\theta) = L(x \mapsto (x - \theta)_+^n)
\]
is the \emph{Peano kernel}\index{Peano kernel}.

\begin{eg}[Simpson's rule]
  \[
    \int_{-1}^1 f(x)dx \approx \frac{1}{3} [f(-1) + 4f(0) + f(1)].
  \]
  Then
  \[
    L(f) = \int_{-1}^1 f(x)dx - \frac{1}{3}[f(-1) + 4f(0) + f(1)].
  \]
  We can check that \(L(f) = 0\) for all \(f \in P_2[x]\). The Peano kernel for \(L\) is
  \[
    K(\theta) = L(x \mapsto (x - \theta)_+^2) =
    \begin{cases}
      -\frac{1}{3}\theta(1 + \theta)^2 & \text{if } -1 \leq \theta \leq 0 \\
      -\frac{1}{3}\theta(1 - \theta)^2 & \text{if } 0 \leq \theta \leq 0 \\
      0 & \text{otherwise}
    \end{cases}
  \]
  Assume \(0 \leq \theta \leq 1\) for example,
  \begin{align*}
    K(\theta)
    &= L(x \mapsto (x - \theta)_+^2) \\
    &= \int_{-1}^1 (x - \theta)_+^2 dx - \frac{1}{3} \left[(-1 - \theta)_+^2 + 4(0 - \theta)_+^2 + (1 - \theta)_+^2 \right] \\
    &= \int_\theta^1 (x - \theta)^2 dx - \frac{1}{3}[0 + 0 + (1 - \theta)^2] \\
    &= \frac{(x - \theta)^3}{3}\Big|_0^1 - \frac{1}{3}(1 - \theta)^2 \\
    &= -\frac{1}{3}\theta(1 - \theta)^2
  \end{align*}
  For any \(f \in C^3[-1, 1]\),
  \begin{align*}
    |L(f)| &= \frac{1}{2}\left| \int_{-1}^1 K(\theta)f^{(3)}(\theta)d\theta \right| \\
           &\leq \frac{1}{2} \max_{\theta \in [-1, 1]} |f^{(3)}(\theta)| \int_{-1}^1|K(\theta)| d\theta \\
           &\leq \frac{1}{36} \norm{f^{(3)}(\theta)}_\infty
  \end{align*}

  In particular, applying the result to \(P_2[x]\) tells us that the quadrature is exact for polynomials of degree \(2\) or smaller.
\end{eg}

In fact Peano Kernel Theorem applies to other kind of numerical algorithms as well.

\begin{eg}[Numerical differentiation]
  \[
    f'(0) \approx -\frac{3}{2}f(0) + 2f(1) - \frac{1}{2}f(2).
  \]
  We can check that the formula is exact for \(f(x) = 1, x, x^2\) so by linearity this extends to all elements of \(P_2[x]\). The error is
  \[
    L(f) = f'(0) - [-\frac{3}{2}f(0) + 2f(1) - \frac{1}{2}f(2)]
  \]
  so
  \[
    K(\theta) = L(x \mapsto (x - \theta)_+^2) =
    \begin{cases}
      2\theta - \frac{3}{2}\theta^2 & \text{if } 0 \leq \theta \leq 1 \\
      \frac{1}{2}(2 - \theta)^2 & \text{if } 1 \leq \theta \leq 2 \\
      0 & \text{otherwise}
    \end{cases}
  \]
  For any \(f \in C^3[0, 2]\),
  \begin{align*}
    |L(f)|
    &= \frac{1}{2} \left| \int_0^2 K(\theta)f^{(3)}(0)d\theta d\theta \right| \\
    &\leq \frac{1}{2} \int_0^2 |K(\theta)||f^{(3)}(\theta)|fd\theta \\
    &\leq \frac{1}{2} \norm{f^{(3)}}_\infty \int_0^2 |K(\theta)| d\theta \\
    &\leq \frac{1}{3} \norm{f^{(3)}}_\infty
  \end{align*}
\end{eg}

\section{Ordinary Differential Equations}

Given an ordinary differential equation of the form
\[
  \frac{dy}{dt} = y' = f(t, y),\, y(0) = y_0,\, t \geq 0
\]
where \(f: \R \times \R^N \to \R^N, y \in \R^N\), we want to solve it by compute \(y(t_n)\) where \(t_n = nh\). \(h\) can be thought as the time step.

\subsection{One-step methods}

A \emph{one-step method} is defined as a map
\[
  y_{n + 1} = \varphi_h(t_n, y_n)
\]
where \(y_n\) is our approximation for \(y(nh)\). The map only depends on one previous value, ergo the name.

The \emph{Euler method}\index{Euler method} is a one-step method defined by

\[
  y_{n + 1} = y_n + hf(t_n, y_n), n \geq 0.
\]
The ``deviation'' is
\begin{align*}
  y((n + 1)h)
  &= y(nh + h) \\
  &\approx y(nh) + hy'(nh) \\
  &= y_n + hf(t_n, y_n)
\end{align*}

\begin{definition}[Convergence of algorithm]
  We say that a method \emph{converges} if given \(t^* > 0\),
  \[
    \lim_{h \to 0} \max_{0 \leq n \leq \floor{\frac{t^*}{h}}} \norm{y_n(h) - y(nh)} = 0.
  \]
\end{definition}

\begin{theorem}
  Assume \(f\) is Lipschitz in the second argument. Then Euler's method is convergent.
\end{theorem}

\begin{proof}
  Let \(e_n(h) = y_n(h) - y(nh)\). Then
  \begin{align*}
    e_{n + 1}(h)
    &= y_{n + 1}(h) - y((n + 1)h) \\
    &= [y_n(h) + h f(nh, y_n(h))] - [y(nh) + hy'(nh) + \eta(h)] \\
    \intertext{where we Taylor expand the second term and \(\eta(h) \sim O(h^2)\)}
    &= y_n(h) - y(nh) + h(f(nh, y_n(h) - y'(nh)) - \eta(h) \\
    &= e_n(h) + h[f(nh, y_n(h)) - f(nh, y(nh))] - \eta(h)
  \end{align*}
  Assume the Lipschitz constant is \(\lambda\), we thus get
  \begin{align*}
    \norm{e_{n + 1}(h)}
    &\leq \norm{e_n(h)} + \lambda \norm{y_n(h) - y(nh)} + \norm{\eta(h)} \\
    &\leq \norm{e_n(h)}(1 + h\lambda) + Ch^2
  \end{align*}
  Continue recursively,
  \begin{align*}
    \norm{e_n(h)}
    &\leq \norm{e_{n - 1}(h)}(1 + h\lambda) + Ch^2 \\
    &\leq \norm{e_{n - 2}(h)}(1 + h\lambda)^2 + Ch^2(1 + h\lambda) + Ch^2 \\
    &\leq \cdots \\
    &\leq \underbrace{\norm{e_0(h)}}_0 (1 + h\lambda)^n + Ch^2 \sum_{j = 0}^{n - 1} (1 + h\lambda)^j \\
    &\leq Ch^2 \frac{(1 + h\lambda)^n - 1}{h\lambda} \\
    &= \frac{Ch}{\lambda} [(1 + h\lambda)^n - 1] \\
    \intertext{As \(e^x \geq 1 + x\),}
    &\leq \frac{Ch}{\lambda} (e^{h\lambda n} - 1) \\
    \intertext{Since \(n \leq \frac{t^*}{h}\),}
    &\leq \frac{Ch}{\lambda}(e^{\lambda t^*} - 1) \\
    &\to 0
  \end{align*}
  as \(h \to 0\).
\end{proof}

\subsection{Multistep methods}

An \emph{\(s\)-step method} is defined by a recursion rule
\begin{equation}
  \label{eqn:multi-step}
  \sum_{\ell = 0}^s \rho_\ell y_{n + \ell} = h \sum_{\ell = 0}^s \sigma_\ell f(t_{n + \ell}, y_{n + \ell})
\end{equation}
where \(\rho_s = 1\).

The method is called \emph{explicit} if \(\sigma_s = 0\). Otherwise it is called \emph{implicit}.

For example for Euler's method,
\[
  y_{n + 1} - y_n = hf(t_n, y_n)
\]
\(s = 1, \rho_1 = 1, \rho_0 = -1, \sigma_1 = 0, \sigma_s = 1\).
This is an explicit method.

By constrast, the implicit Euler method is
\[
  y_{n + 1} - y_n = hf(t_{n + 1}, y_{n + 1}).
\]

An example of a \(2\)-step method is \emph{\(2\)-step Adams-Bashforth} defined by
\[
  y_{n + 2} - y_{n + 1} = h \left( \frac{3}{2}f(t_{n + 1}, y_{n + 1}) - \frac{1}{2}f(t_n, y_n) \right)
\]

\begin{definition}[Order]\index{order}
  The \emph{order} of a multistep method is the biggest integer \(p \geq 0\) such that
  \[
    \sum_{\ell = 0}^s \sigma_s y(t_{n + \ell}) - h \sum_{\ell = 0}^s \sigma_\ell y'(t_{n + \ell}) = O(h^{p + 1})
  \]
  for all sufficiently smooth functions \(y\).
\end{definition}

If we think \(y'(t_{n + \ell}) = f(t_{n + \ell}, y(t_{n + \ell}))\), this is the error of starting at the previous iteration, i.e.\ the ``local error''.

Now we calculate the order of Euler method.
\begin{align*}
  y(t_{n + 1}) - y(t_n) - hy'(t_n)
  &= y(t_n + h) - y(t_n) - hy'(t_n) \\
  &= y(t_n) + hy'(t_n) + O(h^2) - y(t_n) -hy'(t_n) \\
  &= O(h^2)
\end{align*}
so it has order \(1\).

Another example: the \emph{theta-method} is given by
\[
  y_{n + 1} = y_n + h[\theta f(t_n, y_n) + (1 - \theta) f(t_{n + 1}, y_{n + 1})]
\]
which can thought as a parameterised Euler method, with the parameter \(\theta\) controlling the ``explicitness''. For \(\theta = \frac{1}{2}\), it is given a special name Trapezoidal rule.

The order of the theta-method is

\begin{align*}
  &\phantom{={} } y(t_{n + 1}) - y(t_n) - h[\theta y'(t_n, y_n) + (1 - \theta) y'(t_{n + 1})] \\
  &= y(t_n) + hy'(t_n) + \frac{h^2}{2} y''(t_n) + \frac{h^3}{3!}y'''(t_n) + O(h^4) \\
  &\quad - y(t_n) - h[\theta y'(t_n) + (1 - \theta)(y'(t_n) + hy''(t_n) + \frac{h^2}{2} y'''(t_n) + O(h^3))] \\
  &= h^2 \left( \frac{y''(t_n)}{2} - (1 - \theta)y''(t_n) \right) + O(h^3) \\
  &= y''(t_n)(\theta - \frac{1}{2})h^2 + O(h^3)
\end{align*}
Thus if \(\theta = \frac{1}{2}\) then it has order \(2\), otherwise order \(1\)\footnote{Strictly speaking we conclude that the order when \(\theta = \frac{1}{2}\) is at least \(2\). To show the order is \(2\) we need to in addition show that the coefficient of \(h^3\) does not vanish.}.

\begin{theorem}
  Let \(\rho(w) = \sum_{\ell = 0}^s \rho_\ell w^\ell\) and \(\sigma(w) = \sum_{\ell = 0}^s \sigma_\ell w^\ell\). Then the order of the multistep method is the largest integer \(p \geq 0\) such that
  \[
    \rho(e^z) - z \sigma(e^z) = O(z^{p + 1})
  \]
  as \(z \to 0\).
\end{theorem}

\begin{proof}
  This is just tedious exercise using Taylor expansion. For analytic \(y\), we have
  \begin{align*}
    &\phantom{={} } \sum_{\ell = 0}^s \rho_\ell y(t_{n + \ell}) - h \sum_{\ell = 0}^s \sigma_\ell y'(t_{n + \ell}) \\
    &= \sum_{\ell = 0}^s \rho_\ell \sum_{k = 0}^\infty \frac{(\ell h)^k}{k!} y^{(k)}(t_n) - h \sum_{\ell = 0}^s \sigma_\ell \sum_{k = 0}^\infty \frac{(\ell h)^k}{k!} y^{(k + 1)}(t_n) \\
    &= \sum_{\ell = 0}^s \rho_\ell y(t_n) + \sum_{k = 1}^\infty \frac{h^k}{k!} \left[\sum_{\ell = 0}^s \rho_\ell \ell^k y^{(k)}(t_n) - \sum_{\ell = 0}^s \sigma_\ell \ell^{k - 1} ky^{(k)}(t_n) \right] \\
    &= \sum_{\ell = 0}^s \rho_\ell y(t_n) + \sum_{k = 1}^\infty \frac{h^k}{k!} \left[ \sum_{\ell = 0}^s \rho_\ell \ell^k - \sum_{\ell = 0}^s \sigma_\ell \ell^{k - 1} k \right] y^{(k)}(t_n)
  \end{align*}
  This shows that the method has order \(p\) if and only if
  \begin{align*}
    \sum_{\ell = 0}^s \rho_\ell &= 0 \\
    \sum_{\ell = 0}^s \rho^\ell \ell^k - \sum_{\ell = 0}^s \sigma_\ell \ell^{k - 1}k &= 0
  \end{align*}
  for all \(1 \leq k \leq p\).

  Now repeat the same for the exponentials,
  \begin{align*}
    &\phantom{={} } \rho(e^z) - z\rho(e^z) \\
    &= \sum_{\ell = 0}^s \rho_\ell e^{\ell z} - z \sum_{\ell = 0}^s \sigma_\ell e^{\ell z} \\
    &= \sum_{\ell = 0}^s \rho_\ell \sum_{k = 0}^\infty \frac{(\ell z)^k}{k!} - z\sum_{\ell = 0}^s \sigma_\ell \sum_{k = 0}^\infty \frac{(\ell z)^k}{k!} \\
    &= \sum_{\ell = 0}^s \rho_\ell + \sum_{k = 1}^\infty \frac{z^k}{k!} \left[ \sum_{\ell = 0}^s \rho_\ell \ell^k - \sum_{\ell = 0}^s \sigma_\ell \ell^{k - 1}k \right]
  \end{align*}
  which is of \(O(z^{p + 1})\) if and only if
  \begin{align*}
    \sum_{\ell = 0}^s \rho_\ell &= 0 \\
    \sum_{\ell = 0}^s \rho_\ell \ell^k - \sum_{\ell = 0}^s \sigma_\ell \ell^{k - 1}k &= 0
  \end{align*}
  for all \(1 \leq k \leq p\), which is exactly the same condition as before. The result thus follows.
\end{proof}

\begin{eg}[Adams-Bashforth]
  Recall
  \[
    y_{n + 2} - y_{n + 1} = h \left( \frac{3}{2}f(t_{n + 1}, y_{n + 1}) - \frac{1}{2}f(t_n, y_n) \right).
  \]
  We have
  \begin{align*}
    \rho(w) &= w^2 - w \\
    \sigma(w) &= \frac{3}{2}w - \frac{1}{2}
  \end{align*}
  Thus the exponential formula says
  \begin{align*}
    \rho(e^z) -z\sigma(e^z)
    &= e^{2z} - e^z - z (\frac{3}{2}e^z - \frac{1}{2}) \\
    &= 1 + 2z + \frac{(2z)^2}{2} + \frac{(2x)^3}{3!} \\
    &\quad - (1 + z + \frac{z^2}{2} + \frac{z^3}{3!}) \\
    &\quad -z(\frac{3}{2} + \frac{3z}{2} + \frac{3z^2}{4} - \frac{1}{2}) + O(z^4) \\
    &= z^3(\frac{8}{6} - \frac{1}{6} - \frac{3}{4}) + O(z^4)
  \end{align*}
  so it has order \(2\).
\end{eg}

\subsubsection{Convergence of multistep methods}

\begin{definition}[Root condition]\index{root condition}
  We say that a polynomial \(\rho(w) = \sum_{\ell = 0}^s \rho_\ell w^\ell\) satisfies the \emph{root condition} if all the roots of \(\rho\) lie inside the unit disc \(\{z \in \C: |z| \leq 1\}\) and any root with modulus \(1\) is simple.
\end{definition}

\begin{theorem}[Dahlquist equivalence theorem]\index{Dahquist equivalence theorem}
  The general \(s\)-step method given by \eqref{eqn:multi-step} is convergent if and only if it has order \(p \geq 1\) and \(\rho(w) = \sum_{\ell = 0}^s \rho_\ell w^\ell\) satisfies the root condition.
\end{theorem}

\begin{eg}[Adams-Bashforth]
  \[
    y_{n + 2} - y_{n + 1} = h \left( \frac{3}{2} f(t_{n + 1}, t_{n + 2}) - \frac{1}{2}f(t_n, y_n) \right).
  \]
  We have checked last time that the order is 2. \(\rho(w) = w^2 - w\) with roots \(0\) and \(1\) so the method satisfies the root condition and converges.
\end{eg}

\begin{eg}
  Consider
  \[
    y_{n + 2} - 2y_{n + 1} + y_n = 0.
  \]
  To find the order,
  \begin{align*}
    y(t_{n + 2}) - 2y(t_{n + 1}) + y(t_n)
    &= y(t_n + 2h) - 2y(t_n + h) + y(t_n) \\
    &= \dots \\
    &= O(h^2)
  \end{align*}
  and further computation shows \(p = 1\). \(\rho(w) = w^2 - 2w + 1\) so \(1\) is a root of multiplicity 2 so root condition is not satisfied.
\end{eg}

To construct a convergent \(s\)-step method, follow the two step:
\begin{enumerate}
\item Choose a polynomial \(\rho\) of degree \(s\) that satisfies the root condition and \(\rho(1) = 0\).
\item To get an implicit method, take \(\sigma(w)\) to be the \(s\)-degree truncation of Taylor expansion of \(\frac{\rho(w)}{\log w}\) around \(w = 1\) (which exists since \(\rho(1) = 0\)). For an explicit method, take \(\sigma\) to be the \((s - 1)\)-degree Taylor truncation of \(\frac{\rho(w)}{\log w}\).
\end{enumerate}

We verify that the order \(\geq s\): for an implicit method, by construction
\[
  \sigma(w) = \frac{\rho(w)}{\log w} + O(|w - 1|^{s + 1})
\]
as \(w \to 1\). Substitute \(w = e^z\) and since \(e^z - 1 = z + O(z^2)\)
\[
  \sigma(e^z) = \frac{\rho(e^z)}{z} + O(z^{s + 1})
\]
as \(z \to 0\). So the order \(\geq s + 1\). In the case of an explicit method order \(\geq s\).

\begin{eg}
  \(s = 2, \rho(w) = w^2 - w\) satisfies the root equation.
  \begin{align*}
    \frac{\rho(w)}{\log w}
    &= \frac{w(w - 1)}{\log w} \\
    &= \frac{(v + 1)v}{\log (1 + v)} \\
    &= \frac{(v + 1)v}{ v - \frac{v^2}{2} + \frac{v^3}{3} + O(v^4)} \\
    &= \dots \\
    &= 1 + \frac{3}{2}v + \frac{5}{12}v^2 + O(v^3)
  \end{align*}
  Thus
  \[
    \sigma(w) = 1 + \frac{3}{2}(w - 1) + \frac{5}{12}(w - 1)^2
  \]
  for an implicit method (which is called Adams-Moutton method) and
  \[
    \sigma(w) = 1 + \frac{3}{2}(w - 1) = -\frac{1}{2} + \frac{3}{2}w
    \]
    which gives Adams-Bashforth method.
\end{eg}

\subsection{Backward differentiation formula}

\begin{definition}[Backward differentiation formula]\index{backward differentiation formula}
  A \emph{backward differentiation formula} (BDF) is an \(s\)-step methods with
  \begin{equation}
    \label{eqn:BDF}
    \sigma(w) = \sigma_sw^s.
  \end{equation}
\end{definition}

\begin{theorem}
  For \eqref{eqn:BDF} to have order \(s\), we must have
  \[
    \rho(w) = \sigma_s \sum_{\ell = 1}^s \frac{1}{\ell} w^{s - \ell}(w - 1)^\ell
  \]
  with \(\sigma_s = \left( \sum_{\ell = 1}^s \frac{1}{\ell} \right)^{-1}\).
\end{theorem}

\begin{proof}
  We want \(\rho\) to satisfy
  \[
    \rho(w) - \sigma_sw^s\log w = O(|w - 1|^{s + 1})
  \]
  as \(w \to 1\).
\end{proof}
We have
\[
  \log w
  = -\log \frac{1}{w}
  = -\log \left( 1 - \frac{w - 1}{w} \right)
  = \sum_{\ell = 1}^\infty \frac{1}{\ell} \left( \frac{w - 1}{w} \right)^\ell.
\]
But our choice of \(\rho\) is precisely the first \(s\)-terms in this expansion. The value of \(\sigma_s\) is chosen such that \(\rho_s = 1\).

\begin{eg}
  Let \(s = 2, \sigma_s = (1 + \frac{1}{2})^{-1} = \frac{2}{3}\), then
  \[
    \sigma(w) = \frac{2}{3}(w(w - 1) + \frac{1}{2}(w - 1)^2) = w^2 - \frac{4}{3}w + \frac{1}{3}
  \]
  which gives \(2\)-step BDF
  \[
    y_{n + 2} - \frac{4}{3}y_{n + 1} + \frac{1}{3}y_n = \frac{2}{3}h f(t_{n + 2}, y_{n + 2}).
  \]
\end{eg}

\subsection{Runge-Kutta methods}

Suppose we want to solve the ODE
\[
  y' = f(t, y),\, y(0) = y_0.
\]
The solution of this ODE satisfies
\begin{align*}
  y(t_{n + 1})
  &= y(t_n) + \int_{t_n}^{t_{n + 1}} y'(\tau) d\tau \\
  &= y(t_n) + \int_{t_n}^{t_{n + 1}} f(\tau, y(\tau)) d\tau \\
  &= y(t_n) + h \int_0^1 f(t_n + \alpha h, y(t_n + \alpha h)) d\alpha \\
\intertext{at this point we can apply quadrature formulas}
  &\approx y(t_n) + h \sum_{j = 1}^\nu b_j f(t_n + c_j h, y(t_n + c_j h)) d\alpha
\end{align*}
and Runge-Kutta will give estimates to the \(y_n\)'s.




\printindex

\iffalse
http://damtp.cam.ac.uk/user/hf323/L18-IB-NA/
\fi

\end{document}
